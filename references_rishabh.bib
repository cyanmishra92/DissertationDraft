@book{lamport94,
 author = "Leslie Lamport",
 title = "{\LaTeX: A Document Preparation System}",
 year = "1994",
 publisher = "Addison-Wesley",
 edition = "2nd",
 address = "Reading, Massachusetts"
}

@inproceedings{nicepaper1,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2",
  title = "A Very Nice Paper To Cite",
  year = "2016",
  booktitle = "Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@inproceedings{mutlu2003runahead,
  title={Runahead execution: An alternative to very large instruction windows for out-of-order processors},
  author={Mutlu, Onur and Stark, Jared and Wilkerson, Chris and Patt, Yale N},
  booktitle={The Ninth International Symposium on High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings.},
  pages={129--140},
  year={2003},
  organization={IEEE}
}

@inproceedings{sethi2022recshard,
  title={RecShard: statistical feature-based memory optimization for industry-scale neural recommendation},
  author={Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={344--358},
  year={2022}
}

@inproceedings{zha2022autoshard,
  title={Autoshard: Automated embedding table sharding for recommender systems},
  author={Zha, Daochen and Feng, Louis and Bhushanam, Bhargav and Choudhary, Dhruv and Nie, Jade and Tian, Yuandong and Chae, Jay and Ma, Yinbin and Kejariwal, Arun and Hu, Xia},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={4461--4471},
  year={2022}
}

@article{zha2022dreamshard,
  title={Dreamshard: Generalizable embedding table placement for recommender systems},
  author={Zha, Daochen and Feng, Louis and Tan, Qiaoyu and Liu, Zirui and Lai, Kwei-Herng and Bhushanam, Bhargav and Tian, Yuandong and Kejariwal, Arun and Hu, Xia},
  journal={arXiv preprint arXiv:2210.02023},
  year={2022}
}

@inproceedings{acun2021understanding,
  title={Understanding training efficiency of deep learning recommendation models at scale},
  author={Acun, Bilge and Murphy, Matthew and Wang, Xiaodong and Nie, Jade and Wu, Carole-Jean and Hazelwood, Kim},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={802--814},
  year={2021},
  organization={IEEE}
}
% https://github.com/facebookresearch/dlrm_datasets

@inproceedings{mudigere2022software,
  title={Software-hardware co-design for fast and scalable training of deep learning recommendation models},
  author={Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and others},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={993--1011},
  year={2022}
}

@article{ardalani2022understanding,
  title={Understanding Scaling Laws for Recommendation Models},
  author={Ardalani, Newsha and Wu, Carole-Jean and Chen, Zeliang and Bhushanam, Bhargav and Aziz, Adnan},
  journal={arXiv preprint arXiv:2208.08489},
  year={2022}
}

@inproceedings{isca2022_dsi,
  title={Understanding data storage and ingestion for large-scale deep recommendation model training: industrial product},
  author={Zhao, Mark and Agarwal, Niket and Basant, Aarti and Gedik, Bu{\u{g}}ra and Pan, Satadru and Ozdal, Mustafa and Komuravelli, Rakesh and Pan, Jerry and Bao, Tianshu and Lu, Haowei and others},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={1042--1057},
  year={2022}
}

@inproceedings{deeprecsys,
  title={Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference},
  author={Gupta, Udit and Hsia, Samuel and Saraph, Vikram and Wang, Xiaodong and Reagen, Brandon and Wei, Gu-Yeon and Lee, Hsien-Hsin S and Brooks, David and Wu, Carole-Jean},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={982--995},
  year={2020},
  organization={IEEE}
}

@inproceedings{udit_hpca2020,
  title={The architectural implications of facebook's dnn-based personalized recommendation},
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={488--501},
  year={2020},
  organization={IEEE}
}


@inproceedings{iiswc2020,
  title={Cross-stack workload characterization of deep recommendation systems},
  author={Hsia, Samuel and Gupta, Udit and Wilkening, Mark and Wu, Carole-Jean and Wei, Gu-Yeon and Brooks, David},
  booktitle={2020 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={157--168},
  year={2020},
  organization={IEEE}
}

@inproceedings{recpipe2021,
  title={RecPipe: Co-designing models and hardware to jointly optimize recommendation quality and performance},
  author={Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={870--884},
  year={2021}
}

@misc{kaggle,
  author = {Meta Research},
  year = {2022},
  title = {{DLRM configuration for Criteo Kaggle Training}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm/blob/main/bench/dlrm_s_criteo_kaggle.sh}"},
  note = {}
}

@misc{icelake_anandtech,
  author = {Dr. Ian Cutress},
  year = {2019},
  title = {{Analysis of Icelake architecture: }},
  howpublished = {"\url{https://www.anandtech.com/show/14514/examining-intels-ice-lake-microarchitecture-and-sunny-cove/3}"},
  note = {}
}

@misc{icelake_silver,
  author = {Intel},
  year = {2021},
  title = {{Intel Xeon Silver 4314 Processor}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/products/sku/215269/intel-xeon-silver-4314-processor-24m-cache-2-40-ghz/specifications.html}"},
  note = {}
}

@misc{skylake_gold,
  author = {Intel},
  year = {2017},
  title = {{Intel Xeon Gold 6136 Processor}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/products/sku/120479/intel-xeon-gold-6136-processor-24-75m-cache-3-00-ghz/specifications.html}"},
  note = {}
}

@misc{spr_platinum,
  author = {Intel},
  year = {2023},
  title = {{Intel Xeon Platinum 8480+ Processor}},
  howpublished = {"\url{https://ark.intel.com/content/www/us/en/ark/products/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz.html}"},
  note = {}
}



@misc{epyc7763,
  author = {AMD},
  year = {2021},
  title = {{AMD EPYC 7763}},
  howpublished = {"\url{https://www.amd.com/en/products/cpu/amd-epyc-7763}"},
  note = {}
}


@misc{hw_pref_intel,
  author = {Intel},
  year = {2022},
  title = {{Hardware Prefetchers in Intel CPU}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html}"},
  note = {}
}

@misc{ipex_mlperf,
  author = {MLPerf},
  year = {2022},
  title = {{MLPerf benchmarking on CPUs using Intel Extension for PyTorch}},
  howpublished = {"\url{https://github.com/mlcommons/inference_results_v2.1/tree/master/closed/Intel/code/dlrm-99.9/pytorch-cpu}"},
  note = {}
}

@misc{ipex,
  author = {Intel},
  year = {2022},
  title = {{Intel Extension for PyTorch}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html}"},
  note = {}
}

@misc{vtune,
  author = {Intel},
  year = {2022},
  title = {{Intel VTune Profiler}},
  howpublished = {"\url{https://github.com/intel/intel-extension-for-pytorch}"},
  note = {}
}

@misc{pintool,
  author = {Intel},
  year = {2022},
  title = {{Pin - A Dynamic Binary Instrumentation Tool}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html}"},
  note = {}
}

@misc{amd-milan-x,
  author = {AMD},
  year = {2022},
  title = {{AMD EPYC 7773X}},
  author = {},
  howpublished = {"\url{https://www.amd.com/en/products/cpu/amd-epyc-7773x}"},
  note = {}
}

@inproceedings{hassan2007_lru_stack,
  title={Synthetic trace-driven simulation of cache memory},
  author={Hassan, Rahman and Harris, Antony and Topham, Nigel and Efthymiou, Aris},
  booktitle={21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07)},
  volume={1},
  pages={764--771},
  year={2007},
  organization={IEEE}
}


@misc{rmc3,
  author = {Udit Gupta},
  year = {2020},
  title = {{DLRM configuration in DeepRecSys RMC3}},
  howpublished = {"\url{https://github.com/harvard-acc/DeepRecSys/blob/master/models/configs/dlrm_rm3.json}"},
  note = {}
}



@misc{mlperf,
  author = {MLPerf},
  year = {2022},
  title = {{MLPerf Datacenter Inference Submissions v2.1}},
  howpublished = {"\url{https://mlcommons.org/en/inference-datacenter-21/}"},
  note = {}
}
@misc{prod_dataset,
  author = {Meta Research},
  year = {2021},
  title = {{Embedding Lookup Synthetic Dataset}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm_datasets}"},
  note = {}
}

@misc{emb_bag_operator,
  author = {PyTorch},
  year = {2022},
  title = {{Embedding Bag Operator}},
  howpublished = {"\url{https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html}"},
  note = {}
}

@misc{mm_prefetch,
  author = {Intel},
  year = {2022},
  title = {{Prefetch Intrinsic}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_prefetch}"},
  note = {}
}

@misc{cascade_lake,
  author = {Intel},
  year = {2019},
  title = {{Intel Cascade Lake Architecture}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/products/platforms/details/cascade-lake.html}"},
  note = {}
}

@misc{ice_lake,
  author = {Intel},
  year = {2021},
  title = {{Intel Ice Lake Architecture}},
  howpublished = {"\url{https://ark.intel.com/content/www/us/en/ark/products/codename/74979/products-formerly-ice-lake.html}"},
  note = {}
}

@misc{icc-prefetch,
  author = {Intel},
  year = {2022},
  title = {{Intel C++ Compiler Classic Developer Guide and Reference}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/compiler-options/advanced-optimization-options/qopt-prefetch-qopt-prefetch.html}"},
  note = {}
}

@misc{gcc-prefetch,
  author = {GCC},
  year = {2022},
  title = {{GCC Data Prefetch Support}},
  howpublished = {"\url{https://gcc.gnu.org/projects/prefetch.html}"},
  note = {}
}

@article{naumov_dlrm,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}


@inproceedings{recssd,
  title={RecSSD: near data processing for solid state drive based recommendation inference},
  author={Wilkening, Mark and Gupta, Udit and Hsia, Samuel and Trippel, Caroline and Wu, Carole-Jean and Brooks, David and Wei, Gu-Yeon},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={717--729},
  year={2021}
}

@inproceedings{hercules_hpca2022,
  title={Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation},
  author={Ke, Liu and Gupta, Udit and Hempsteadis, Mark and Wu, Carole-Jean and Lee, Hsien-Hsin S and Zhang, Xuan},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={141--144},
  year={2022},
  organization={IEEE}
}

@inproceedings{recnmp_isca2020,
  title={Recnmp: Accelerating personalized recommendation with near-memory processing},
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={790--803},
  year={2020},
  organization={IEEE}
}

@inproceedings{space_isca2021,
  title={SPACE: locality-aware processing in heterogeneous memory for personalized recommendations},
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={679--691},
  year={2021},
  organization={IEEE}
}

@inproceedings{TensorDIMM_micro2019,
  title={Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning},
  author={Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={740--753},
  year={2019}
}

@inproceedings{scratchpipe_isca2022,
  title={Training personalized recommendation systems from (GPU) scratch: look forward not backwards},
  author={Kwon, Youngeun and Rhu, Minsoo},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={860--873},
  year={2022}
}


@article{eisenman201bandana,
  title={Bandana: Using non-volatile memory for storing deep learning models},
  author={Eisenman, Assaf and Naumov, Maxim and Gardner, Darryl and Smelyanskiy, Misha and Pupyrev, Sergey and Hazelwood, Kim and Cidon, Asaf and Katti, Sachin},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={40--52},
  year={2019}
}

@article{Balasubramanian2021cDLRMLA,
  title={cDLRM: Look Ahead Caching for Scalable Training of Recommendation Models},
  author={Keshav Balasubramanian and Abdulla Alshabanah and Joshua D. Choe and Murali Annavaram},
  journal={Proceedings of the 15th ACM Conference on Recommender Systems},
  year={2021}
}

@inproceedings{Desai2022RandomOB,
  title={Random Offset Block Embedding (ROBE) for compressed embedding tables in deep learning recommendation systems},
  author={Aditya Desai and Li Chou and Anshumali Shrivastava},
  booktitle={MLSys},
  year={2022}
}

@inproceedings{gong2022graphite,
  title={Graphite: optimizing graph neural networks on CPUs through cooperative software-hardware techniques.},
  author={Gong, Zhangxiaowen and Ji, Houxiang and Yao, Yao and Fletcher, Christopher W and Hughes, Christopher J and Torrellas, Josep},
  booktitle={ISCA},
  pages={916--931},
  year={2022}
}

@inproceedings{tullsen2001handling,
  title={Handling long-latency loads in a simultaneous multithreading processor},
  author={Tullsen, Dean M and Brown, Jeffery A},
  booktitle={Proceedings. 34th ACM/IEEE International Symposium on Microarchitecture. MICRO-34},
  pages={318--327},
  year={2001},
  organization={IEEE}
}

@inproceedings{tullsen1995simultaneous,
  title={Simultaneous multithreading: Maximizing on-chip parallelism},
  author={Tullsen, Dean M and Eggers, Susan J and Levy, Henry M},
  booktitle={Proceedings of the 22nd annual international symposium on Computer architecture},
  pages={392--403},
  year={1995}
}

@inproceedings{kraken,
  title={Kraken: Adaptive container provisioning for deploying dynamic dags in serverless platforms},
  author={Bhasi, Vivek M and Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and Mishra, Cyan Subhra and Kandemir, Mahmut Taylan and Das, Chita},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={153--167},
  year={2021}
}

@inproceedings{grandslam,
  title={Grandslam: Guaranteeing slas for jobs in microservices execution frameworks},
  author={Kannan, Ram Srivatsa and Subramanian, Lavanya and Raju, Ashwin and Ahn, Jeongseob and Mars, Jason and Tang, Lingjia},
  booktitle={Proceedings of the Fourteenth EuroSys Conference 2019},
  pages={1--16},
  year={2019}
}

@inproceedings{atoll,
  title={Atoll: A scalable low-latency serverless platform},
  author={Singhvi, Arjun and Balasubramanian, Arjun and Houck, Kevin and Shaikh, Mohammed Danish and Venkataraman, Shivaram and Akella, Aditya},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={138--152},
  year={2021}
}

@inproceedings{cypress,
  title={Cypress: input size-sensitive container provisioning and request scheduling for serverless platforms},
  author={Bhasi, Vivek M and Gunasekaran, Jashwant Raj and Sharma, Aakash and Kandemir, Mahmut Taylan and Das, Chita},
  booktitle={Proceedings of the 13th Symposium on Cloud Computing},
  pages={257--272},
  year={2022}
}


@INPROCEEDINGS{MLatFB,
  author={Wu, Carole-Jean and Brooks, David and Chen, Kevin and Chen, Douglas and Choudhury, Sy and Dukhan, Marat and Hazelwood, Kim and Isaac, Eldad and Jia, Yangqing and Jia, Bill and Leyvand, Tommer and Lu, Hao and Lu, Yang and Qiao, Lin and Reagen, Brandon and Spisak, Joe and Sun, Fei and Tulloch, Andrew and Vajda, Peter and Wang, Xiaodong and Wang, Yanghan and Wasti, Bram and Wu, Yiming and Xian, Ran and Yoo, Sungjoo and Zhang, Peizhao},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Machine Learning at Facebook: Understanding Inference at the Edge}, 
  year={2019},
  volume={},
  number={},
  pages={331-344},
  doi={10.1109/HPCA.2019.00048}}

  @inproceedings{liu2019optimizing,
  title={Optimizing $\{$CNN$\}$ Model Inference on $\{$CPUs$\}$},
  author={Liu, Yizhi and Wang, Yao and Yu, Ruofei and Li, Mu and Sharma, Vin and Wang, Yida},
  booktitle={2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  pages={1025--1040},
  year={2019}
}

@inproceedings{weng2022mlaas,
  title={$\{$MLaaS$\}$ in the Wild: Workload Analysis and Scheduling in $\{$Large-Scale$\}$ Heterogeneous $\{$GPU$\}$ Clusters},
  author={Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
  booktitle={19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
  pages={945--960},
  year={2022}
}

@inproceedings{sarma2019cash,
  title={CASH: compiler assisted hardware design for improving DRAM energy efficiency in CNN inference},
  author={Sarma, Anup and Jiang, Huaipan and Pattnaik, Ashutosh and Kotra, Jagadish and Kandemir, Mahmut Taylan and Das, Chita R},
  booktitle={Proceedings of the International Symposium on Memory Systems},
  pages={396--407},
  year={2019}
}

@inproceedings{centaur_isca2020,
  title={Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations},
  author={Hwang, Ranggi and Kim, Taehun and Kwon, Youngeun and Rhu, Minsoo},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={968--981},
  year={2020},
  organization={IEEE}
}



@INPROCEEDINGS{timothy_pf_cgo,
  author={Ainsworth, Sam and Jones, Timothy M.},
  booktitle={2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Software prefetching for indirect memory accesses}, 
  year={2017},
  volume={},
  number={},
  pages={305-317},
  doi={10.1109/CGO.2017.7863749}}

@inproceedings{lui2021understanding,
  author    = {Michael Lui and
               Yavuz Yetim and
               {\"{O}}zg{\"{u}}r {\"{O}}zkan and
               Zhuoran Zhao and
               Shin{-}Yeh Tsai and
               Carole{-}Jean Wu and
               Mark Hempstead},
  title     = {Understanding Capacity-Driven Scale-Out Neural Recommendation Inference},
  booktitle = {{IEEE} International Symposium on Performance Analysis of Systems
               and Software, {ISPASS} 2021, Stony Brook, NY, USA, March 28-30, 2021},
  pages     = {162--171},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ISPASS51385.2021.00033},
  doi       = {10.1109/ISPASS51385.2021.00033},
  timestamp = {Wed, 05 May 2021 10:18:50 +0200},
  biburl    = {https://dblp.org/rec/conf/ispass/LuiYOZTWH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ibrahim2021efficient,
  title={Efficient Cache Utilization via Model-aware Data Placement for Recommendation Models},
  author={Ibrahim, Mohamed Assem and Kayiran, Onur and Aga, Shaizeen},
  booktitle={The International Symposium on Memory Systems},
  pages={1--11},
  year={2021}
}

@article{callahan1991software,
  title={Software prefetching},
  author={Callahan, David and Kennedy, Ken and Porterfield, Allan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={19},
  number={2},
  pages={40--52},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{taram2022secsmt,
  title={$\{$SecSMT$\}$: Securing $\{$SMT$\}$ Processors against $\{$Contention-Based$\}$ Covert Channels},
  author={Taram, Mohammadkazem and Ren, Xida and Venkat, Ashish and Tullsen, Dean},
  booktitle={31st USENIX Security Symposium (USENIX Security 22)},
  pages={3165--3182},
  year={2022}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{zhou2019deep,
  title={Deep interest evolution network for click-through rate prediction},
  author={Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5941--5948},
  year={2019}
}

@inproceedings{he2017neural,
  title={Neural collaborative filtering},
  author={He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
  booktitle={Proceedings of the 26th international conference on world wide web},
  pages={173--182},
  year={2017}
}

@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}

@inproceedings{zhao2019recommending,
  title={Recommending what video to watch next: a multitask ranking system},
  author={Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
  booktitle={Proceedings of the 13th ACM Conference on Recommender Systems},
  pages={43--51},
  year={2019}
}

@article{Re2020MLPerfIB,
  title={MLPerf Inference Benchmark},
  author={Vijayarāghava Reḍḍī and Christina Miu Bing Cheng and David Kanter and Pete H Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody A. Coleman and S. Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and Julian Gardner and Itay Hubara and Sachin Satish Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and C. Kent Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem C. Wu and Ling Xu and Koichiro Yamada and Bing Yu and George Y. Yuan and Aaron Zhong and Pei Sheng Zhang and Yuchen Zhou},
  journal={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  year={2020},
  pages={446-459}
}

@article{Marr2002HyperThreadingTA,
  title={Hyper-Threading Technology Architecture and Microarchitecture.},
  author={Marr, Deborah T and Binns, Frank and Hill, David L and Hinton, Glenn and Koufaty, David A and Miller, J Alan and Upton, Michael},
  journal={Intel Technology Journal},
  volume={6},
  number={1},
  year={2002}
}


@inproceedings{covington2016deep,
  title={Deep neural networks for youtube recommendations},
  author={Covington, Paul and Adams, Jay and Sargin, Emre},
  booktitle={Proceedings of the 10th ACM conference on recommender systems},
  pages={191--198},
  year={2016}
}

@inproceedings{elkahky2015multi,
  title={A multi-view deep learning approach for cross domain user modeling in recommendation systems},
  author={Elkahky, Ali Mamdouh and Song, Yang and He, Xiaodong},
  booktitle={Proceedings of the 24th international conference on world wide web},
  pages={278--288},
  year={2015}
}

@inproceedings{wang2020m2grl,
  title={M2GRL: A multi-task multi-view graph representation learning framework for web-scale recommender systems},
  author={Wang, Menghan and Lin, Yujie and Lin, Guli and Yang, Keping and Wu, Xiao-ming},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2349--2358},
  year={2020}
}

@article{gomez2015netflix,
  title={The netflix recommender system: Algorithms, business value, and innovation},
  author={Gomez-Uribe, Carlos A and Hunt, Neil},
  journal={ACM Transactions on Management Information Systems (TMIS)},
  volume={6},
  number={4},
  pages={1--19},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kumar2022overflowing,
  title={Overflowing emerging neural network inference tasks from the GPU to the CPU on heterogeneous servers},
  author={Kumar, Adithya and Sivasubramaniam, Anand and Zhu, Timothy},
  booktitle={Proceedings of the 15th ACM International Conference on Systems and Storage},
  pages={26--39},
  year={2022}
}

@inproceedings{zhao2019aibox,
  title={Aibox: Ctr prediction model training on a single node},
  author={Zhao, Weijie and Zhang, Jingyuan and Xie, Deping and Qian, Yulei and Jia, Ronglai and Li, Ping},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={319--328},
  year={2019}
}

@inproceedings{hazelwood2018applied,
  title={Applied machine learning at facebook: A datacenter infrastructure perspective},
  author={Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and others},
  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={620--629},
  year={2018},
  organization={IEEE}
}

@article{dean2013tail,
  title={The tail at scale},
  author={Dean, Jeffrey and Barroso, Luiz Andr{\'e}},
  journal={Communications of the ACM},
  volume={56},
  number={2},
  pages={74--80},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@inproceedings{guttman2015performance,
  title={Performance and energy evaluation of data prefetching on intel xeon phi},
  author={Guttman, Diana and Kandemir, Mahmut Taylan and Arunachalamy, Meenakshi and Calina, Vlad},
  booktitle={2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  pages={288--297},
  year={2015},
  organization={IEEE}
}

@article{park2018deep,
  title={Deep learning inference in facebook data centers: Characterization, performance optimizations and hardware implications},
  author={Park, Jongsoo and Naumov, Maxim and Basu, Protonu and Deng, Summer and Kalaiah, Aravind and Khudia, Daya and Law, James and Malani, Parth and Malevich, Andrey and Nadathur, Satish and others},
  journal={arXiv preprint arXiv:1811.09886},
  year={2018}
}

@article{chui2018notes,
  title={Notes from the AI frontier: Insights from hundreds of use cases},
  author={Chui, Michael and Manyika, James and Miremadi, Mehdi and Henke, Nicolaus and Chung, Rita and Nel, Pieter and Malhotra, Sankalp},
  journal={McKinsey Global Institute},
  pages={28},
  year={2018}
}

@inproceedings{wang2021dcn,
  title={Dcn v2: Improved deep \& cross network and practical lessons for web-scale learning to rank systems},
  author={Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1785--1797},
  year={2021}
}

@incollection{wang2017deep,
  title={Deep \& cross network for ad click predictions},
  author={Wang, Ruoxi and Fu, Bin and Fu, Gang and Wang, Mingliang},
  booktitle={Proceedings of the ADKDD'17},
  pages={1--7},
  year={2017}
}

@inproceedings{lee2021merci,
  title={MERCI: efficient embedding reduction on commodity hardware via sub-query memoization},
  author={Lee, Yejin and Seo, Seong Hoon and Choi, Hyunji and Sul, Hyoung Uk and Kim, Soosung and Lee, Jae W and Ham, Tae Jun},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={302--313},
  year={2021}
}

@inproceedings{jamilan2022apt,
  title={Apt-get: Profile-guided timely software prefetching},
  author={Jamilan, Saba and Khan, Tanvir Ahmed and Ayers, Grant and Kasikci, Baris and Litz, Heiner},
  booktitle={Proceedings of the Seventeenth European Conference on Computer Systems},
  pages={747--764},
  year={2022}
}

@inproceedings{talati2021prodigy,
  title={Prodigy: Improving the memory latency of data-indirect irregular workloads using hardware-software co-design},
  author={Talati, Nishil and May, Kyle and Behroozi, Armand and Yang, Yichen and Kaszyk, Kuba and Vasiladiotis, Christos and Verma, Tarunesh and Li, Lu and Nguyen, Brandon and Sun, Jiawen and others},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={654--667},
  year={2021},
  organization={IEEE}
}

@inproceedings{basak2019dropet,
  title={Analysis and optimization of the memory hierarchy for graph processing workloads},
  author={Basak, Abanti and Li, Shuangchen and Hu, Xing and Oh, Sang Min and Xie, Xinfeng and Zhao, Li and Jiang, Xiaowei and Xie, Yuan},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={373--386},
  year={2019},
  organization={IEEE}
}

@inproceedings{naithani2021vector,
  title={Vector runahead},
  author={Naithani, Ajeya and Ainsworth, Sam and Jones, Timothy M and Eeckhout, Lieven},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={195--208},
  year={2021},
  organization={IEEE}
}

@article{daghaghi2021accelerating,
  title={Accelerating slide deep learning on modern cpus: Vectorization, quantizations, memory optimizations, and more},
  author={Daghaghi, Shabnam and Meisburger, Nicholas and Zhao, Mengnan and Shrivastava, Anshumali},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={156--166},
  year={2021}
}

@article{lee2012prefetching,
  title={When prefetching works, when it doesn’t, and why},
  author={Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={1},
  pages={1--29},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@inproceedings{firoozshahian2023mtia,
author = {Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and Hutchin, Adam and Diril, Utku and Nair, Krishnakumar and Aredestani, Ehsan K. and Schatz, Martin and Hao, Yuchen and Komuravelli, Rakesh and Ho, Kunming and Abu Asal, Sameer and Shajrawi, Joe and Quinn, Kevin and Sreedhara, Nagesh and Kansal, Pankaj and Wei, Willie and Jayaraman, Dheepak and Cheng, Linda and Chopda, Pritam and Wang, Eric and Bikumandla, Ajay and Karthik Sengottuvel, Arun and Thottempudi, Krishna and Narasimha, Ashwin and Dodds, Brian and Gao, Cao and Zhang, Jiyuan and Al-Sanabani, Mohammed and Zehtabioskuie, Ana and Fix, Jordan and Yu, Hangchen and Li, Richard and Gondkar, Kaustubh and Montgomery, Jack and Tsai, Mike and Dwarakapuram, Saritha and Desai, Sanjay and Avidan, Nili and Ramani, Poorvaja and Narayanan, Karthik and Mathews, Ajit and Gopal, Sethu and Naumov, Maxim and Rao, Vijay and Noru, Krishna and Reddy, Harikrishna and Venkatapuram, Prahlad and Bjorlin, Alexis},
title = {MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589348},
doi = {10.1145/3579371.3589348},
abstract = {Meta has traditionally relied on using CPU-based servers for running inference workloads, specifically Deep Learning Recommendation Models (DLRM), but the increasing compute and memory requirements of these models have pushed the company towards using specialized solutions such as GPUs or other hardware accelerators. This paper describes the company's effort in constructing its first silicon specifically designed for recommendation systems; it describes the accelerator architecture and platform design, the software stack for enabling and optimizing PyTorch-based models and provides an initial performance evaluation. With our emerging software stack, we have made significant progress towards reaching the same or higher efficiency as the GPU: We averaged 0.9x perf/W across various DLRMs, and benchmarks show operators such as GEMMs reaching 2x perf/W. Finally, the paper describes the lessons we learned during this journey which can improve the performance and programmability of future generations of architecture.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {80},
numpages = {13},
keywords = {programmability, performance, recommendation systems, inference, machine learning, accelerators},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@INPROCEEDINGS{gupta2020architectural,
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and Lee, Hsien-Hsin S. and Malevich, Andrey and Mudigere, Dheevatsa and Smelyanskiy, Mikhail and Xiong, Liang and Zhang, Xuan},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={The Architectural Implications of Facebook's DNN-Based Personalized Recommendation}, 
  year={2020},
  volume={},
  number={},
  pages={488-501},
  keywords={Computational modeling;Data centers;Computer architecture;Throughput;Optimization;Artificial intelligence;Videos},
  doi={10.1109/HPCA47549.2020.00047}}


@inproceedings{jain2023optimizing,
author = {Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and Das, Chita R.},
title = {Optimizing CPU Performance for Recommendation Systems At-Scale},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589112},
doi = {10.1145/3579371.3589112},
abstract = {Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {77},
numpages = {15},
keywords = {CPU, hyperthreading, prefetching, reuse distance, irregular memory accesses, embeddings, recommendation systems},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@INPROCEEDINGS{ke2020recnmp,
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S. and Li, Meng and Maher, Bert and Mudigere, Dheevatsa and Naumov, Maxim and Schatz, Martin and Smelyanskiy, Mikhail and Wang, Xiaodong and Reagen, Brandon and Wu, Carole-Jean and Hempstead, Mark and Zhang, Xuan},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing}, 
  year={2020},
  volume={},
  number={},
  pages={790-803},
  keywords={Systematics;Limiting;Scheduling algorithms;Energy conservation;Random access memory;Production;Parallel processing},
  doi={10.1109/ISCA45697.2020.00070}}


@ARTICLE{ke2021near,
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and Kim, KyungSoo and Jung, Jin and Yun, Ilkwon and Park, Sung Joo and Park, Hyunsun and Song, Joonho and Cho, Jeonghyeon and Sohn, Kyomin and Kim, Nam Sung and Lee, Hsien-Hsin S.},
  journal={IEEE Micro}, 
  title={Near-Memory Processing in Action: Accelerating Personalized Recommendation With AxDIMM}, 
  year={2022},
  volume={42},
  number={1},
  pages={116-127},
  keywords={Random access memory;Bandwidth;Throughput;Computational modeling;Hardware;Production;Field programmable gate arrays},
  doi={10.1109/MM.2021.3097700}}


@inproceedings {lai2023adaembed,
author = {Fan Lai and Wei Zhang and Rui Liu and William Tsai and Xiaohan Wei and Yuxi Hu and Sabin Devkota and Jianyu Huang and Jongsoo Park and Xing Liu and Zeliang Chen and Ellie Wen and Paul Rivera and Jie You and Chun-cheng Jason Chen and Mosharaf Chowdhury},
title = {$\{$AdaEmbed$\}$: Adaptive Embedding for $\{$Large-Scale$\}$ Recommendation Models},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {817--831},
url = {https://www.usenix.org/conference/osdi23/presentation/lai},
publisher = {USENIX Association},
month = jul
}

@inproceedings{mudigere2022software,
author = {Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and Yang, Jie (Amy) and Gao, Leon and Ivchenko, Dmytro and Basant, Aarti and Hu, Yuxi and Yang, Jiyan and Ardestani, Ehsan K. and Wang, Xiaodong and Komuravelli, Rakesh and Chu, Ching-Hsiang and Yilmaz, Serhat and Li, Huayu and Qian, Jiyuan and Feng, Zhuobo and Ma, Yinbin and Yang, Junjie and Wen, Ellie and Li, Hong and Yang, Lin and Sun, Chonglin and Zhao, Whitney and Melts, Dimitry and Dhulipala, Krishna and Kishore, KR and Graf, Tyler and Eisenman, Assaf and Matam, Kiran Kumar and Gangidi, Adi and Chen, Guoqiang Jerry and Krishnan, Manoj and Nayak, Avinash and Nair, Krishnakumar and Muthiah, Bharath and khorashadi, Mahmoud and Bhattacharya, Pallab and Lapukhov, Petr and Naumov, Maxim and Mathews, Ajit and Qiao, Lin and Smelyanskiy, Mikhail and Jia, Bill and Rao, Vijay},
title = {Software-hardware co-design for fast and scalable training of deep learning recommendation models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533727},
doi = {10.1145/3470496.3533727},
abstract = {Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\texttimes{} for training 12-trillion-parameter DLRM models deployed in production.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {993–1011},
numpages = {19},
location = {New York, New York},
series = {ISCA '22}
}


@misc{naumov2019deep,
      title={Deep Learning Recommendation Model for Personalization and Recommendation Systems}, 
      author={Maxim Naumov and Dheevatsa Mudigere and Hao-Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole-Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},
      year={2019},
      eprint={1906.00091},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}



@inproceedings{gupta2020deeprecsys,
  title={Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference},
  author={Gupta, Udit and Hsia, Samuel and Saraph, Vikram and Wang, Xiaodong and Reagen, Brandon and Wei, Gu-Yeon and Lee, Hsien-Hsin S and Brooks, David and Wu, Carole-Jean},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={982--995},
  year={2020},
  organization={IEEE}
}

@inproceedings{ke2022hercules,
  title={Hercules: Heterogeneity-aware inference serving for at-scale personalized recommendation},
  author={Ke, Liu and Gupta, Udit and Hempstead, Mark and Wu, Carole-Jean and Lee, Hsien-Hsin S and Zhang, Xuan},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={141--154},
  year={2022},
  organization={IEEE}
}


@inproceedings{mudigere2022software_old,
  title={Software-hardware co-design for fast and scalable training of deep learning recommendation models},
  author={Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and others},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={993--1011},
  year={2022}
}

@inproceedings{firoozshahian2023mtia_old,
  title={MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
  author={Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--13},
  year={2023}
}

@inproceedings{jain2023optimizing_old,
  title={Optimizing CPU Performance for Recommendation Systems At-Scale},
  author={Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}

@inproceedings{gupta2021recpipe,
  title={Recpipe: Co-designing models and hardware to jointly optimize recommendation quality and performance},
  author={Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={870--884},
  year={2021}
}

@inproceedings{hsia2020cross,
  title={Cross-stack workload characterization of deep recommendation systems},
  author={Hsia, Samuel and Gupta, Udit and Wilkening, Mark and Wu, Carole-Jean and Wei, Gu-Yeon and Brooks, David},
  booktitle={2020 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={157--168},
  year={2020},
  organization={IEEE}
}

@inproceedings{lai2023adaembed_old,
  title={$\{$AdaEmbed$\}$: Adaptive Embedding for $\{$Large-Scale$\}$ Recommendation Models},
  author={Lai, Fan and Zhang, Wei and Liu, Rui and Tsai, William and Wei, Xiaohan and Hu, Yuxi and Devkota, Sabin and Huang, Jianyu and Park, Jongsoo and Liu, Xing and others},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={817--831},
  year={2023}
}

@inproceedings{lin2022building,
  title={Building a performance model for deep learning recommendation model training on gpus},
  author={Lin, Zhongyi and Feng, Louis and Ardestani, Ehsan K and Lee, Jaewon and Lundell, John and Kim, Changkyu and Kejariwal, Arun and Owens, John D},
  booktitle={2022 IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC)},
  pages={48--58},
  year={2022},
  organization={IEEE}
}

@misc{mlperf_submissions,
  author = {MLPerf Datacenter Inference},
  year = {2024},
  title = {MLPerf Datacenter Inference 2024},
  howpublished = {"\url{https://mlcommons.org/benchmarks/inference-datacenter/}"}
}


@misc{h100_nvl,
  author = {Nvidia},
  year = {2024},
  title = {H100 NVL 96 GB},
  howpublished = {"\url{https://www.nvidia.com/en-us/data-center/h100/}"}
}



@misc{meta_h100_infra,
  author = {Meta},
  year = {2024},
  title = {Meta H100 Infrastructure 2024},
  howpublished = {"\url{https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/}"}
}


@misc{amazon_h100,
  author = {Amazon},
  year = {2024},
  title = {Amazon EC2-P5 H100 Instance},
  howpublished = {"\url{https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/}"}
}




@misc{ncu_profiling_limitation,
  author = {Nvidia},
  year = {2024},
  title = {NCU profiling limitation},
  howpublished = {"\url{https://forums.developer.nvidia.com/t/ncu-profiling-with-cache-control/246113/}"}
}

@misc{mlperf_dlrm,
  author = {MLPerf DLRM},
  year = {2024},
  title = {MLPerf DLRM},
  howpublished = {"\url{https://github.com/mlcommons/inference/tree/master/recommendation/dlrm_v2/pytorch}"}
}

@inproceedings{gupta2020architectural_old,
  title={The architectural implications of facebook's dnn-based personalized recommendation},
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={488--501},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{zhou2019deep,
  title={Deep interest evolution network for click-through rate prediction},
  author={Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5941--5948},
  year={2019}
}


@article{naumov2019deep_old,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@inproceedings{kal2021space,
  title={Space: locality-aware processing in heterogeneous memory for personalized recommendations},
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={679--691},
  year={2021},
  organization={IEEE}
}

@article{luo2024benchmarking,
  title={Benchmarking and Dissecting the Nvidia Hopper GPU Architecture},
  author={Luo, Weile and Fan, Ruibo and Li, Zeyu and Du, Dayou and Wang, Qiang and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2402.13499},
  year={2024}
}


@misc{pytorch_dlrm,
  author = {Meta},
  year = {2024},
  title = {PyTorch DLRM},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm/blob/639e3d25a59b35e6b703506a5764e611cdfe8bea/dlrm_s_pytorch.py#L590}"}
}


@misc{param_embedding,
  author = {Meta},
  year = {2024},
  title = {Embedding uBenchmark in PARAM},
  howpublished = {"\url{https://github.com/facebookresearch/param/blob/0a073429d2139b5947212863b32b222a09239cd3/train/compute/pt/pytorch_emb.py#L49}"}
}



@misc{dlrm-dataset,
  author = {Meta},
  year = {2023},
  title = {{Embedding lookup Production dataset}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm_datasets}"}
}


@misc{homogeneous-dlrm-dataset,
  author = {Rishabh Jain},
  year = {2023},
  title = {{Homogeneous Production Traces}},
  howpublished = {"\url{https://github.com/rishucoding/reproduce_isca23_cpu_DLRM_inference}"}
}


@misc{emb_bag_cuda_kernel,
  author = {PyTorch},
  year = {2024},
  title = {{Embedding Bag CUDA Kernel in PyTorch}},
  howpublished = {"\url{https://github.com/pytorch/pytorch/blob/da7db5d345a10ffb5092b26c5159f56faec1d0ea/aten/src/ATen/native/cuda/EmbeddingBag.cu#L115
}"}
}

@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}





@misc{meta_purchasing_gpu,
  author = {Meta},
  year = {2024},
  title = {{Meta's 2024 ML Infrastructure}},
  howpublished = {"\url{https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/
}"}
}


@misc{ptx_prefetch,
  author = {Nvidia},
  year = {2024},
  title = {{Parallel Thread Execution ISA Version 8.4}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prefetch-prefetchu
}"}
}

@inproceedings{lee2021merci,
  title={MERCI: efficient embedding reduction on commodity hardware via sub-query memoization},
  author={Lee, Yejin and Seo, Seong Hoon and Choi, Hyunji and Sul, Hyoung Uk and Kim, Soosung and Lee, Jae W and Ham, Tae Jun},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={302--313},
  year={2021}
}


@article{emma2005exploring,
  title={Exploring the limits of prefetching},
  author={Emma, Philip G and Hartstein, Allan and Puzak, Thomas R and Srinivasan, Viji},
  journal={IBM Journal of Research and Development},
  volume={49},
  number={1},
  pages={127--144},
  year={2005},
  publisher={IBM}
}





@misc{local_memory,
  author = {StackOverflow},
  year = {2024},
  title = {{Where does Local Memory reside?}},
  howpublished = {"\url{https://stackoverflow.com/questions/72381905/seeking-a-better-understanding-of-local-memory-in-cuda-where-does-it-live-how
}"}
}


@misc{cpu_prefetch_intrinsics,
  author = {GCC},
  year = {2024},
  title = {{Data Prefetching with GCC}},
  howpublished = {"\url{https://gcc.gnu.org/projects/prefetch.html
}"}
}

@book{falsafi2022primer,
  title={A primer on hardware prefetching},
  author={Falsafi, Babak and Wenisch, Thomas F},
  year={2022},
  publisher={Springer Nature}
}

@misc{pytorch_2.1,
  author = {PyTorch},
  year = {2024},
  title = {{PyTorch 2.1.0}},
  howpublished = {"\url{https://github.com/pytorch/pytorch/tree/v2.1.0
}"}
}


@article{lee2012prefetching,
  title={When prefetching works, when it doesn’t, and why},
  author={Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={1},
  pages={1--29},
  year={2012},
  publisher={ACM New York, NY, USA}
}


@misc{nvcc_compiler,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia’s NVCC compiler}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#opt-level-n-o
}"}
}

@misc{ncu_tool,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia’s Nsight Compute Tool}},
  howpublished = {"\url{https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html
}"}
}

@inproceedings{kal2021space,
  title={Space: locality-aware processing in heterogeneous memory for personalized recommendations},
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={679--691},
  year={2021},
  organization={IEEE}
}


@article{fu2023autoscratch,
  title={AutoScratch: ML-Optimized Cache Management for Inference-Oriented GPUs},
  author={Fu, Yaosheng and Bolotin, Evgeny and Jaleel, Aamer and Dalal, Gal and Mannor, Shie and Subag, Jacob and Korem, Noam and Behar, Michael and Nellans, David},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{zhang2023perks,
  title={PERKS: a Locality-Optimized Execution Model for Iterative Memory-bound GPU Applications},
  author={Zhang, Lingqi and Wahib, Mohamed and Chen, Peng and Meng, Jintao and Wang, Xiao and Endo, Toshio and Matsuoka, Satoshi},
  booktitle={Proceedings of the 37th International Conference on Supercomputing},
  pages={167--179},
  year={2023}
}


@inproceedings{sethi2022recshard,
  title={RecShard: statistical feature-based memory optimization for industry-scale neural recommendation},
  author={Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={344--358},
  year={2022}
}

@inproceedings{lui2021understanding,
  title={Understanding capacity-driven scale-out neural recommendation inference},
  author={Lui, Michael and Yetim, Yavuz and {\"O}zkan, {\"O}zg{\"u}r and Zhao, Zhuoran and Tsai, Shin-Yeh and Wu, Carole-Jean and Hempstead, Mark},
  booktitle={2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  pages={162--171},
  year={2021},
  organization={IEEE}
}

@article{ke2022disaggrec,
  title={DisaggRec: Architecting Disaggregated Systems for Large-Scale Personalized Recommendation},
  author={Ke, Liu and Zhang, Xuan and Lee, Benjamin and Suh, G Edward and Lee, Hsien-Hsin S},
  journal={arXiv preprint arXiv:2212.00939},
  year={2022}
}

@inproceedings{hwang2020centaur,
  title={Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations},
  author={Hwang, Ranggi and Kim, Taehun and Kwon, Youngeun and Rhu, Minsoo},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={968--981},
  year={2020},
  organization={IEEE}
}

@article{ke2021near_old,
  title={Near-memory processing in action: Accelerating personalized recommendation with axdimm},
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and others},
  journal={IEEE Micro},
  volume={42},
  number={1},
  pages={116--127},
  year={2021},
  publisher={IEEE}
}

@inproceedings{kwon2019tensordimm,
  title={Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning},
  author={Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={740--753},
  year={2019}
}

@inproceedings{ke2020recnmp_old,
  title={Recnmp: Accelerating personalized recommendation with near-memory processing},
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={790--803},
  year={2020},
  organization={IEEE}
}

@inproceedings{ye2023grace,
  title={GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference},
  author={Ye, Haojie and Vedula, Sanketh and Chen, Yuhan and Yang, Yichen and Bronstein, Alex and Dreslinski, Ronald and Mudge, Trevor and Talati, Nishil},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={282--301},
  year={2023}
}

@article{adnan2021accelerating,
  title={Accelerating recommendation system training by leveraging popular choices},
  author={Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J},
  journal={arXiv preprint arXiv:2103.00686},
  year={2021}
}

@inproceedings{kwon2022training,
  title={Training personalized recommendation systems from (GPU) scratch: Look forward not backwards},
  author={Kwon, Youngeun and Rhu, Minsoo},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={860--873},
  year={2022}
}

%%%%%%%%%%%% GPU hardware and resources


@misc{h100_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia Hopper GPU WhitePaper}},
  howpublished = {"\url{https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper
}"}
}



@misc{a100_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{A100 GPU White Paper}},
  howpublished = {"\url{https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf
}"}
}


@misc{why_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{CUDA Programming Guide: The benefits of using GPUs}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/#the-benefits-of-using-gpus
}"}
}


@inproceedings{adufu2023l2,
  title={L2 Cache Access Pattern Analysis using Static Profiling of an Application},
  author={Adufu, Theodora and Kim, Yoonhee},
  booktitle={2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)},
  pages={97--102},
  year={2023},
  organization={IEEE}
}

@inproceedings{adufu2023optimizing,
  title={Optimizing Performance Using GPU Cache Data Residency Based on Application’s Access Patterns},
  author={Adufu, Theodora and Kim, Yoonhee},
  booktitle={2023 24st Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  pages={42--47},
  year={2023},
  organization={IEEE}
}


@misc{l2_residency,
  author = {Nvidia},
  year = {2024},
  title = {{L2 cache residency control}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-l2-access-management
}"}
}

%%%%%%%%%%%% use cases of Recommendation
@misc{fb_recsys,
  author = {Meta},
  year = {2024},
  title = {{Facebook Recommendation System}},
  howpublished = {"\url{https://ai.meta.com/blog/ai-unconnected-content-recommendations-facebook-instagram/
}"}
}

@misc{instagram_recsys,
  author = {Meta},
  year = {2024},
  title = {{Instagram Recommendation System}},
  howpublished = {"\url{https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/
}"}
}

@misc{tiktok_recsys,
  author = {TikTok},
  year = {2024},
  title = {{TikTok Recommendation System}},
  howpublished = {"\url{https://www.tiktok.com/transparency/en-us/recommendation-system/
}"}
}

@misc{amazon_recsys,
  author = {Amazon},
  year = {2024},
  title = {{Amazon Recommendation System}},
  howpublished = {"\url{https://aws.amazon.com/personalize/ 
}"}
}

@misc{netflix_recsys,
  author = {Netflix},
  year = {2024},
  title = {{Netflix Recommendation System}},
  howpublished = {"\url{https://research.netflix.com/research-area/recommendations
}"}
}

@misc{hulu_recsys,
  author = {Hulu},
  year = {2024},
  title = {{Hulu Recommendation System}},
  howpublished = {"\url{https://help.hulu.com/article/hulu-personalized-recommendations#:~:text=While%20you're%20looking%20for,get%20to%20know%20you%20better.
}"}
}


@misc{ebay_recsys,
  author = {eBay},
  year = {2024},
  title = {{eBay Recommendation System}},
  howpublished = {"\url{https://innovation.ebayinc.com/tech/engineering/building-a-deep-learning-based-retrieval-system-for-personalized-recommendations/
}"}
}

@misc{alibaba_recsys,
  author = {AliBaba},
  year = {2024},
  title = {{eBay Recommendation System}},
  howpublished = {"\url{https://www.alibabacloud.com/blog/getting-started-with-recommendation-system_597740
}"}
}




%%%%%%%%%%%% related works on prefetching on GPUs

@inproceedings{jog2013orchestrated,
  title={Orchestrated scheduling and prefetching for GPGPUs},
  author={Jog, Adwait and Kayiran, Onur and Mishra, Asit K and Kandemir, Mahmut T and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R},
  booktitle={Proceedings of the 40th Annual International Symposium on Computer Architecture},
  pages={332--343},
  year={2013}
}

@inproceedings{sethia2013apogee,
  title={APOGEE: Adaptive prefetching on GPUs for energy efficiency},
  author={Sethia, Ankit and Dasika, Ganesh and Samadi, Mehrzad and Mahlke, Scott},
  booktitle={Proceedings of the 22nd international conference on Parallel architectures and compilation techniques},
  pages={73--82},
  year={2013},
  organization={IEEE}
}

@article{oh2018adaptive,
  title={Adaptive cooperation of prefetching and warp scheduling on gpus},
  author={Oh, Yunho and Kim, Keunsoo and Yoon, Myung Kuk and Park, Jong Hyun and Park, Yongjun and Annavaram, Murali and Ro, Won Woo},
  journal={IEEE Transactions on Computers},
  volume={68},
  number={4},
  pages={609--616},
  year={2018},
  publisher={IEEE}
}

@inproceedings{wu2011pacman,
  title={PACMan: prefetch-aware cache management for high performance caching},
  author={Wu, Carole-Jean and Jaleel, Aamer and Martonosi, Margaret and Steely Jr, Simon C and Emer, Joel},
  booktitle={Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={442--453},
  year={2011}
}


%%%%%%%%%%%% related works on multi-threading (warp level parallelism) on GPUs

@article{jog2013owl,
  title={OWL: cooperative thread array aware scheduling techniques for improving GPGPU performance},
  author={Jog, Adwait and Kayiran, Onur and Chidambaram Nachiappan, Nachiappan and Mishra, Asit K and Kandemir, Mahmut T and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R},
  journal={ACM SIGPLAN Notices},
  volume={48},
  number={4},
  pages={395--406},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kayiran2013neither,
  title={Neither more nor less: Optimizing thread-level parallelism for GPGPUs},
  author={Kay{\i}ran, Onur and Jog, Adwait and Kandemir, Mahmut T and Das, Chita R},
  booktitle={Proceedings of the 22nd international conference on Parallel architectures and compilation techniques},
  pages={157--166},
  year={2013},
  organization={IEEE}
}

@inproceedings{song2023ugache,
  title={UGACHE: A Unified GPU Cache for Embedding-based Deep Learning},
  author={Song, Xiaoniu and Zhang, Yiwen and Chen, Rong and Chen, Haibo},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={627--641},
  year={2023}
}

@article{yuan2023everest,
  title={Everest: GPU-Accelerated System For Mining Temporal Motifs},
  author={Yuan, Yichao and Ye, Haojie and Kaza, Sanketh Vedula Wynn and Talati, Nishil},
  journal={arXiv preprint arXiv:2310.02800},
  year={2023}
}

@inproceedings{sethia2015mascar,
  title={Mascar: Speeding up GPU warps by reducing memory pitstops},
  author={Sethia, Ankit and Jamshidi, D Anoushe and Mahlke, Scott},
  booktitle={2015 IEEE 21st International symposium on high performance computer architecture (HPCA)},
  pages={174--185},
  year={2015},
  organization={IEEE}
}


@misc{A100,
title={{NVIDIA A100 Tensor Core GPU.}},
note={\url{https://www.nvidia.com/en-us/data-center/a100/}},
year={2024},
}


@misc{H100,
title={{NVIDIA H100 Tensor Core GPU}},
note={\url{https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/}},
year=2024
}

%%%%%%%%%%%% related works on register file virtualization

@inproceedings{jeon2015gpu,
  title={GPU register file virtualization},
  author={Jeon, Hyeran and Ravi, Gokul Subramanian and Kim, Nam Sung and Annavaram, Murali},
  booktitle={Proceedings of the 48th International Symposium on Microarchitecture},
  pages={420--432},
  year={2015}
}

@article{voitsechov2018software,
  title={Software-directed techniques for improved gpu register file utilization},
  author={Voitsechov, Dani and Zulfiqar, Arslan and Stephenson, Mark and Gebhart, Mark and Keckler, Stephen W},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={15},
  number={3},
  pages={1--23},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{oh2018finereg,
  title={FineReg: Fine-grained register file management for augmenting GPU throughput},
  author={Oh, Yunho and Yoon, Myung Kuk and Song, William J and Ro, Won Woo},
  booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={364--376},
  year={2018},
  organization={IEEE}
}


@misc{nvidia_nsight_systems,
title={{NVIDIA Nsight Systems}},
note={\url{https://docs.nvidia.com/nsight-systems/UserGuide/index.html}},
year=2024
}

@misc{nvcc_maxrregcount,
title={{nvcc maxrregcount flag}},
note={\url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#maxrregcount-amount-maxrregcount}},
year=2024
}


@misc{gpgpu-sim_scoreboard,
title={{Scoreboarding in GPUs}},
note={\url{http://gpgpu-sim.org/manual/index.php/Main_Page#Scoreboard}},
year=2024
}



@inproceedings{jha2024mem,
  title={Mem-Rec: Memory Efficient Recommendation System using Alternative Representation},
  author={Jha, Gopi Krishna and Thomas, Anthony and Jain, Nilesh and Gobriel, Sameh and Rosing, Tajana and Iyer, Ravi},
  booktitle={Asian Conference on Machine Learning},
  pages={518--533},
  year={2024},
  organization={PMLR}
}

@inproceedings{matam2024quickupdate,
  title={$\{$QuickUpdate$\}$: a $\{$Real-Time$\}$ Personalization System for $\{$Large-Scale$\}$ Recommendation Models},
  author={Matam, Kiran Kumar and Ramezani, Hani and Wang, Fan and Chen, Zeliang and Dong, Yue and Ding, Maomao and Zhao, Zhiwei and Zhang, Zhengyu and Wen, Ellie and Eisenman, Assaf},
  booktitle={21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={731--744},
  year={2024}
}

@article{zha2022dreamshard,
  title={Dreamshard: Generalizable embedding table placement for recommender systems},
  author={Zha, Daochen and Feng, Louis and Tan, Qiaoyu and Liu, Zirui and Lai, Kwei-Herng and Bhushanam, Bhargav and Tian, Yuandong and Kejariwal, Arun and Hu, Xia},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15190--15203},
  year={2022}
}

@book{lamport94,
 author = "Leslie Lamport",
 title = "{\LaTeX: A Document Preparation System}",
 year = "1994",
 publisher = "Addison-Wesley",
 edition = "2nd",
 address = "Reading, Massachusetts"
}

@inproceedings{nicepaper1,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2",
  title = "A Very Nice Paper To Cite",
  year = "2016",
  booktitle = "Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@inproceedings{mutlu2003runahead,
  title={Runahead execution: An alternative to very large instruction windows for out-of-order processors},
  author={Mutlu, Onur and Stark, Jared and Wilkerson, Chris and Patt, Yale N},
  booktitle={The Ninth International Symposium on High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings.},
  pages={129--140},
  year={2003},
  organization={IEEE}
}

@inproceedings{sethi2022recshard,
  title={RecShard: statistical feature-based memory optimization for industry-scale neural recommendation},
  author={Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={344--358},
  year={2022}
}

@inproceedings{zha2022autoshard,
  title={Autoshard: Automated embedding table sharding for recommender systems},
  author={Zha, Daochen and Feng, Louis and Bhushanam, Bhargav and Choudhary, Dhruv and Nie, Jade and Tian, Yuandong and Chae, Jay and Ma, Yinbin and Kejariwal, Arun and Hu, Xia},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={4461--4471},
  year={2022}
}

@article{zha2022dreamshard,
  title={Dreamshard: Generalizable embedding table placement for recommender systems},
  author={Zha, Daochen and Feng, Louis and Tan, Qiaoyu and Liu, Zirui and Lai, Kwei-Herng and Bhushanam, Bhargav and Tian, Yuandong and Kejariwal, Arun and Hu, Xia},
  journal={arXiv preprint arXiv:2210.02023},
  year={2022}
}

@inproceedings{acun2021understanding,
  title={Understanding training efficiency of deep learning recommendation models at scale},
  author={Acun, Bilge and Murphy, Matthew and Wang, Xiaodong and Nie, Jade and Wu, Carole-Jean and Hazelwood, Kim},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={802--814},
  year={2021},
  organization={IEEE}
}
% https://github.com/facebookresearch/dlrm_datasets

@inproceedings{mudigere2022software,
  title={Software-hardware co-design for fast and scalable training of deep learning recommendation models},
  author={Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and others},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={993--1011},
  year={2022}
}

@article{ardalani2022understanding,
  title={Understanding Scaling Laws for Recommendation Models},
  author={Ardalani, Newsha and Wu, Carole-Jean and Chen, Zeliang and Bhushanam, Bhargav and Aziz, Adnan},
  journal={arXiv preprint arXiv:2208.08489},
  year={2022}
}

@inproceedings{isca2022_dsi,
  title={Understanding data storage and ingestion for large-scale deep recommendation model training: industrial product},
  author={Zhao, Mark and Agarwal, Niket and Basant, Aarti and Gedik, Bu{\u{g}}ra and Pan, Satadru and Ozdal, Mustafa and Komuravelli, Rakesh and Pan, Jerry and Bao, Tianshu and Lu, Haowei and others},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={1042--1057},
  year={2022}
}

@inproceedings{deeprecsys,
  title={Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference},
  author={Gupta, Udit and Hsia, Samuel and Saraph, Vikram and Wang, Xiaodong and Reagen, Brandon and Wei, Gu-Yeon and Lee, Hsien-Hsin S and Brooks, David and Wu, Carole-Jean},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={982--995},
  year={2020},
  organization={IEEE}
}

@inproceedings{udit_hpca2020,
  title={The architectural implications of facebook's dnn-based personalized recommendation},
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={488--501},
  year={2020},
  organization={IEEE}
}


@inproceedings{iiswc2020,
  title={Cross-stack workload characterization of deep recommendation systems},
  author={Hsia, Samuel and Gupta, Udit and Wilkening, Mark and Wu, Carole-Jean and Wei, Gu-Yeon and Brooks, David},
  booktitle={2020 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={157--168},
  year={2020},
  organization={IEEE}
}

@inproceedings{recpipe2021,
  title={RecPipe: Co-designing models and hardware to jointly optimize recommendation quality and performance},
  author={Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={870--884},
  year={2021}
}

@misc{kaggle,
  author = {Meta Research},
  year = {2022},
  title = {{DLRM configuration for Criteo Kaggle Training}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm/blob/main/bench/dlrm_s_criteo_kaggle.sh}"},
  note = {}
}

@misc{icelake_anandtech,
  author = {Dr. Ian Cutress},
  year = {2019},
  title = {{Analysis of Icelake architecture: }},
  howpublished = {"\url{https://www.anandtech.com/show/14514/examining-intels-ice-lake-microarchitecture-and-sunny-cove/3}"},
  note = {}
}

@misc{icelake_silver,
  author = {Intel},
  year = {2021},
  title = {{Intel Xeon Silver 4314 Processor}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/products/sku/215269/intel-xeon-silver-4314-processor-24m-cache-2-40-ghz/specifications.html}"},
  note = {}
}

@misc{skylake_gold,
  author = {Intel},
  year = {2017},
  title = {{Intel Xeon Gold 6136 Processor}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/products/sku/120479/intel-xeon-gold-6136-processor-24-75m-cache-3-00-ghz/specifications.html}"},
  note = {}
}

@misc{spr_platinum,
  author = {Intel},
  year = {2023},
  title = {{Intel Xeon Platinum 8480+ Processor}},
  howpublished = {"\url{https://ark.intel.com/content/www/us/en/ark/products/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz.html}"},
  note = {}
}



@misc{epyc7763,
  author = {AMD},
  year = {2021},
  title = {{AMD EPYC 7763}},
  howpublished = {"\url{https://www.amd.com/en/products/cpu/amd-epyc-7763}"},
  note = {}
}


@misc{hw_pref_intel,
  author = {Intel},
  year = {2022},
  title = {{Hardware Prefetchers in Intel CPU}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html}"},
  note = {}
}

@misc{ipex_mlperf,
  author = {MLPerf},
  year = {2022},
  title = {{MLPerf benchmarking on CPUs using Intel Extension for PyTorch}},
  howpublished = {"\url{https://github.com/mlcommons/inference_results_v2.1/tree/master/closed/Intel/code/dlrm-99.9/pytorch-cpu}"},
  note = {}
}

@misc{ipex,
  author = {Intel},
  year = {2022},
  title = {{Intel Extension for PyTorch}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html}"},
  note = {}
}

@misc{vtune,
  author = {Intel},
  year = {2022},
  title = {{Intel VTune Profiler}},
  howpublished = {"\url{https://github.com/intel/intel-extension-for-pytorch}"},
  note = {}
}

@misc{pintool,
  author = {Intel},
  year = {2022},
  title = {{Pin - A Dynamic Binary Instrumentation Tool}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html}"},
  note = {}
}

@misc{amd-milan-x,
  author = {AMD},
  year = {2022},
  title = {{AMD EPYC 7773X}},
  author = {},
  howpublished = {"\url{https://www.amd.com/en/products/cpu/amd-epyc-7773x}"},
  note = {}
}

@inproceedings{hassan2007_lru_stack,
  title={Synthetic trace-driven simulation of cache memory},
  author={Hassan, Rahman and Harris, Antony and Topham, Nigel and Efthymiou, Aris},
  booktitle={21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07)},
  volume={1},
  pages={764--771},
  year={2007},
  organization={IEEE}
}


@misc{rmc3,
  author = {Udit Gupta},
  year = {2020},
  title = {{DLRM configuration in DeepRecSys RMC3}},
  howpublished = {"\url{https://github.com/harvard-acc/DeepRecSys/blob/master/models/configs/dlrm_rm3.json}"},
  note = {}
}



@misc{mlperf,
  author = {MLPerf},
  year = {2022},
  title = {{MLPerf Datacenter Inference Submissions v2.1}},
  howpublished = {"\url{https://mlcommons.org/en/inference-datacenter-21/}"},
  note = {}
}
@misc{prod_dataset,
  author = {Meta Research},
  year = {2021},
  title = {{Embedding Lookup Synthetic Dataset}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm_datasets}"},
  note = {}
}

@misc{emb_bag_operator,
  author = {PyTorch},
  year = {2022},
  title = {{Embedding Bag Operator}},
  howpublished = {"\url{https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html}"},
  note = {}
}

@misc{mm_prefetch,
  author = {Intel},
  year = {2022},
  title = {{Prefetch Intrinsic}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_prefetch}"},
  note = {}
}

@misc{cascade_lake,
  author = {Intel},
  year = {2019},
  title = {{Intel Cascade Lake Architecture}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/products/platforms/details/cascade-lake.html}"},
  note = {}
}

@misc{ice_lake,
  author = {Intel},
  year = {2021},
  title = {{Intel Ice Lake Architecture}},
  howpublished = {"\url{https://ark.intel.com/content/www/us/en/ark/products/codename/74979/products-formerly-ice-lake.html}"},
  note = {}
}

@misc{icc-prefetch,
  author = {Intel},
  year = {2022},
  title = {{Intel C++ Compiler Classic Developer Guide and Reference}},
  howpublished = {"\url{https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/compiler-options/advanced-optimization-options/qopt-prefetch-qopt-prefetch.html}"},
  note = {}
}

@misc{gcc-prefetch,
  author = {GCC},
  year = {2022},
  title = {{GCC Data Prefetch Support}},
  howpublished = {"\url{https://gcc.gnu.org/projects/prefetch.html}"},
  note = {}
}

@article{naumov_dlrm,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}


@inproceedings{recssd,
  title={RecSSD: near data processing for solid state drive based recommendation inference},
  author={Wilkening, Mark and Gupta, Udit and Hsia, Samuel and Trippel, Caroline and Wu, Carole-Jean and Brooks, David and Wei, Gu-Yeon},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={717--729},
  year={2021}
}

@inproceedings{hercules_hpca2022,
  title={Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation},
  author={Ke, Liu and Gupta, Udit and Hempsteadis, Mark and Wu, Carole-Jean and Lee, Hsien-Hsin S and Zhang, Xuan},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={141--144},
  year={2022},
  organization={IEEE}
}

@inproceedings{recnmp_isca2020,
  title={Recnmp: Accelerating personalized recommendation with near-memory processing},
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={790--803},
  year={2020},
  organization={IEEE}
}

@inproceedings{space_isca2021,
  title={SPACE: locality-aware processing in heterogeneous memory for personalized recommendations},
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={679--691},
  year={2021},
  organization={IEEE}
}

@inproceedings{TensorDIMM_micro2019,
  title={Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning},
  author={Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={740--753},
  year={2019}
}

@inproceedings{scratchpipe_isca2022,
  title={Training personalized recommendation systems from (GPU) scratch: look forward not backwards},
  author={Kwon, Youngeun and Rhu, Minsoo},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={860--873},
  year={2022}
}


@article{eisenman201bandana,
  title={Bandana: Using non-volatile memory for storing deep learning models},
  author={Eisenman, Assaf and Naumov, Maxim and Gardner, Darryl and Smelyanskiy, Misha and Pupyrev, Sergey and Hazelwood, Kim and Cidon, Asaf and Katti, Sachin},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={40--52},
  year={2019}
}

@article{Balasubramanian2021cDLRMLA,
  title={cDLRM: Look Ahead Caching for Scalable Training of Recommendation Models},
  author={Keshav Balasubramanian and Abdulla Alshabanah and Joshua D. Choe and Murali Annavaram},
  journal={Proceedings of the 15th ACM Conference on Recommender Systems},
  year={2021}
}

@inproceedings{Desai2022RandomOB,
  title={Random Offset Block Embedding (ROBE) for compressed embedding tables in deep learning recommendation systems},
  author={Aditya Desai and Li Chou and Anshumali Shrivastava},
  booktitle={MLSys},
  year={2022}
}

@inproceedings{gong2022graphite,
  title={Graphite: optimizing graph neural networks on CPUs through cooperative software-hardware techniques.},
  author={Gong, Zhangxiaowen and Ji, Houxiang and Yao, Yao and Fletcher, Christopher W and Hughes, Christopher J and Torrellas, Josep},
  booktitle={ISCA},
  pages={916--931},
  year={2022}
}

@inproceedings{tullsen2001handling,
  title={Handling long-latency loads in a simultaneous multithreading processor},
  author={Tullsen, Dean M and Brown, Jeffery A},
  booktitle={Proceedings. 34th ACM/IEEE International Symposium on Microarchitecture. MICRO-34},
  pages={318--327},
  year={2001},
  organization={IEEE}
}

@inproceedings{tullsen1995simultaneous,
  title={Simultaneous multithreading: Maximizing on-chip parallelism},
  author={Tullsen, Dean M and Eggers, Susan J and Levy, Henry M},
  booktitle={Proceedings of the 22nd annual international symposium on Computer architecture},
  pages={392--403},
  year={1995}
}

@inproceedings{kraken,
  title={Kraken: Adaptive container provisioning for deploying dynamic dags in serverless platforms},
  author={Bhasi, Vivek M and Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and Mishra, Cyan Subhra and Kandemir, Mahmut Taylan and Das, Chita},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={153--167},
  year={2021}
}

@inproceedings{grandslam,
  title={Grandslam: Guaranteeing slas for jobs in microservices execution frameworks},
  author={Kannan, Ram Srivatsa and Subramanian, Lavanya and Raju, Ashwin and Ahn, Jeongseob and Mars, Jason and Tang, Lingjia},
  booktitle={Proceedings of the Fourteenth EuroSys Conference 2019},
  pages={1--16},
  year={2019}
}

@inproceedings{atoll,
  title={Atoll: A scalable low-latency serverless platform},
  author={Singhvi, Arjun and Balasubramanian, Arjun and Houck, Kevin and Shaikh, Mohammed Danish and Venkataraman, Shivaram and Akella, Aditya},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={138--152},
  year={2021}
}

@inproceedings{cypress,
  title={Cypress: input size-sensitive container provisioning and request scheduling for serverless platforms},
  author={Bhasi, Vivek M and Gunasekaran, Jashwant Raj and Sharma, Aakash and Kandemir, Mahmut Taylan and Das, Chita},
  booktitle={Proceedings of the 13th Symposium on Cloud Computing},
  pages={257--272},
  year={2022}
}


@INPROCEEDINGS{MLatFB,
  author={Wu, Carole-Jean and Brooks, David and Chen, Kevin and Chen, Douglas and Choudhury, Sy and Dukhan, Marat and Hazelwood, Kim and Isaac, Eldad and Jia, Yangqing and Jia, Bill and Leyvand, Tommer and Lu, Hao and Lu, Yang and Qiao, Lin and Reagen, Brandon and Spisak, Joe and Sun, Fei and Tulloch, Andrew and Vajda, Peter and Wang, Xiaodong and Wang, Yanghan and Wasti, Bram and Wu, Yiming and Xian, Ran and Yoo, Sungjoo and Zhang, Peizhao},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Machine Learning at Facebook: Understanding Inference at the Edge}, 
  year={2019},
  volume={},
  number={},
  pages={331-344},
  doi={10.1109/HPCA.2019.00048}}

  @inproceedings{liu2019optimizing,
  title={Optimizing $\{$CNN$\}$ Model Inference on $\{$CPUs$\}$},
  author={Liu, Yizhi and Wang, Yao and Yu, Ruofei and Li, Mu and Sharma, Vin and Wang, Yida},
  booktitle={2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  pages={1025--1040},
  year={2019}
}

@inproceedings{weng2022mlaas,
  title={$\{$MLaaS$\}$ in the Wild: Workload Analysis and Scheduling in $\{$Large-Scale$\}$ Heterogeneous $\{$GPU$\}$ Clusters},
  author={Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
  booktitle={19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
  pages={945--960},
  year={2022}
}

@inproceedings{sarma2019cash,
  title={CASH: compiler assisted hardware design for improving DRAM energy efficiency in CNN inference},
  author={Sarma, Anup and Jiang, Huaipan and Pattnaik, Ashutosh and Kotra, Jagadish and Kandemir, Mahmut Taylan and Das, Chita R},
  booktitle={Proceedings of the International Symposium on Memory Systems},
  pages={396--407},
  year={2019}
}

@inproceedings{centaur_isca2020,
  title={Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations},
  author={Hwang, Ranggi and Kim, Taehun and Kwon, Youngeun and Rhu, Minsoo},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={968--981},
  year={2020},
  organization={IEEE}
}



@INPROCEEDINGS{timothy_pf_cgo,
  author={Ainsworth, Sam and Jones, Timothy M.},
  booktitle={2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Software prefetching for indirect memory accesses}, 
  year={2017},
  volume={},
  number={},
  pages={305-317},
  doi={10.1109/CGO.2017.7863749}}

@inproceedings{lui2021understanding,
  author    = {Michael Lui and
               Yavuz Yetim and
               {\"{O}}zg{\"{u}}r {\"{O}}zkan and
               Zhuoran Zhao and
               Shin{-}Yeh Tsai and
               Carole{-}Jean Wu and
               Mark Hempstead},
  title     = {Understanding Capacity-Driven Scale-Out Neural Recommendation Inference},
  booktitle = {{IEEE} International Symposium on Performance Analysis of Systems
               and Software, {ISPASS} 2021, Stony Brook, NY, USA, March 28-30, 2021},
  pages     = {162--171},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ISPASS51385.2021.00033},
  doi       = {10.1109/ISPASS51385.2021.00033},
  timestamp = {Wed, 05 May 2021 10:18:50 +0200},
  biburl    = {https://dblp.org/rec/conf/ispass/LuiYOZTWH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ibrahim2021efficient,
  title={Efficient Cache Utilization via Model-aware Data Placement for Recommendation Models},
  author={Ibrahim, Mohamed Assem and Kayiran, Onur and Aga, Shaizeen},
  booktitle={The International Symposium on Memory Systems},
  pages={1--11},
  year={2021}
}

@article{callahan1991software,
  title={Software prefetching},
  author={Callahan, David and Kennedy, Ken and Porterfield, Allan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={19},
  number={2},
  pages={40--52},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{taram2022secsmt,
  title={$\{$SecSMT$\}$: Securing $\{$SMT$\}$ Processors against $\{$Contention-Based$\}$ Covert Channels},
  author={Taram, Mohammadkazem and Ren, Xida and Venkat, Ashish and Tullsen, Dean},
  booktitle={31st USENIX Security Symposium (USENIX Security 22)},
  pages={3165--3182},
  year={2022}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{zhou2019deep,
  title={Deep interest evolution network for click-through rate prediction},
  author={Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5941--5948},
  year={2019}
}

@inproceedings{he2017neural,
  title={Neural collaborative filtering},
  author={He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
  booktitle={Proceedings of the 26th international conference on world wide web},
  pages={173--182},
  year={2017}
}

@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}

@inproceedings{zhao2019recommending,
  title={Recommending what video to watch next: a multitask ranking system},
  author={Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
  booktitle={Proceedings of the 13th ACM Conference on Recommender Systems},
  pages={43--51},
  year={2019}
}

@article{Re2020MLPerfIB,
  title={MLPerf Inference Benchmark},
  author={Vijayarāghava Reḍḍī and Christina Miu Bing Cheng and David Kanter and Pete H Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody A. Coleman and S. Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and Julian Gardner and Itay Hubara and Sachin Satish Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and C. Kent Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem C. Wu and Ling Xu and Koichiro Yamada and Bing Yu and George Y. Yuan and Aaron Zhong and Pei Sheng Zhang and Yuchen Zhou},
  journal={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  year={2020},
  pages={446-459}
}

@article{Marr2002HyperThreadingTA,
  title={Hyper-Threading Technology Architecture and Microarchitecture.},
  author={Marr, Deborah T and Binns, Frank and Hill, David L and Hinton, Glenn and Koufaty, David A and Miller, J Alan and Upton, Michael},
  journal={Intel Technology Journal},
  volume={6},
  number={1},
  year={2002}
}


@inproceedings{covington2016deep,
  title={Deep neural networks for youtube recommendations},
  author={Covington, Paul and Adams, Jay and Sargin, Emre},
  booktitle={Proceedings of the 10th ACM conference on recommender systems},
  pages={191--198},
  year={2016}
}

@inproceedings{elkahky2015multi,
  title={A multi-view deep learning approach for cross domain user modeling in recommendation systems},
  author={Elkahky, Ali Mamdouh and Song, Yang and He, Xiaodong},
  booktitle={Proceedings of the 24th international conference on world wide web},
  pages={278--288},
  year={2015}
}

@inproceedings{wang2020m2grl,
  title={M2GRL: A multi-task multi-view graph representation learning framework for web-scale recommender systems},
  author={Wang, Menghan and Lin, Yujie and Lin, Guli and Yang, Keping and Wu, Xiao-ming},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2349--2358},
  year={2020}
}

@article{gomez2015netflix,
  title={The netflix recommender system: Algorithms, business value, and innovation},
  author={Gomez-Uribe, Carlos A and Hunt, Neil},
  journal={ACM Transactions on Management Information Systems (TMIS)},
  volume={6},
  number={4},
  pages={1--19},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kumar2022overflowing,
  title={Overflowing emerging neural network inference tasks from the GPU to the CPU on heterogeneous servers},
  author={Kumar, Adithya and Sivasubramaniam, Anand and Zhu, Timothy},
  booktitle={Proceedings of the 15th ACM International Conference on Systems and Storage},
  pages={26--39},
  year={2022}
}

@inproceedings{zhao2019aibox,
  title={Aibox: Ctr prediction model training on a single node},
  author={Zhao, Weijie and Zhang, Jingyuan and Xie, Deping and Qian, Yulei and Jia, Ronglai and Li, Ping},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={319--328},
  year={2019}
}

@inproceedings{hazelwood2018applied,
  title={Applied machine learning at facebook: A datacenter infrastructure perspective},
  author={Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and others},
  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={620--629},
  year={2018},
  organization={IEEE}
}

@article{dean2013tail,
  title={The tail at scale},
  author={Dean, Jeffrey and Barroso, Luiz Andr{\'e}},
  journal={Communications of the ACM},
  volume={56},
  number={2},
  pages={74--80},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@inproceedings{guttman2015performance,
  title={Performance and energy evaluation of data prefetching on intel xeon phi},
  author={Guttman, Diana and Kandemir, Mahmut Taylan and Arunachalamy, Meenakshi and Calina, Vlad},
  booktitle={2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  pages={288--297},
  year={2015},
  organization={IEEE}
}

@article{park2018deep,
  title={Deep learning inference in facebook data centers: Characterization, performance optimizations and hardware implications},
  author={Park, Jongsoo and Naumov, Maxim and Basu, Protonu and Deng, Summer and Kalaiah, Aravind and Khudia, Daya and Law, James and Malani, Parth and Malevich, Andrey and Nadathur, Satish and others},
  journal={arXiv preprint arXiv:1811.09886},
  year={2018}
}

@article{chui2018notes,
  title={Notes from the AI frontier: Insights from hundreds of use cases},
  author={Chui, Michael and Manyika, James and Miremadi, Mehdi and Henke, Nicolaus and Chung, Rita and Nel, Pieter and Malhotra, Sankalp},
  journal={McKinsey Global Institute},
  pages={28},
  year={2018}
}

@inproceedings{wang2021dcn,
  title={Dcn v2: Improved deep \& cross network and practical lessons for web-scale learning to rank systems},
  author={Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1785--1797},
  year={2021}
}

@incollection{wang2017deep,
  title={Deep \& cross network for ad click predictions},
  author={Wang, Ruoxi and Fu, Bin and Fu, Gang and Wang, Mingliang},
  booktitle={Proceedings of the ADKDD'17},
  pages={1--7},
  year={2017}
}

@inproceedings{lee2021merci,
  title={MERCI: efficient embedding reduction on commodity hardware via sub-query memoization},
  author={Lee, Yejin and Seo, Seong Hoon and Choi, Hyunji and Sul, Hyoung Uk and Kim, Soosung and Lee, Jae W and Ham, Tae Jun},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={302--313},
  year={2021}
}

@inproceedings{jamilan2022apt,
  title={Apt-get: Profile-guided timely software prefetching},
  author={Jamilan, Saba and Khan, Tanvir Ahmed and Ayers, Grant and Kasikci, Baris and Litz, Heiner},
  booktitle={Proceedings of the Seventeenth European Conference on Computer Systems},
  pages={747--764},
  year={2022}
}

@inproceedings{talati2021prodigy,
  title={Prodigy: Improving the memory latency of data-indirect irregular workloads using hardware-software co-design},
  author={Talati, Nishil and May, Kyle and Behroozi, Armand and Yang, Yichen and Kaszyk, Kuba and Vasiladiotis, Christos and Verma, Tarunesh and Li, Lu and Nguyen, Brandon and Sun, Jiawen and others},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={654--667},
  year={2021},
  organization={IEEE}
}

@inproceedings{basak2019dropet,
  title={Analysis and optimization of the memory hierarchy for graph processing workloads},
  author={Basak, Abanti and Li, Shuangchen and Hu, Xing and Oh, Sang Min and Xie, Xinfeng and Zhao, Li and Jiang, Xiaowei and Xie, Yuan},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={373--386},
  year={2019},
  organization={IEEE}
}

@inproceedings{naithani2021vector,
  title={Vector runahead},
  author={Naithani, Ajeya and Ainsworth, Sam and Jones, Timothy M and Eeckhout, Lieven},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={195--208},
  year={2021},
  organization={IEEE}
}

@article{daghaghi2021accelerating,
  title={Accelerating slide deep learning on modern cpus: Vectorization, quantizations, memory optimizations, and more},
  author={Daghaghi, Shabnam and Meisburger, Nicholas and Zhao, Mengnan and Shrivastava, Anshumali},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={156--166},
  year={2021}
}

@article{lee2012prefetching,
  title={When prefetching works, when it doesn’t, and why},
  author={Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={1},
  pages={1--29},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@misc{recsys_market_growth,
  author = {GVR},
  year = {2024},
  title = {Recommendation Engine Market Size},
  howpublished = {"\url{https://www.grandviewresearch.com/industry-analysis/recommendation-engine-market-report}"}
}

@misc{blackwell_gpu,
  author = {Nvidia},
  year = {2024},
  title = {BlackWell GPU Technical Brief},
  howpublished = {"\url{https://cdn.prod.website-files.com/61dda201f29b7efc52c5fbaf/6602ea9d0ce8cb73fb6de87f_nvidia-blackwell-architecture-technical-brief.pdf}"}
}

@inproceedings{bhasi2024towards,
  title={Towards SLO-Compliant and Cost-Effective Serverless Computing on Emerging GPU Architectures},
  author={Bhasi, Vivek M and Sharma, Aakash and Jain, Rishabh and Gunasekaran, Jashwant Raj and Pattnaik, Ashutosh and Kandemir, Mahmut Taylan and Das, Chita},
  booktitle={Proceedings of the 25th International Middleware Conference},
  pages={211--224},
  year={2024}
}

@inproceedings{jain2024pushing,
  title={Pushing the Performance Envelope of DNN-based Recommendation Systems Inference on GPUs},
  author={Jain, Rishabh and Bhasi, Vivek M and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut T and Das, Chita R},
  booktitle={2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={1217--1232},
  year={2024},
  organization={IEEE}
}


@misc{nvidia_dlrm,
  author = {Nvidia},
  year = {2024},
  title = {Nvidia Deep Learning Examples},
  howpublished = {"\url{https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Recommendation/DLRM/README.md}"}
}



@misc{meta_q1_2024,
  author = {Meta},
  year = {2024},
  title = {Meta Q1 2024 Financial Report},
  howpublished = {"\url{https://s21.q4cdn.com/399680738/files/doc_financials/2024/q1/Earnings-Presentation-Q1-2024.pdf}"}
}

@misc{google_q1_2024,
  author = {Google},
  year = {2024},
  title = {Google Q2 2024 Financial Report},
  howpublished = {"\url{https://abc.xyz/assets/ae/e9/753110054014b6de4d620a2853f6/goog-10-q-q2-2024.pdf}"}
}


@misc{intel_ipex,
  author = {Intel},
  year = {2024},
  title = {Intel Extension For Pytorch},
  howpublished = {"\url{https://github.com/intel/intel-extension-for-pytorch}"}
}

@misc{dlrm_pytorch,
  author = {Meta},
  year = {2024},
  title = {DLRM using PyTorch},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm}"}
}

@misc{cost_hbm_vs_ddr,
  author = {SemiAnalysis},
  year = {2024},
  title = {The Memory Wall},
  howpublished = {"\url{https://www.semianalysis.com/p/the-memory-wall}"}
}

@inproceedings{su2024compiling,
  title={Compiling Loop-Based Nested Parallelism for Irregular Workloads},
  author={Su, Yian and Rainey, Mike and Wanninger, Nick and Dhiantravan, Nadharm and Liang, Jasper and Acar, Umut A and Dinda, Peter and Campanoni, Simone},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={232--250},
  year={2024}
}

@article{ardalani2022understanding,
  title={Understanding scaling laws for recommendation models},
  author={Ardalani, Newsha and Wu, Carole-Jean and Chen, Zeliang and Bhushanam, Bhargav and Aziz, Adnan},
  journal={arXiv preprint arXiv:2208.08489},
  year={2022}
}

@inproceedings{adnan2024heterogeneous,
  title={Heterogeneous acceleration pipeline for recommendation system training},
  author={Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={1063--1079},
  year={2024},
  organization={IEEE}
}

@article{nair2024parallelization,
  title={Parallelization Strategies for DLRM Embedding Bag Operator on AMD CPUs},
  author={Nair, Krishnakumar and Pandey, Avinash-Chandra and Karabannavar, Siddappa and Arunachalam, Meena and Kalamatianos, John and Agrawal, Varun and Gupta, Saurabh and Sirasao, Ashish and Delaye, Elliott and Reinhardt, Steve and others},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}

@inproceedings{jain2023optimizing,
  title={Optimizing cpu performance for recommendation systems at-scale},
  author={Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}

@misc{amd-zendnn,
  author = {AMD},
  year = {2023},
  title = {{AMD Zen Deep Neural Network}},
  howpublished = {"\url{https://www.amd.com/en/developer/zendnn.html}"}
}

@misc{fbgemm,
  author = {Facebook},
  year = {2023},
  title = {{Facebook Generalized Matrix Multiplication}},
  howpublished = {"\url{https://github.com/pytorch/FBGEMM}"}
}

@misc{ipex,
  author = {Intel},
  year = {2023},
  title = {{Intel® Extension for PyTorch*}},
  howpublished = {"\url{https://github.com/intel/intel-extension-for-pytorch}"}
}

@misc{dlrm-dataset,
  author = {Meta},
  year = {2023},
  title = {{Embedding lookup Production dataset}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm_datasets}"}
}

@misc{aws,
  author = {Amazon},
  year = {2024},
  title = {{Amazon EC2 Instance Types – AWS}},
  howpublished = {"\url{https://aws.amazon.com/ec2/instance-types/}"}
}

@misc{hotchips_latency_values,
  author = {Chips And Cheese},
  year = {2024},
  title = {{Ryzen-9950x}},
  howpublished = {"\url{https://chipsandcheese.com/p/amds-ryzen-9950x-zen-5-on-desktop}"}
}

@misc{gcp,
  author = {Google},
  year = {2024},
  title = {{Machine families resource and comparison guide | Compute Engine Documentation | Google Cloud}},
  howpublished = {"\url{https://cloud.google.com/compute/docs/machine-resource}"}
}

@misc{azure,
  author = {Microsoft},
  year = {2024},
  title = {{Cloud Computing Services | Microsoft Azure}},
  howpublished = {"\url{https://azure.microsoft.com/en-us/}"}
}



@inproceedings{Paszke_PyTorch_An_Imperative_2019,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'Alché-Buc, F. and Fox, E. and Garnett, R.},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}

@inproceedings{ardestani2022supporting,
  title={Supporting massive DLRM inference through software defined memory},
  author={Ardestani, Ehsan K and Kim, Changkyu and Lee, Seung Jae and Pan, Luoshang and Axboe, Jens and Rampersad, Valmiki and Agrawal, Banit and Yu, Fuxun and Yu, Ansha and Le, Trung and others},
  booktitle={2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)},
  pages={302--312},
  year={2022},
  organization={IEEE}
}

@inproceedings{gupta2020architectural,
  title={The architectural implications of facebook's dnn-based personalized recommendation},
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={488--501},
  year={2020},
  organization={IEEE}
}

@INPROCEEDINGS {acun2021understanding,
author = {B. Acun and M. Murphy and X. Wang and J. Nie and C. Wu and K. Hazelwood},
booktitle = {2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
title = {Understanding Training Efficiency of Deep Learning Recommendation Models at Scale},
year = {2021},
volume = {},
issn = {},
pages = {802-814},
abstract = {The use of GPUs has proliferated for machine learning workflows and is now considered mainstream for many deep learning models. Meanwhile, when training state-of-the-art personal recommendation models, which consume the highest number of compute cycles at our large-scale datacenters, the use of GPUs came with various challenges due to having both compute-intensive and memory-intensive components. GPU performance and efficiency of these recommendation models are largely affected by model architecture configurations such as dense and sparse features, MLP dimensions. Furthermore, these models often contain large embedding tables that do not fit into limited GPU memory. The goal of this paper is to explain the intricacies of using GPUs for training recommendation models, factors affecting hardware efficiency at scale, and learnings from a new scale-up GPU server design, Zion.},
keywords = {training;deep learning;computational modeling;memory management;graphics processing units;production;throughput},
doi = {10.1109/HPCA51647.2021.00072},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA51647.2021.00072},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@inproceedings {eisenman2022check,
author = {Assaf Eisenman and Kiran Kumar Matam and Steven Ingram and Dheevatsa Mudigere and Raghuraman Krishnamoorthi and Krishnakumar Nair and Misha Smelyanskiy and Murali Annavaram},
title = {{Check-N-Run}: a Checkpointing System for Training Deep Learning Recommendation Models},
booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
year = {2022},
isbn = {978-1-939133-27-4},
address = {Renton, WA},
pages = {929--943},
url = {https://www.usenix.org/conference/nsdi22/presentation/eisenman},
publisher = {USENIX Association},
month = apr
}

@INPROCEEDINGS{gupta2020deeprecsys,
  author={Gupta, Udit and Hsia, Samuel and Saraph, Vikram and Wang, Xiaodong and Reagen, Brandon and Wei, Gu-Yeon and Lee, Hsien-Hsin S. and Brooks, David and Wu, Carole-Jean},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference}, 
  year={2020},
  volume={},
  number={},
  pages={982-995},
  doi={10.1109/ISCA45697.2020.00084}}

@inproceedings{gupta2021recpipe,
author = {Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
title = {RecPipe: Co-Designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480127},
doi = {10.1145/3466752.3480127},
abstract = {Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 \texttimes{} and 6 \texttimes{}.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {870–884},
numpages = {15},
keywords = {deep learning, datacenter, hardware accelerator, personalized recommendation},
location = {Virtual Event, Greece},
series = {MICRO '21}
}


@INPROCEEDINGS{kalamkar2020optimizing,
  author={Kalamkar, Dhiraj and Georganas, Evangelos and Srinivasan, Sudarshan and Chen, Jianping and Shiryaev, Mikhail and Heinecke, Alexander},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={Optimizing Deep Learning Recommender Systems Training on CPU Cluster Architectures}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/SC41405.2020.00047}}

@inproceedings{kwon2022training,
author = {Kwon, Youngeun and Rhu, Minsoo},
title = {Training Personalized Recommendation Systems from (GPU) Scratch: Look Forward Not Backwards},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527386},
doi = {10.1145/3470496.3527386},
abstract = {Personalized recommendation models (RecSys) are one of the most popular machine learning workload serviced by hyperscalers. A critical challenge of training RecSys is its high memory capacity requirements, reaching hundreds of GBs to TBs of model size. In RecSys, the so-called embedding layers account for the majority of memory usage so current systems employ a hybrid CPU-GPU design to have the large CPU memory store the memory hungry embedding layers. Unfortunately, training embeddings involve several memory bandwidth intensive operations which is at odds with the slow CPU memory, causing performance overheads. Prior work proposed to cache frequently accessed embeddings inside GPU memory as means to filter down the embedding layer traffic to CPU memory, but this paper observes several limitations with such cache design. In this work, we present a fundamentally different approach in designing embedding caches for RecSys. Our proposed ScratchPipe architecture utilizes unique properties of RecSys training to develop an embedding cache that not only sees the past but also the "future" cache accesses. ScratchPipe exploits such property to guarantee that the active working set of embedding layers can "always" be captured inside our proposed cache design, enabling embedding layer training to be conducted at GPU memory speed.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {860–873},
numpages = {14},
keywords = {systems for machine learning, recommendation system, neural network, memory architecture, graphics processing unit (GPU)},
location = {New York, New York},
series = {ISCA '22}
}

@INPROCEEDINGS {ke2022hercules,
author = {L. Ke and U. Gupta and M. Hempstead and C. Wu and H. S. Lee and X. Zhang},
booktitle = {2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
title = {Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation},
year = {2022},
volume = {},
issn = {},
pages = {141-154},
abstract = {Personalized recommendation is an important class of deep-learning applications that powers a large collection of internet services and consumes a considerable amount of datacenter resources. As the scale of production-grade recommendation systems continues to grow, optimizing their serving performance and efficiency in a heterogeneous datacenter is important and can translate into infrastructure capacity saving. In this paper, we propose Hercules, an optimized framework for personalized recommendation inference serving that targets diverse industry-representative models and cloud-scale heterogeneous systems. Hercules performs a two-stage optimization procedure — offline profiling and online serving. The first stage searches the large under-explored task scheduling space with a gradient-based search algorithm achieving up to 9.0× latency-bounded throughput improvement on individual servers; it also identifies the optimal heterogeneous server architecture for each recommendation workload. The second stage performs heterogeneity-aware cluster provisioning to optimize resource mapping and allocation in response to fluctuating diurnal loads. The proposed cluster scheduler in Hercules achieves 47.7% cluster capacity saving and reduces the provisioned power by 23.7% over a state-of-the-art greedy scheduler.},
keywords = {job shop scheduling;web and internet services;computer architecture;throughput;real-time systems;servers;resource management;task analysis;recommender systems;optimization},
doi = {10.1109/HPCA53966.2022.00019},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA53966.2022.00019},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {apr}
}

@inproceedings{mudigere2022software,
author = {Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and Yang, Jie (Amy) and Gao, Leon and Ivchenko, Dmytro and Basant, Aarti and Hu, Yuxi and Yang, Jiyan and Ardestani, Ehsan K. and Wang, Xiaodong and Komuravelli, Rakesh and Chu, Ching-Hsiang and Yilmaz, Serhat and Li, Huayu and Qian, Jiyuan and Feng, Zhuobo and Ma, Yinbin and Yang, Junjie and Wen, Ellie and Li, Hong and Yang, Lin and Sun, Chonglin and Zhao, Whitney and Melts, Dimitry and Dhulipala, Krishna and Kishore, KR and Graf, Tyler and Eisenman, Assaf and Matam, Kiran Kumar and Gangidi, Adi and Chen, Guoqiang Jerry and Krishnan, Manoj and Nayak, Avinash and Nair, Krishnakumar and Muthiah, Bharath and khorashadi, Mahmoud and Bhattacharya, Pallab and Lapukhov, Petr and Naumov, Maxim and Mathews, Ajit and Qiao, Lin and Smelyanskiy, Mikhail and Jia, Bill and Rao, Vijay},
title = {Software-Hardware Co-Design for Fast and Scalable Training of Deep Learning Recommendation Models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533727},
doi = {10.1145/3470496.3533727},
abstract = {Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\texttimes{} for training 12-trillion-parameter DLRM models deployed in production.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {993–1011},
numpages = {19},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{sethi2022recshard,
author = {Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
title = {RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale Neural Recommendation},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507777},
doi = {10.1145/3503222.3507777},
abstract = {We propose RecShard, a fine-grained embedding table (EMB) partitioning and placement technique for deep learning recommendation models (DLRMs). RecShard is designed based on two key observations. First, not all EMBs are equal, nor all rows within an EMB are equal in terms of access patterns. EMBs exhibit distinct memory characteristics, providing performance optimization opportunities for intelligent EMB partitioning and placement across a tiered memory hierarchy. Second, in modern DLRMs, EMBs function as hash tables. As a result, EMBs display interesting phenomena, such as the birthday paradox, leaving EMBs severely under-utilized. RecShard determines an optimal EMB sharding strategy for a set of EMBs based on training data distributions and model characteristics, along with the bandwidth characteristics of the underlying tiered memory hierarchy. In doing so, RecShard achieves over 6 times higher EMB training throughput on average for capacity constrained DLRMs. The throughput increase comes from improved EMB load balance by over 12 times and from the reduced access to the slower memory by over 87 times.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {344–358},
numpages = {15},
keywords = {Deep learning recommendation models, AI training systems, Neural networks, Memory optimization},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@INPROCEEDINGS{sun2022rmssd,
  author={Sun, Xuan and Wan, Hu and Li, Qiao and Yang, Chia-Lin and Kuo, Tei-Wei and Xue, Chun Jason},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={RM-SSD: In-Storage Computing for Large-Scale Recommendation Inference}, 
  year={2022},
  volume={},
  number={},
  pages={1056-1070},
  doi={10.1109/HPCA53966.2022.00081}}


@inproceedings{wilkening2021recssd,
author = {Wilkening, Mark and Gupta, Udit and Hsia, Samuel and Trippel, Caroline and Wu, Carole-Jean and Brooks, David and Wei, Gu-Yeon},
title = {RecSSD: Near Data Processing for Solid State Drive Based Recommendation Inference},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446763},
doi = {10.1145/3445814.3446763},
abstract = {Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2\texttimes{} compared to using COTS SSDs across eight industry-representative models.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {717–729},
numpages = {13},
keywords = {solid state drives, neural networks, near data processing},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{sun2022rm,
  title={Rm-ssd: In-storage computing for large-scale recommendation inference},
  author={Sun, Xuan and Wan, Hu and Li, Qiao and Yang, Chia-Lin and Kuo, Tei-Wei and Xue, Chun Jason},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={1056--1070},
  year={2022},
  organization={IEEE}
}

@inproceedings{zhao2022understanding,
author = {Zhao, Mark and Agarwal, Niket and Basant, Aarti and Gedik, Bu\u{g}ra and Pan, Satadru and Ozdal, Mustafa and Komuravelli, Rakesh and Pan, Jerry and Bao, Tianshu and Lu, Haowei and Narayanan, Sundaram and Langman, Jack and Wilfong, Kevin and Rastogi, Harsha and Wu, Carole-Jean and Kozyrakis, Christos and Pol, Parik},
title = {Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training: Industrial Product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533044},
doi = {10.1145/3470496.3533044},
abstract = {Datacenter-scale AI training clusters consisting of thousands of domain-specific accelerators (DSA) are used to train increasingly-complex deep learning models. These clusters rely on a data storage and ingestion (DSI) pipeline, responsible for storing exabytes of training data and serving it at tens of terabytes per second. As DSAs continue to push training efficiency and throughput, the DSI pipeline is becoming the dominating factor that constrains the overall training performance and capacity. Innovations that improve the efficiency and performance of DSI systems and hardware are urgent, demanding a deep understanding of DSI characteristics and infrastructure at scale.This paper presents Meta's end-to-end DSI pipeline, composed of a central data warehouse built on distributed storage and a Data PreProcessing Service that scales to eliminate data stalls. We characterize how hundreds of models are collaboratively trained across geo-distributed datacenters via diverse and continuous training jobs. These training jobs read and heavily filter massive and evolving datasets, resulting in popular features and samples used across training jobs. We measure the intense network, memory, and compute resources required by each training job to preprocess samples during training. Finally, we synthesize key takeaways based on our production infrastructure characterization. These include identifying hardware bottlenecks, discussing opportunities for heterogeneous DSI hardware, motivating research in datacenter scheduling and benchmark datasets, and assimilating lessons learned in optimizing DSI infrastructure.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1042–1057},
numpages = {16},
keywords = {data ingestion, distributed systems, data storage, machine learning systems, databases},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{shi2020compositional,
author = {Shi, Hao-Jun Michael and Mudigere, Dheevatsa and Naumov, Maxim and Yang, Jiyan},
title = {Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403059},
doi = {10.1145/3394486.3403059},
abstract = {Modern deep learning-based recommendation systems exploit hundreds to thousands of different categorical features, each with millions of different categories ranging from clicks to posts. To respect the natural diversity within the categorical data, embeddings map each category to a unique dense representation within an embedded space. Since each categorical feature could take on as many as tens of millions of different possible categories, the embedding tables form the primary memory bottleneck during both training and inference. We propose a novel approach for reducing the embedding size in an end-to-end fashion by exploiting complementary partitions of the category set to produce a unique embedding vector for each category without explicit definition. By storing multiple smaller embedding tables based on each complementary partition and combining embeddings from each table, we define a unique embedding for each category at smaller cost. This approach may be interpreted as using a specific fixed codebook to ensure uniqueness of each category's representation. Our experimental results demonstrate the effectiveness of our approach over the hashing trick for reducing the size of the embedding tables in terms of model loss and accuracy, while retaining a similar reduction in the number of parameters.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {165–175},
numpages = {11},
keywords = {recommendation systems, model compression, embeddings},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{liu2023accelerating,
author = {Liu, Haifeng and Zheng, Long and Huang, Yu and Liu, Chaoqiang and Ye, Xiangyu and Yuan, Jingrui and Liao, Xiaofei and Jin, Hai and Xue, Jingling},
title = {Accelerating Personalized Recommendation with Cross-Level Near-Memory Processing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589101},
doi = {10.1145/3579371.3589101},
abstract = {The memory-intensive embedding layers of the personalized recommendation systems are the performance bottleneck as they demand large memory bandwidth and exhibit irregular and sparse memory access patterns. Recent studies propose near memory processing (NMP) to accelerate memory-bound embedding operations. However, due to the load imbalance caused by the skewed access frequency of the embedding data, existing NMP solutions that exploit fine-grained memory parallelism fail to translate the increasingly massive internal bandwidth to performance improvements, leading to resource underutilization and hardware overhead.We propose an efficient yet practical fine-grained NMP accelerator for embedding operations. We architect ReCross, a cross-level NMP architecture that exploits rank, bank-group, and subarray-level memory parallelism in a unified DIMM-based memory system by supporting rank, bank-group, and bank-level NMP to accommodate various bandwidth requirements of embedding data. In addition, we present a novel embedding partitioning technique to quantify the bandwidth requirements of embedding tables and allocate them to appropriate NMP levels. ReCross innovatively collaborates the data and architecture characteristics for the NMP embedding layer acceleration, achieving high resource utilization and performance. Our evaluation shows that ReCross outperforms a state-of-the-art bank-group level NMP solution, TRiM-G, by 2.5\texttimes{} with nearly the same area overhead and bank-level NMP solution, TRiM-B, by 1.8\texttimes{} with an area overhead reduction of 4\texttimes{}.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {66},
numpages = {13},
keywords = {DRAM, memory system, near-memory-processing, DIMM},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{rishabh_isca23,
author = {Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and Das, Chita R.},
title = {Optimizing CPU Performance for Recommendation Systems At-Scale},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589112},
doi = {10.1145/3579371.3589112},
abstract = {Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {77},
numpages = {15},
keywords = {reuse distance, hyperthreading, prefetching, CPU, recommendation systems, irregular memory accesses, embeddings},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@inproceedings{ibrahim2022efficient,
author = {Ibrahim, Mohamed Assem and Kayiran, Onur and Aga, Shaizeen},
title = {Efficient Cache Utilization via Model-Aware Data Placement for Recommendation Models},
year = {2022},
isbn = {9781450385701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488423.3519317},
doi = {10.1145/3488423.3519317},
abstract = {Deep neural network (DNN) based recommendation models (RMs) represent a class of critical workloads that are broadly used in social media, entertainment content, and online businesses. Given their pervasive usage, understanding the memory subsystem behavior of these models is crucial, particularly from the perspective of future memory subsystem design. To this end, in this work, we first do an in-depth memory footprint and traffic analysis of emerging RMs. We observe that emerging RMs will severely stress future (and possibly larger) caches and memories. To address this challenge, we make the key observation that a data placement strategy that is aware of the components within these models (as opposed to one that considers the entire model as a whole) stands a better chance of relieving the stress on the memory subsystem. Specifically, of the two key components of these models, namely, embedding tables and multi-layer perceptron layers, we show how we can exploit the locality of memory accesses to embedding tables to come up with a more nuanced data placement scheme. We demonstrate how our proposed data placement strategy can reduce overall memory traffic (approximately 32\%) while improving performance (up to 1.99 \texttimes{}). We argue that memory subsystems that are more amenable to residency controls stand a better chance to address the needs of emerging models.},
booktitle = {Proceedings of the International Symposium on Memory Systems},
articleno = {2},
numpages = {11},
keywords = {Neural Networks, DLRM, Embedding Tables, Caches, Recommendation Models, MLPs},
location = {Washington DC, DC, USA},
series = {MEMSYS '21}
}

@INPROCEEDINGS{kal2021space,
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={SPACE: Locality-Aware Processing in Heterogeneous Memory for Personalized Recommendations}, 
  year={2021},
  volume={},
  number={},
  pages={679-691},
  doi={10.1109/ISCA52012.2021.00059}}


@inproceedings{ke2020recnmp,
author = {Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S. and Li, Meng and Maher, Bert and Mudigere, Dheevatsa and Naumov, Maxim and Schatz, Martin and Smelyanskiy, Mikhail and Wang, Xiaodong and Reagen, Brandon and Wu, Carole-Jean and Hempstead, Mark and Zhang, Xuan},
title = {RecNMP: Accelerating Personalized Recommendation with near-Memory Processing},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00070},
doi = {10.1109/ISCA45697.2020.00070},
abstract = {Personalized recommendation systems leverage deep learning models and account for the majority of data center AI cycles. Their performance is dominated by memory-bound sparse embedding operations with unique irregular memory access patterns that pose a fundamental challenge to accelerate. This paper proposes a lightweight, commodity DRAM compliant, near-memory processing solution to accelerate personalized recommendation inference. The in-depth characterization of production-grade recommendation models shows that embedding operations with high model-, operator- and data-level parallelism lead to memory bandwidth saturation, limiting recommendation inference performance. We propose RecNMP which provides a scalable solution to improve system throughput, supporting a broad range of sparse embedding models. RecNMP is specifically tailored to production environments with heavy co-location of operators on a single server. Several hardware/software co-optimization techniques such as memory-side caching, table-aware packet scheduling, and hot entry profiling are studied, providing up to 9.8x memory latency speedup over a highly-optimized baseline. Overall, RecNMP offers 4.2x throughput improvement and 45.8\% memory energy savings.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {790–803},
numpages = {14},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{lee2021merci,
author = {Lee, Yejin and Seo, Seong Hoon and Choi, Hyunji and Sul, Hyoung Uk and Kim, Soosung and Lee, Jae W. and Ham, Tae Jun},
title = {MERCI: Efficient Embedding Reduction on Commodity Hardware via Sub-Query Memoization},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446717},
doi = {10.1145/3445814.3446717},
abstract = {Deep neural networks (DNNs) with embedding layers are widely adopted to capture complex relationships among entities within a dataset. Embedding layers aggregate multiple embeddings — a dense vector used to represent the complicated nature of a data feature— into a single embedding; such operation is called embedding reduction. Embedding reduction spends a significant portion of its runtime on reading embeddings from memory and thus is known to be heavily memory-bandwidth-bound. Recent works attempt to accelerate this critical operation, but they often require either hardware modifications or emerging memory technologies, which makes it hardly deployable on commodity hardware. Thus, we propose MERCI, Memoization for Embedding Reduction with ClusterIng, a novel memoization framework for efficient embedding reduction. MERCI provides a mechanism for memoizing partial aggregation of correlated embeddings and retrieving the memoized partial result at a low cost. MERCI substantially reduces the number of memory accesses by 44\% (29\%), leading to 102\% (74\%) throughput improvement on real machines and 40.2\% (28.6\%) energy savings at the expense of 8\texttimes{}(1\texttimes{}) additional memory usage.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {302–313},
numpages = {12},
keywords = {Memoization, Recommender Systems, Embedding Lookup},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{gupta2021training,
author = {Gupta, Vipul and Choudhary, Dhruv and Tang, Peter and Wei, Xiaohan and Wang, Xing and Huang, Yuzhen and Kejariwal, Arun and Ramchandran, Kannan and Mahoney, Michael W.},
title = {Training Recommender Systems at Scale: Communication-Efficient Model and Data Parallelism},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467080},
doi = {10.1145/3447548.3467080},
abstract = {In this paper, we consider hybrid parallelism---a paradigm that employs both Data Parallelism (DP) and Model Parallelism (MP)---to scale distributed training of large recommendation models. We propose a compression framework called Dynamic Communication Thresholding (DCT) for communication-efficient hybrid training. DCT filters the entities to be communicated across the network through a simple hard-thresholding function, allowing only the most relevant information to pass through. For communication efficient DP, DCT compresses the parameter gradients sent to the parameter server during model synchronization. The threshold is updated only once every few thousand iterations to reduce the computational overhead of compression. For communication efficient MP, DCT incorporates a novel technique to compress the activations and gradients sent across the network during the forward and backward propagation, respectively. This is done by identifying and updating only the most relevant neurons of the neural network for each training sample in the data. We evaluate DCT on publicly available natural language processing and recommender models and datasets, as well as recommendation systems used in production at Facebook. DCT reduces communication by at least 100x and 20x during DP and MP, respectively. The algorithm has been deployed in production, and it improves end-to-end training time for a state-of-the-art industrial recommender model by 37\%, without any loss in performance.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {2928–2936},
numpages = {9},
keywords = {recommender systems, neural networks, hybrid parallelism, distributed training},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{wang2022elrec,
author = {Wang, Zheng and Wang, Yuke and Feng, Boyuan and Mudigere, Dheevatsa and Muthiah, Bharath and Ding, Yufei},
title = {EL-Rec: Efficient Large-Scale Recommendation Model Training via Tensor-Train Embedding Table},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {Deep learning Recommendation Models (DLRMs) plays an important role in various application domains. However, existing DLRM training systems require a large number of GPUs due to the memory-intensive embedding tables. To this end, we propose EL-Rec, an efficient computing framework harnessing the Tensor-train (TT) technique to democratize the training of large-scale DLRMs with limited GPU resources. Specifically, EL-Rec optimizes TT decomposition based on key computation primitives of embedding tables and implements a high-performance compressed embedding table which is a drop-in replacement of Pytorch API. EL-Rec introduces an index reordering technique to harvest the performance gains from both local and global information of training inputs. EL-Rec also highlights a pipeline training paradigm to eliminate the communication overhead between the host memory and the training worker. Comprehensive experiments demonstrate that EL-Rec can handle the largest publicly available DLRM dataset with a single GPU and achieves 3\texttimes{} speedup over the state-of-the-art DLRM frameworks.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {70},
numpages = {14},
keywords = {recommender systems, high performance computing, deep learning},
location = {Dallas, Texas},
series = {SC '22}
}

@inproceedings{balasubramanian2021cdlrm,
author = {Balasubramanian, Keshav and Alshabanah, Abdulla and Choe, Joshua D and Annavaram, Murali},
title = {CDLRM: Look Ahead Caching for Scalable Training of Recommendation Models},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474246},
doi = {10.1145/3460231.3474246},
abstract = {Deep learning recommendation models (DLRMs) are typically composed of two sets of parameters: large embedding tables to handle sparse categorical inputs, and neural networks such as multi-layer perceptrons (MLPs) to handle dense non-categorical inputs. Current DLRM training practices keep both these parameters in GPU memory. But as the size of the embedding tables grow, this practice of storing model parameters in GPU memory requires dozens or even hundreds of GPUs. This is an unsustainable trend with severe environmental consequences. Furthermore, such a design forces only a few conglomerates to be the gate keepers of model training. In this work, we propose cDLRM which democratizes recommendation model training by allowing a user to train on a single GPU regardless of the size of embedding tables by storing all embedding tables in CPU memory. A CPU based pre-processor analyzes training batches to prefetch embedding table slices accessed by those batches and caches them in GPU memory just-in-time. An associated caching protocol on the GPU enables efficiently updating the cached embedding table parameters. cDLRM decouples the embedding table size demands from the number of GPUs needed for compute. We first demonstrate that with cDLRM it is possible to train a large recommendation model using a single GPU regardless of model size. We then demonstrate that with its unique caching strategy, cDLRM enables pure data parallel training. We use two publicly available datasets to show that a cDLRM achieves identical model accuracy compared to a baseline trained completely on GPUs, while benefiting from large reduction in GPU demand.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {263–272},
numpages = {10},
keywords = {distributed data parallel training, caching, efficient training, prefetching, Recommendation models},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@INPROCEEDINGS{rashidi2020scalable,
  author={Rashidi, Saeed and Shurpali, Pallavi and Sridharan, Srinivas and Hassani, Naader and Mudigere, Dheevatsa and Nair, Krishnakumar and Smelyanski, Misha and Krishna, Tushar},
  booktitle={2020 IEEE Symposium on High-Performance Interconnects (HOTI)}, 
  title={Scalable Distributed Training of Recommendation Models: An ASTRA-SIM + NS3 case-study with TCP/IP transport}, 
  year={2020},
  volume={},
  number={},
  pages={33-42},
  doi={10.1109/HOTI51249.2020.00020}}

@inproceedings{hildebrand2023efficient,
author = {Hildebrand, Mark and Lowe-Power, Jason and Akella, Venkatesh},
title = {Efficient Large Scale DLRM Implementation on Heterogeneous Memory Systems},
year = {2023},
isbn = {978-3-031-32040-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-32041-5_3},
doi = {10.1007/978-3-031-32041-5_3},
abstract = {We propose a new data structure called CachedEmbeddings for training large scale deep learning recommendation models (DLRM) efficiently on heterogeneous (DRAM + non-volatile) memory platforms. CachedEmbeddings implements an implicit software-managed cache and data movement optimization that is integrated with the Julia programming framework to optimize the implementation of large scale DLRM implementations with multiple sparse embedded tables operations. In particular we show an implementation that is 1.4X to 2X better than the best known Intel CPU based implementations on state-of-the-art DLRM benchmarks on a real heterogeneous memory platform from Intel, and 1.32X to 1.45X improvement over Intel’s 2LM implementation that treats the DRAM as a hardware managed cache.},
booktitle = {High Performance Computing: 38th International Conference, ISC High Performance 2023, Hamburg, Germany, May 21–25, 2023, Proceedings},
pages = {42–61},
numpages = {20},
location = {Hamburg, Germany}
}

@article{wang2023emsi,
author = {Wang, Yitu and Li, Shiyu and Zheng, Qilin and Chang, Andrew and Li, Hai and Chen, Yiran},
title = {
EMS-i: An Efficient Memory System Design with Specialized Caching Mechanism for Recommendation Inference},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609384},
doi = {10.1145/3609384},
abstract = {Recommendation systems have been widely embedded into many Internet services. For example, Meta’s deep learning recommendation model (DLRM) shows high prefictive accuracy of click-through rate in processing large-scale embedding tables. The SparseLengthSum (SLS) kernel of the DLRM dominates the inference time of the DLRM due to intensive irregular memory accesses to the embedding vectors. Some prior works directly adopt near data processing (NDP) solutions to obtain higher memory bandwidth to accelerate SLS. However, their inferior memory hierarchy induces low performance-cost ratio and fails to fully exploit the data locality. Although some software-managed cache policies were proposed to improve the cache hit rate, the incurred cache miss penalty is unacceptable considering the high overheads of executing the corresponding programs and the communication between the host and the accelerator. To address the issues aforementioned, we propose EMS-i, an efficient memory system design that integrates Solide State Drive (SSD) into the memory hierarchy using Compute Express Link (CXL) for recommendation system inference. We specialize the caching mechanism according to the characteristics of various DLRM workloads and propose a novel prefetching mechanism to further improve the performance. In addition, we delicately design the inference kernel and develop a customized mapping scheme for SLS operation, considering the multi-level parallelism in SLS and the data locality within a batch of queries. Compared to the state-of-the-art NDP solutions, EMS-i achieves up to 10.9\texttimes{} speedup over RecSSD and the performance comparable to RecNMP with 72\% energy savings. EMS-i also saves up to 8.7\texttimes{} and 6.6 \texttimes{} memory cost w.r.t. RecSSD and RecNMP, respectively.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {100},
numpages = {22},
keywords = {Recommendation system, compute express link}
}

@inproceedings{Nagrecha2023InTune,
author = {Nagrecha, Kabir and Liu, Lingyi and Delgado, Pablo and Padmanabhan, Prasanna},
title = {InTune: Reinforcement Learning-Based Data Pipeline Optimization for Deep Recommendation Models},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608778},
doi = {10.1145/3604915.3608778},
abstract = {Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- \& time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning (DL) training jobs are dominated by model execution times, the most important factor in DLRM training performance is often online data ingestion.In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into the specific bottlenecks and challenges of the DLRM training pipeline at scale. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to both observe the performance impacts of online ingestion and to identify shortfalls in existing data pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune. InTune&nbsp;employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data-loading and improve throughput. Our experiments show that InTune&nbsp;can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune&nbsp;achieves significantly higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune&nbsp;to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus current state-of-the-art data pipeline optimizers while also improving both CPU \& GPU utilization.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {430–442},
numpages = {13},
keywords = {data processing, recommendation systems, resource allocation, deep learning, parallel computing},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{fang2022frequency,
  author       = {Jiarui Fang and
                  Geng Zhang and
                  Jiatong Han and
                  Shenggui Li and
                  Zhengda Bian and
                  Yongbin Li and
                  Jin Liu and
                  Yang You},
  title        = {A Frequency-aware Software Cache for Large Recommendation System Embeddings},
  journal      = {CoRR},
  volume       = {abs/2208.05321},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2208.05321},
  doi          = {10.48550/ARXIV.2208.05321},
  eprinttype    = {arXiv},
  eprint       = {2208.05321},
  timestamp    = {Sat, 17 Dec 2022 01:15:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2208-05321.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Agarwal2023Bagpipe,
author = {Agarwal, Saurabh and Yan, Chengpo and Zhang, Ziyi and Venkataraman, Shivaram},
title = {Bagpipe: Accelerating Deep Recommendation Model Training},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613142},
doi = {10.1145/3600006.3613142},
abstract = {Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\% of embeddings representing more than 92\% of total accesses. Further, we also observe that during offline training we can lookahead at future batches to determine which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to overlap remote embedding accesses with the computation. We design an Oracle Cacher, a new component that uses a lookahead algorithm to generate optimal cache update decisions while providing strong consistency guarantees against staleness. We also design a logically replicated, physically partitioned cache and show that our design can reduce synchronization overheads in a distributed setting. Finally, we propose a disaggregated system architecture and show that our design can enable low-overhead fault tolerance. Our experiments using three datasets and four models show that Bagpipe provides a speed up of up to 5.6x compared to state of the art baselines, while providing the same convergence and reproducibility guarantees as synchronous training.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {348–363},
numpages = {16},
keywords = {distributed training, recommendation models},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@INPROCEEDINGS {Jiang2023MixRec,
author = {J. Jiang and R. Tian and J. Du and D. Huang and Y. Lu},
booktitle = {2023 IEEE 41st International Conference on Computer Design (ICCD)},
title = {MixRec: Orchestrating Concurrent Recommendation Model Training on CPU-GPU platform},
year = {2023},
volume = {},
issn = {},
pages = {366-374},
abstract = {The development of deep learning recommendation models (DLRM) and recommendation systems has significantly improved the precision of information matching. Due to distinct computation, data access, and memory usage characteristics of recommendation models, they may suffer from low resource utilization on prevalent heterogeneous CPU-GPU hardware platforms. Existing concurrent training solutions cannot be directly applied to DLRM due to various factors, such as insufficient fine-grained memory management and the lack of collaborative CPU-GPU scheduling. In this paper, we introduce MixRec, a scheduling framework that addresses these challenges by pro-viding an efficient job management and scheduling mechanism for DLRM training jobs on heterogeneous CPU-GPU platforms. To facilitate training co-location, we first estimate the peak memory consumption of each job. Additionally, we track and collect resource utilization for DLRM training jobs. Based on the information of resource usage, a batched job dispatcher with dynamic resource-complementary scheduling policy is proposed to co-locate DLRM training jobs on CPU-GPU platform. Experimental results demonstrate that our implementation achieved up to 4.42× higher throughput and 3.97× higher resource utilization for training jobs involving various recommendation models.},
keywords = {training;processor scheduling;computational modeling;memory management;dynamic scheduling;throughput;hardware},
doi = {10.1109/ICCD58817.2023.00062},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCD58817.2023.00062},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov}
}

@article{Punniyamurthy2023GPUinitiated,
  author       = {Kishore Punniyamurthy and
                  Bradford M. Beckmann and
                  Khaled Hamidouche},
  title        = {GPU-initiated Fine-grained Overlap of Collective Communication with
                  Computation},
  journal      = {CoRR},
  volume       = {abs/2305.06942},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.06942},
  doi          = {10.48550/ARXIV.2305.06942},
  eprinttype    = {arXiv},
  eprint       = {2305.06942},
  timestamp    = {Wed, 17 May 2023 15:47:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-06942.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{Lin2022Building,
  author={Lin, Zhongyi and Feng, Louis and Ardestani, Ehsan K. and Lee, Jaewon and Lundell, John and Kim, Changkyu and Kejariwal, Arun and Owens, John D.},
  booktitle={2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Building a Performance Model for Deep Learning Recommendation Model Training on GPUs}, 
  year={2022},
  volume={},
  number={},
  pages={227-229},
  doi={10.1109/ISPASS55109.2022.00030}}

@inproceedings{Xiao2023Gmeta,
author = {Xiao, Youshao and Zhao, Shangchun and Zhou, Zhenglei and Huan, Zhaoxin and Ju, Lin and Zhang, Xiaolu and Wang, Lin and Zhou, Jun},
title = {G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615208},
doi = {10.1145/3583780.3615208},
abstract = {Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the G PU cluster, namely G -Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental results show that G-Meta achieves notable training speed without loss of statistical performance. Since early 2022, G-Meta has been deployed in Alipay's core advertising and recommender system, shrinking the continuous delivery of models by four times. It also obtains 6.48\% improvement in Conversion Rate (CVR) and 1.06\% increase in CPM (Cost Per Mille) in Alipay's homepage display advertising, with the benefit of larger training samples and tasks.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4365–4369},
numpages = {5},
keywords = {deep meta learning, recommender system, distributed training},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{Wei2022GPUSpecialized,
author = {Wei, Yingcan and Langer, Matthias and Yu, Fan and Lee, Minseok and Liu, Jie and Shi, Ji and Wang, Zehuan},
title = {A GPU-Specialized Inference Parameter Server for Large-Scale Deep Recommendation Models},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3546765},
doi = {10.1145/3523227.3546765},
abstract = {Recommendation systems are of crucial importance for a variety of modern apps and web services, such as news feeds, social networks, e-commerce, search, etc. To achieve peak prediction accuracy, modern recommendation models combine deep learning with terabyte-scale embedding tables to obtain a fine-grained representation of the underlying data. Traditional inference serving architectures require deploying the whole model to standalone servers, which is infeasible at such massive scale. In this paper, we provide insights into the intriguing and challenging inference domain of online recommendation systems. We propose the HugeCTR Hierarchical Parameter Server (HPS), an industry-leading distributed recommendation inference framework, that combines a high-performance GPU embedding cache with an hierarchical storage architecture, to realize low-latency retrieval of embeddings for online model inference tasks. Among other things, HPS features (1) a redundant hierarchical storage system, (2) a novel high-bandwidth cache to accelerate parallel embedding lookup on NVIDIA GPUs, (3) online training support and (4) light-weight APIs for easy integration into existing large-scale recommendation workflows. To demonstrate its capabilities, we conduct extensive studies using both synthetically engineered and public datasets. We show that our HPS can dramatically reduce end-to-end inference latency, achieving 5~62x speedup (depending on the batch size) over CPU baseline implementations for popular recommendation models. Through multi-GPU concurrent deployment, the HPS can also greatly increase the inference QPS.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {408–419},
numpages = {12},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{Desai2022tradeoffs,
 author = {Desai, Aditya and Shrivastava, Anshumali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {33961--33972},
 publisher = {Curran Associates, Inc.},
 title = {The trade-offs of model size in large recommendation models : 100GB to 10MB Criteo-tb DLRM model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/dbae915128892556134f1c5375855590-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{Adnan2022Heterogeneous,
  author       = {Muhammad Adnan and
                  Yassaman Ebrahimzadeh Maboud and
                  Divya Mahajan and
                  Prashant J. Nair},
  title        = {Heterogeneous Acceleration Pipeline for Recommendation System Training},
  journal      = {CoRR},
  volume       = {abs/2204.05436},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2204.05436},
  doi          = {10.48550/ARXIV.2204.05436},
  eprinttype    = {arXiv},
  eprint       = {2204.05436},
  timestamp    = {Mon, 26 Jun 2023 07:38:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2204-05436.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ye2023GRACE,
author = {Ye, Haojie and Vedula, Sanketh and Chen, Yuhan and Yang, Yichen and Bronstein, Alex and Dreslinski, Ronald and Mudge, Trevor and Talati, Nishil},
title = {GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582029},
doi = {10.1145/3582016.3582029},
abstract = {The high memory bandwidth demand of sparse embedding layers continues to be a critical challenge in scaling the performance of recommendation models. While prior works have exploited heterogeneous memory system designs and partial embedding sum memoization techniques, they offer limited benefits. This is because prior designs either target a very small subset of embeddings to simplify their analysis or incur a high processing cost to account for all embeddings, which does not scale with the large sizes of modern embedding tables. This paper proposes GRACE-a lightweight and scalable graph-based algorithm-system co-design framework to significantly improve the embedding layer performance of recommendation models. GRACE proposes a novel Item Co-occurrence Graph (ICG) that scalably records item co-occurrences. GRACE then presents a new system-aware ICG clustering algorithm to find frequently accessed item combinations of arbitrary lengths to compute and memoize their partial sums. High-frequency partial sums are stored in a software-managed cache space to reduce memory traffic and improve the throughput of computing sparse features. We further present a cache data layout and low-cost address computation logic to efficiently lookup item embeddings and their partial sums. Our evaluation shows that GRACE significantly outperforms the state-of-the-art techniques SPACE and MERCI by 1.5x and 1.4x, respectively.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {282–301},
numpages = {20},
keywords = {Embedding Reduction, DLRM, Algorithm-System Co-Design},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{Huang2021Hierarchical,
author = {Huang, Yuzhen and Wei, Xiaohan and Wang, Xing and Yang, Jiyan and Su, Bor-Yiing and Bharuka, Shivam and Choudhary, Dhruv and Jiang, Zewei and Zheng, Hai and Langman, Jack},
title = {Hierarchical Training: Scaling Deep Recommendation Models on Large CPU Clusters},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467084},
doi = {10.1145/3447548.3467084},
abstract = {Neural network based recommendation models are widely used to power many internet-scale applications including product recommendation and feed ranking. As the models become more complex and more training data is required during training, improving the training scalability of these recommendation models becomes an urgent need. However, improving the scalability without sacrificing the model quality is challenging. In this paper, we conduct an in-depth analysis of the scalability bottleneck in existing training architecture on large scale CPU clusters. Based on these observations, we propose a new training architecture called Hierarchical Training, which exploits both data parallelism and model parallelism for the neural network part of the model within a group. We implement hierarchical training with a two-layer design: a tagging system that decides the operator placement and a net transformation system that materializes the training plans, and integrate hierarchical training into existing training stack. We propose several optimizations to improve the scalability of hierarchical training including model architecture optimization, communication compression, and various system-level improvements. Extensive experiments at massive scale demonstrate that hierarchical training can speed up distributed recommendation model training by 1.9x without model quality drop.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {3050–3058},
numpages = {9},
keywords = {system for machine learning, optimization, distributed training},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{GenoaHotChips,
  author={Troester, Kai and Bhargava, Ravi},
  booktitle={2023 IEEE Hot Chips 35 Symposium (HCS)}, 
  title={AMD Next Generation “Zen 4” Core and 4th Gen AMD EPYC™ 9004 Server CPU}, 
  year={2023},
  volume={},
  number={},
  pages={1-25},
  doi={10.1109/HCS59251.2023.10254726}}


@misc{GenoaLaunch,
  author = {AMD},
  year = {2023},
  title = {{AMD Zen Deep Neural Network}},
  howpublished = {"\url{https://www.amd.com/en/developer/zendnn.html}"}
}

@misc{GenoaSpec,
  author = {AMD},
  year = {2022},
  title = {{AMD Epyc 9654 CPU, code-named Genoa}},
  howpublished = {"\url{https://www.amd.com/en/product/12191}"}
}

@misc{GenoaXSpec,
  author = {AMD},
  year = {2022},
  title = {{AMD Epyc 9684X CPU, code-named Genoa X}},
  howpublished = {"https://www.amd.com/en/products/cpu/amd-epyc-9684x"}
}

@inproceedings{firoozshahian2023mtia,
author = {Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and Hutchin, Adam and Diril, Utku and Nair, Krishnakumar and Aredestani, Ehsan K. and Schatz, Martin and Hao, Yuchen and Komuravelli, Rakesh and Ho, Kunming and Abu Asal, Sameer and Shajrawi, Joe and Quinn, Kevin and Sreedhara, Nagesh and Kansal, Pankaj and Wei, Willie and Jayaraman, Dheepak and Cheng, Linda and Chopda, Pritam and Wang, Eric and Bikumandla, Ajay and Karthik Sengottuvel, Arun and Thottempudi, Krishna and Narasimha, Ashwin and Dodds, Brian and Gao, Cao and Zhang, Jiyuan and Al-Sanabani, Mohammed and Zehtabioskuie, Ana and Fix, Jordan and Yu, Hangchen and Li, Richard and Gondkar, Kaustubh and Montgomery, Jack and Tsai, Mike and Dwarakapuram, Saritha and Desai, Sanjay and Avidan, Nili and Ramani, Poorvaja and Narayanan, Karthik and Mathews, Ajit and Gopal, Sethu and Naumov, Maxim and Rao, Vijay and Noru, Krishna and Reddy, Harikrishna and Venkatapuram, Prahlad and Bjorlin, Alexis},
title = {MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589348},
doi = {10.1145/3579371.3589348},
abstract = {Meta has traditionally relied on using CPU-based servers for running inference workloads, specifically Deep Learning Recommendation Models (DLRM), but the increasing compute and memory requirements of these models have pushed the company towards using specialized solutions such as GPUs or other hardware accelerators. This paper describes the company's effort in constructing its first silicon specifically designed for recommendation systems; it describes the accelerator architecture and platform design, the software stack for enabling and optimizing PyTorch-based models and provides an initial performance evaluation. With our emerging software stack, we have made significant progress towards reaching the same or higher efficiency as the GPU: We averaged 0.9x perf/W across various DLRMs, and benchmarks show operators such as GEMMs reaching 2x perf/W. Finally, the paper describes the lessons we learned during this journey which can improve the performance and programmability of future generations of architecture.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {80},
numpages = {13},
keywords = {programmability, performance, recommendation systems, inference, machine learning, accelerators},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@INPROCEEDINGS{gupta2020architectural,
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and Lee, Hsien-Hsin S. and Malevich, Andrey and Mudigere, Dheevatsa and Smelyanskiy, Mikhail and Xiong, Liang and Zhang, Xuan},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={The Architectural Implications of Facebook's DNN-Based Personalized Recommendation}, 
  year={2020},
  volume={},
  number={},
  pages={488-501},
  keywords={Computational modeling;Data centers;Computer architecture;Throughput;Optimization;Artificial intelligence;Videos},
  doi={10.1109/HPCA47549.2020.00047}}


@inproceedings{jain2023optimizing,
author = {Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and Das, Chita R.},
title = {Optimizing CPU Performance for Recommendation Systems At-Scale},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589112},
doi = {10.1145/3579371.3589112},
abstract = {Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {77},
numpages = {15},
keywords = {CPU, hyperthreading, prefetching, reuse distance, irregular memory accesses, embeddings, recommendation systems},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@INPROCEEDINGS{ke2020recnmp,
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S. and Li, Meng and Maher, Bert and Mudigere, Dheevatsa and Naumov, Maxim and Schatz, Martin and Smelyanskiy, Mikhail and Wang, Xiaodong and Reagen, Brandon and Wu, Carole-Jean and Hempstead, Mark and Zhang, Xuan},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing}, 
  year={2020},
  volume={},
  number={},
  pages={790-803},
  keywords={Systematics;Limiting;Scheduling algorithms;Energy conservation;Random access memory;Production;Parallel processing},
  doi={10.1109/ISCA45697.2020.00070}}


@ARTICLE{ke2021near,
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and Kim, KyungSoo and Jung, Jin and Yun, Ilkwon and Park, Sung Joo and Park, Hyunsun and Song, Joonho and Cho, Jeonghyeon and Sohn, Kyomin and Kim, Nam Sung and Lee, Hsien-Hsin S.},
  journal={IEEE Micro}, 
  title={Near-Memory Processing in Action: Accelerating Personalized Recommendation With AxDIMM}, 
  year={2022},
  volume={42},
  number={1},
  pages={116-127},
  keywords={Random access memory;Bandwidth;Throughput;Computational modeling;Hardware;Production;Field programmable gate arrays},
  doi={10.1109/MM.2021.3097700}}


@inproceedings {lai2023adaembed,
author = {Fan Lai and Wei Zhang and Rui Liu and William Tsai and Xiaohan Wei and Yuxi Hu and Sabin Devkota and Jianyu Huang and Jongsoo Park and Xing Liu and Zeliang Chen and Ellie Wen and Paul Rivera and Jie You and Chun-cheng Jason Chen and Mosharaf Chowdhury},
title = {$\{$AdaEmbed$\}$: Adaptive Embedding for $\{$Large-Scale$\}$ Recommendation Models},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {817--831},
url = {https://www.usenix.org/conference/osdi23/presentation/lai},
publisher = {USENIX Association},
month = jul
}

@inproceedings{mudigere2022software,
author = {Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and Yang, Jie (Amy) and Gao, Leon and Ivchenko, Dmytro and Basant, Aarti and Hu, Yuxi and Yang, Jiyan and Ardestani, Ehsan K. and Wang, Xiaodong and Komuravelli, Rakesh and Chu, Ching-Hsiang and Yilmaz, Serhat and Li, Huayu and Qian, Jiyuan and Feng, Zhuobo and Ma, Yinbin and Yang, Junjie and Wen, Ellie and Li, Hong and Yang, Lin and Sun, Chonglin and Zhao, Whitney and Melts, Dimitry and Dhulipala, Krishna and Kishore, KR and Graf, Tyler and Eisenman, Assaf and Matam, Kiran Kumar and Gangidi, Adi and Chen, Guoqiang Jerry and Krishnan, Manoj and Nayak, Avinash and Nair, Krishnakumar and Muthiah, Bharath and khorashadi, Mahmoud and Bhattacharya, Pallab and Lapukhov, Petr and Naumov, Maxim and Mathews, Ajit and Qiao, Lin and Smelyanskiy, Mikhail and Jia, Bill and Rao, Vijay},
title = {Software-hardware co-design for fast and scalable training of deep learning recommendation models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533727},
doi = {10.1145/3470496.3533727},
abstract = {Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\texttimes{} for training 12-trillion-parameter DLRM models deployed in production.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {993–1011},
numpages = {19},
location = {New York, New York},
series = {ISCA '22}
}


@misc{naumov2019deep,
      title={Deep Learning Recommendation Model for Personalization and Recommendation Systems}, 
      author={Maxim Naumov and Dheevatsa Mudigere and Hao-Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole-Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},
      year={2019},
      eprint={1906.00091},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}



@inproceedings{gupta2020deeprecsys,
  title={Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference},
  author={Gupta, Udit and Hsia, Samuel and Saraph, Vikram and Wang, Xiaodong and Reagen, Brandon and Wei, Gu-Yeon and Lee, Hsien-Hsin S and Brooks, David and Wu, Carole-Jean},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={982--995},
  year={2020},
  organization={IEEE}
}

@inproceedings{ke2022hercules,
  title={Hercules: Heterogeneity-aware inference serving for at-scale personalized recommendation},
  author={Ke, Liu and Gupta, Udit and Hempstead, Mark and Wu, Carole-Jean and Lee, Hsien-Hsin S and Zhang, Xuan},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={141--154},
  year={2022},
  organization={IEEE}
}


@inproceedings{mudigere2022software_old,
  title={Software-hardware co-design for fast and scalable training of deep learning recommendation models},
  author={Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and others},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={993--1011},
  year={2022}
}

@inproceedings{firoozshahian2023mtia_old,
  title={MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
  author={Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--13},
  year={2023}
}

@inproceedings{jain2023optimizing_old,
  title={Optimizing CPU Performance for Recommendation Systems At-Scale},
  author={Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}

@inproceedings{gupta2021recpipe,
  title={Recpipe: Co-designing models and hardware to jointly optimize recommendation quality and performance},
  author={Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={870--884},
  year={2021}
}

@inproceedings{hsia2020cross,
  title={Cross-stack workload characterization of deep recommendation systems},
  author={Hsia, Samuel and Gupta, Udit and Wilkening, Mark and Wu, Carole-Jean and Wei, Gu-Yeon and Brooks, David},
  booktitle={2020 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={157--168},
  year={2020},
  organization={IEEE}
}

@inproceedings{lai2023adaembed_old,
  title={$\{$AdaEmbed$\}$: Adaptive Embedding for $\{$Large-Scale$\}$ Recommendation Models},
  author={Lai, Fan and Zhang, Wei and Liu, Rui and Tsai, William and Wei, Xiaohan and Hu, Yuxi and Devkota, Sabin and Huang, Jianyu and Park, Jongsoo and Liu, Xing and others},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={817--831},
  year={2023}
}

@inproceedings{lin2022building,
  title={Building a performance model for deep learning recommendation model training on gpus},
  author={Lin, Zhongyi and Feng, Louis and Ardestani, Ehsan K and Lee, Jaewon and Lundell, John and Kim, Changkyu and Kejariwal, Arun and Owens, John D},
  booktitle={2022 IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC)},
  pages={48--58},
  year={2022},
  organization={IEEE}
}

@misc{mlperf_submissions,
  author = {MLPerf Datacenter Inference},
  year = {2024},
  title = {MLPerf Datacenter Inference 2024},
  howpublished = {"\url{https://mlcommons.org/benchmarks/inference-datacenter/}"}
}


@misc{h100_nvl,
  author = {Nvidia},
  year = {2024},
  title = {H100 NVL 96 GB},
  howpublished = {"\url{https://www.nvidia.com/en-us/data-center/h100/}"}
}







@misc{meta_h100_infra,
  author = {Meta},
  year = {2024},
  title = {Meta H100 Infrastructure 2024},
  howpublished = {"\url{https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/}"}
}


@misc{amazon_h100,
  author = {Amazon},
  year = {2024},
  title = {Amazon EC2-P5 H100 Instance},
  howpublished = {"\url{https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/}"}
}




@misc{ncu_profiling_limitation,
  author = {Nvidia},
  year = {2024},
  title = {NCU profiling limitation},
  howpublished = {"\url{https://forums.developer.nvidia.com/t/ncu-profiling-with-cache-control/246113/}"}
}

@misc{mlperf_dlrm,
  author = {MLPerf DLRM},
  year = {2024},
  title = {MLPerf DLRM},
  howpublished = {"\url{https://github.com/mlcommons/inference/tree/master/recommendation/dlrm_v2/pytorch}"}
}

@inproceedings{gupta2020architectural_old,
  title={The architectural implications of facebook's dnn-based personalized recommendation},
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={488--501},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{zhou2019deep,
  title={Deep interest evolution network for click-through rate prediction},
  author={Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5941--5948},
  year={2019}
}


@article{naumov2019deep_old,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@inproceedings{kal2021space,
  title={Space: locality-aware processing in heterogeneous memory for personalized recommendations},
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={679--691},
  year={2021},
  organization={IEEE}
}

@article{luo2024benchmarking,
  title={Benchmarking and Dissecting the Nvidia Hopper GPU Architecture},
  author={Luo, Weile and Fan, Ruibo and Li, Zeyu and Du, Dayou and Wang, Qiang and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2402.13499},
  year={2024}
}


@misc{pytorch_dlrm,
  author = {Meta},
  year = {2024},
  title = {PyTorch DLRM},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm/}"}
}


@misc{pytorch_emb_bag,
  author = {Meta},
  year = {2024},
  title = {PyTorch Embedding Bag Operator},
  howpublished = {"\url{https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/EmbeddingBag.cpp}"}
}

@inproceedings{zhong2024managing,
  title={Managing Memory Tiers with CXL in Virtualized Environments},
  author={Zhong, Yuhong and Berger, Daniel S and Waldspurger, Carl and Agarwal, Ishwar and Agarwal, Rajat and Hady, Frank and Kumar, Karthik and Hill, Mark D and Chowdhury, Mosharaf and Cidon, Asaf},
  booktitle={Symposium on Operating Systems Design and Implementation},
  year={2024}
}

@article{choi2024elasticrec,
  title={ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models},
  author={Choi, Yujeong and Kim, Jiin and Rhu, Minsoo},
  journal={arXiv preprint arXiv:2406.06955},
  year={2024}
}

@article{chen2024updlrm,
  title={UpDLRM: Accelerating Personalized Recommendation using Real-World PIM Architecture},
  author={Chen, Sitian and Tan, Haobin and Zhou, Amelie Chi and Li, Yusen and Balaji, Pavan},
  journal={arXiv preprint arXiv:2406.13941},
  year={2024}
}

@inproceedings{park2024accelerating,
  title={Accelerating Large-Scale DLRM Inference through Dynamic Hot Data Rearrangement},
  author={Park, Taehyung and Yang, Seungjin and Seok, Jongmin and Lee, Hyuk-Jae and Kim, Juhyun and Rhee, Chae Eun},
  booktitle={2024 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--5},
  year={2024},
  organization={IEEE}
}



@inproceedings{zheng2012gmprof,
  title={Gmprof: A low-overhead, fine-grained profiling approach for gpu programs},
  author={Zheng, Mai and Ravi, Vignesh T and Ma, Wenjing and Qin, Feng and Agrawal, Gagan},
  booktitle={2012 19th International Conference on High Performance Computing},
  pages={1--10},
  year={2012},
  organization={IEEE}
}

@article{zhao2010pipa,
  title={Pipa: Pipelined profiling and analysis on multicore systems},
  author={Zhao, Qin and Cutcutache, Ioana and Wong, Weng-Fai},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={7},
  number={3},
  pages={1--29},
  year={2010},
  publisher={ACM New York, NY, USA}
}

@misc{pytorch_jit,
  author = {Meta},
  year = {2024},
  title = {Multi-threading in PyTorch},
  howpublished = {"\url{https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html}"}
}


@misc{param_embedding,
  author = {Meta},
  year = {2024},
  title = {Embedding uBenchmark in PARAM},
  howpublished = {"\url{https://github.com/facebookresearch/param/blob/0a073429d2139b5947212863b32b222a09239cd3/train/compute/pt/pytorch_emb.py#L49}"}
}



@misc{dlrm-dataset,
  author = {Meta},
  year = {2023},
  title = {{Embedding lookup Production dataset}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm_datasets}"}
}


@misc{homogeneous-dlrm-dataset,
  author = {Rishabh Jain},
  year = {2023},
  title = {{Homogeneous Production Traces}},
  howpublished = {"\url{https://github.com/rishucoding/reproduce_isca23_cpu_DLRM_inference}"}
}


@misc{emb_bag_cuda_kernel,
  author = {PyTorch},
  year = {2024},
  title = {{Embedding Bag CUDA Kernel in PyTorch}},
  howpublished = {"\url{https://github.com/pytorch/pytorch/blob/da7db5d345a10ffb5092b26c5159f56faec1d0ea/aten/src/ATen/native/cuda/EmbeddingBag.cu#L115
}"}
}

@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}





@misc{meta_purchasing_gpu,
  author = {Meta},
  year = {2024},
  title = {{Meta's 2024 ML Infrastructure}},
  howpublished = {"\url{https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/
}"}
}


@misc{ptx_prefetch,
  author = {Nvidia},
  year = {2024},
  title = {{Parallel Thread Execution ISA Version 8.4}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prefetch-prefetchu
}"}
}

@inproceedings{lee2021merci,
  title={MERCI: efficient embedding reduction on commodity hardware via sub-query memoization},
  author={Lee, Yejin and Seo, Seong Hoon and Choi, Hyunji and Sul, Hyoung Uk and Kim, Soosung and Lee, Jae W and Ham, Tae Jun},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={302--313},
  year={2021}
}


@article{emma2005exploring,
  title={Exploring the limits of prefetching},
  author={Emma, Philip G and Hartstein, Allan and Puzak, Thomas R and Srinivasan, Viji},
  journal={IBM Journal of Research and Development},
  volume={49},
  number={1},
  pages={127--144},
  year={2005},
  publisher={IBM}
}





@misc{local_memory,
  author = {StackOverflow},
  year = {2024},
  title = {{Where does Local Memory reside?}},
  howpublished = {"\url{https://stackoverflow.com/questions/72381905/seeking-a-better-understanding-of-local-memory-in-cuda-where-does-it-live-how
}"}
}


@misc{cpu_prefetch_intrinsics,
  author = {GCC},
  year = {2024},
  title = {{Data Prefetching with GCC}},
  howpublished = {"\url{https://gcc.gnu.org/projects/prefetch.html
}"}
}

@book{falsafi2022primer,
  title={A primer on hardware prefetching},
  author={Falsafi, Babak and Wenisch, Thomas F},
  year={2022},
  publisher={Springer Nature}
}

@misc{pytorch_2.1,
  author = {PyTorch},
  year = {2024},
  title = {{PyTorch 2.1.0}},
  howpublished = {"\url{https://github.com/pytorch/pytorch/tree/v2.1.0
}"}
}


@article{lee2012prefetching,
  title={When prefetching works, when it doesn’t, and why},
  author={Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={1},
  pages={1--29},
  year={2012},
  publisher={ACM New York, NY, USA}
}


@misc{nvcc_compiler,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia’s NVCC compiler}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#opt-level-n-o
}"}
}

@misc{ncu_tool,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia’s Nsight Compute Tool}},
  howpublished = {"\url{https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html
}"}
}

@inproceedings{kal2021space,
  title={Space: locality-aware processing in heterogeneous memory for personalized recommendations},
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={679--691},
  year={2021},
  organization={IEEE}
}


@article{fu2023autoscratch,
  title={AutoScratch: ML-Optimized Cache Management for Inference-Oriented GPUs},
  author={Fu, Yaosheng and Bolotin, Evgeny and Jaleel, Aamer and Dalal, Gal and Mannor, Shie and Subag, Jacob and Korem, Noam and Behar, Michael and Nellans, David},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{zhang2023perks,
  title={PERKS: a Locality-Optimized Execution Model for Iterative Memory-bound GPU Applications},
  author={Zhang, Lingqi and Wahib, Mohamed and Chen, Peng and Meng, Jintao and Wang, Xiao and Endo, Toshio and Matsuoka, Satoshi},
  booktitle={Proceedings of the 37th International Conference on Supercomputing},
  pages={167--179},
  year={2023}
}


@inproceedings{sethi2022recshard,
  title={RecShard: statistical feature-based memory optimization for industry-scale neural recommendation},
  author={Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={344--358},
  year={2022}
}

@inproceedings{lui2021understanding,
  title={Understanding capacity-driven scale-out neural recommendation inference},
  author={Lui, Michael and Yetim, Yavuz and {\"O}zkan, {\"O}zg{\"u}r and Zhao, Zhuoran and Tsai, Shin-Yeh and Wu, Carole-Jean and Hempstead, Mark},
  booktitle={2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  pages={162--171},
  year={2021},
  organization={IEEE}
}

@article{ke2022disaggrec,
  title={DisaggRec: Architecting Disaggregated Systems for Large-Scale Personalized Recommendation},
  author={Ke, Liu and Zhang, Xuan and Lee, Benjamin and Suh, G Edward and Lee, Hsien-Hsin S},
  journal={arXiv preprint arXiv:2212.00939},
  year={2022}
}

@inproceedings{hwang2020centaur,
  title={Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations},
  author={Hwang, Ranggi and Kim, Taehun and Kwon, Youngeun and Rhu, Minsoo},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={968--981},
  year={2020},
  organization={IEEE}
}

@article{ke2021near_old,
  title={Near-memory processing in action: Accelerating personalized recommendation with axdimm},
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and others},
  journal={IEEE Micro},
  volume={42},
  number={1},
  pages={116--127},
  year={2021},
  publisher={IEEE}
}

@inproceedings{kwon2019tensordimm,
  title={Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning},
  author={Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={740--753},
  year={2019}
}

@inproceedings{ke2020recnmp_old,
  title={Recnmp: Accelerating personalized recommendation with near-memory processing},
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={790--803},
  year={2020},
  organization={IEEE}
}

@inproceedings{ye2023grace,
  title={GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference},
  author={Ye, Haojie and Vedula, Sanketh and Chen, Yuhan and Yang, Yichen and Bronstein, Alex and Dreslinski, Ronald and Mudge, Trevor and Talati, Nishil},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={282--301},
  year={2023}
}

@article{adnan2021accelerating,
  title={Accelerating recommendation system training by leveraging popular choices},
  author={Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J},
  journal={arXiv preprint arXiv:2103.00686},
  year={2021}
}

@inproceedings{kwon2022training,
  title={Training personalized recommendation systems from (GPU) scratch: Look forward not backwards},
  author={Kwon, Youngeun and Rhu, Minsoo},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={860--873},
  year={2022}
}

%%%%%%%%%%%% GPU hardware and resources


@misc{h100_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia Hopper GPU WhitePaper}},
  howpublished = {"\url{https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper
}"}
}



@misc{a100_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{A100 GPU White Paper}},
  howpublished = {"\url{https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf
}"}
}


@misc{why_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{CUDA Programming Guide: The benefits of using GPUs}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/#the-benefits-of-using-gpus
}"}
}


@inproceedings{adufu2023l2,
  title={L2 Cache Access Pattern Analysis using Static Profiling of an Application},
  author={Adufu, Theodora and Kim, Yoonhee},
  booktitle={2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)},
  pages={97--102},
  year={2023},
  organization={IEEE}
}

@inproceedings{adufu2023optimizing,
  title={Optimizing Performance Using GPU Cache Data Residency Based on Application’s Access Patterns},
  author={Adufu, Theodora and Kim, Yoonhee},
  booktitle={2023 24st Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  pages={42--47},
  year={2023},
  organization={IEEE}
}


@misc{l2_residency,
  author = {Nvidia},
  year = {2024},
  title = {{L2 cache residency control}},
  howpublished = {"\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-l2-access-management
}"}
}

%%%%%%%%%%%% use cases of Recommendation
@misc{fb_recsys,
  author = {Meta},
  year = {2024},
  title = {{Facebook Recommendation System}},
  howpublished = {"\url{https://ai.meta.com/blog/ai-unconnected-content-recommendations-facebook-instagram/
}"}
}

@misc{instagram_recsys,
  author = {Meta},
  year = {2024},
  title = {{Instagram Recommendation System}},
  howpublished = {"\url{https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/
}"}
}

@misc{tiktok_recsys,
  author = {TikTok},
  year = {2024},
  title = {{TikTok Recommendation System}},
  howpublished = {"\url{https://www.tiktok.com/transparency/en-us/recommendation-system/
}"}
}

@misc{amazon_recsys,
  author = {Amazon},
  year = {2024},
  title = {{Amazon Recommendation System}},
  howpublished = {"\url{https://aws.amazon.com/personalize/ 
}"}
}

@misc{netflix_recsys,
  author = {Netflix},
  year = {2024},
  title = {{Netflix Recommendation System}},
  howpublished = {"\url{https://research.netflix.com/research-area/recommendations
}"}
}

@misc{hulu_recsys,
  author = {Hulu},
  year = {2024},
  title = {{Hulu Recommendation System}},
  howpublished = {"\url{https://help.hulu.com/article/hulu-personalized-recommendations#:~:text=While%20you're%20looking%20for,get%20to%20know%20you%20better.
}"}
}


@misc{ebay_recsys,
  author = {eBay},
  year = {2024},
  title = {{eBay Recommendation System}},
  howpublished = {"\url{https://innovation.ebayinc.com/tech/engineering/building-a-deep-learning-based-retrieval-system-for-personalized-recommendations/
}"}
}

@misc{alibaba_recsys,
  author = {AliBaba},
  year = {2024},
  title = {{eBay Recommendation System}},
  howpublished = {"\url{https://www.alibabacloud.com/blog/getting-started-with-recommendation-system_597740
}"}
}




%%%%%%%%%%%% related works on prefetching on GPUs

@inproceedings{jog2013orchestrated,
  title={Orchestrated scheduling and prefetching for GPGPUs},
  author={Jog, Adwait and Kayiran, Onur and Mishra, Asit K and Kandemir, Mahmut T and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R},
  booktitle={Proceedings of the 40th Annual International Symposium on Computer Architecture},
  pages={332--343},
  year={2013}
}

@inproceedings{sethia2013apogee,
  title={APOGEE: Adaptive prefetching on GPUs for energy efficiency},
  author={Sethia, Ankit and Dasika, Ganesh and Samadi, Mehrzad and Mahlke, Scott},
  booktitle={Proceedings of the 22nd international conference on Parallel architectures and compilation techniques},
  pages={73--82},
  year={2013},
  organization={IEEE}
}

@article{oh2018adaptive,
  title={Adaptive cooperation of prefetching and warp scheduling on gpus},
  author={Oh, Yunho and Kim, Keunsoo and Yoon, Myung Kuk and Park, Jong Hyun and Park, Yongjun and Annavaram, Murali and Ro, Won Woo},
  journal={IEEE Transactions on Computers},
  volume={68},
  number={4},
  pages={609--616},
  year={2018},
  publisher={IEEE}
}

@inproceedings{wu2011pacman,
  title={PACMan: prefetch-aware cache management for high performance caching},
  author={Wu, Carole-Jean and Jaleel, Aamer and Martonosi, Margaret and Steely Jr, Simon C and Emer, Joel},
  booktitle={Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={442--453},
  year={2011}
}


%%%%%%%%%%%% related works on multi-threading (warp level parallelism) on GPUs

@article{jog2013owl,
  title={OWL: cooperative thread array aware scheduling techniques for improving GPGPU performance},
  author={Jog, Adwait and Kayiran, Onur and Chidambaram Nachiappan, Nachiappan and Mishra, Asit K and Kandemir, Mahmut T and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R},
  journal={ACM SIGPLAN Notices},
  volume={48},
  number={4},
  pages={395--406},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kayiran2013neither,
  title={Neither more nor less: Optimizing thread-level parallelism for GPGPUs},
  author={Kay{\i}ran, Onur and Jog, Adwait and Kandemir, Mahmut T and Das, Chita R},
  booktitle={Proceedings of the 22nd international conference on Parallel architectures and compilation techniques},
  pages={157--166},
  year={2013},
  organization={IEEE}
}

@inproceedings{song2023ugache,
  title={UGACHE: A Unified GPU Cache for Embedding-based Deep Learning},
  author={Song, Xiaoniu and Zhang, Yiwen and Chen, Rong and Chen, Haibo},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={627--641},
  year={2023}
}

@article{yuan2023everest,
  title={Everest: GPU-Accelerated System For Mining Temporal Motifs},
  author={Yuan, Yichao and Ye, Haojie and Kaza, Sanketh Vedula Wynn and Talati, Nishil},
  journal={arXiv preprint arXiv:2310.02800},
  year={2023}
}

@inproceedings{sethia2015mascar,
  title={Mascar: Speeding up GPU warps by reducing memory pitstops},
  author={Sethia, Ankit and Jamshidi, D Anoushe and Mahlke, Scott},
  booktitle={2015 IEEE 21st International symposium on high performance computer architecture (HPCA)},
  pages={174--185},
  year={2015},
  organization={IEEE}
}


@misc{A100,
title={{NVIDIA A100 Tensor Core GPU.}},
note={\url{https://www.nvidia.com/en-us/data-center/a100/}},
year={2024},
}


@misc{H100,
title={{NVIDIA H100 Tensor Core GPU}},
note={\url{https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/}},
year=2024
}

%%%%%%%%%%%% related works on register file virtualization

@inproceedings{jeon2015gpu,
  title={GPU register file virtualization},
  author={Jeon, Hyeran and Ravi, Gokul Subramanian and Kim, Nam Sung and Annavaram, Murali},
  booktitle={Proceedings of the 48th International Symposium on Microarchitecture},
  pages={420--432},
  year={2015}
}

@article{voitsechov2018software,
  title={Software-directed techniques for improved gpu register file utilization},
  author={Voitsechov, Dani and Zulfiqar, Arslan and Stephenson, Mark and Gebhart, Mark and Keckler, Stephen W},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={15},
  number={3},
  pages={1--23},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{oh2018finereg,
  title={FineReg: Fine-grained register file management for augmenting GPU throughput},
  author={Oh, Yunho and Yoon, Myung Kuk and Song, William J and Ro, Won Woo},
  booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={364--376},
  year={2018},
  organization={IEEE}
}


@misc{nvidia_nsight_systems,
title={{NVIDIA Nsight Systems}},
note={\url{https://docs.nvidia.com/nsight-systems/UserGuide/index.html}},
year=2024
}

@misc{nvcc_maxrregcount,
title={{nvcc maxrregcount flag}},
note={\url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#maxrregcount-amount-maxrregcount}},
year=2024
}


@misc{gpgpu-sim_scoreboard,
title={{Scoreboarding in GPUs}},
note={\url{http://gpgpu-sim.org/manual/index.php/Main_Page#Scoreboard}},
year=2024
}



@inproceedings{jha2024mem,
  title={Mem-Rec: Memory Efficient Recommendation System using Alternative Representation},
  author={Jha, Gopi Krishna and Thomas, Anthony and Jain, Nilesh and Gobriel, Sameh and Rosing, Tajana and Iyer, Ravi},
  booktitle={Asian Conference on Machine Learning},
  pages={518--533},
  year={2024},
  organization={PMLR}
}

@inproceedings{matam2024quickupdate,
  title={$\{$QuickUpdate$\}$: a $\{$Real-Time$\}$ Personalization System for $\{$Large-Scale$\}$ Recommendation Models},
  author={Matam, Kiran Kumar and Ramezani, Hani and Wang, Fan and Chen, Zeliang and Dong, Yue and Ding, Maomao and Zhao, Zhiwei and Zhang, Zhengyu and Wen, Ellie and Eisenman, Assaf},
  booktitle={21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={731--744},
  year={2024}
}

@article{zha2022dreamshard,
  title={Dreamshard: Generalizable embedding table placement for recommender systems},
  author={Zha, Daochen and Feng, Louis and Tan, Qiaoyu and Liu, Zirui and Lai, Kwei-Herng and Bhushanam, Bhargav and Tian, Yuandong and Kejariwal, Arun and Hu, Xia},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15190--15203},
  year={2022}
}


@misc{h100_nvl,
  title = {{H100 NVL 96 GB}},
  howpublished = {"\url{https://www.nvidia.com/en-us/data-center/h100/}"},
  note = {Accessed: 2024-02-07},
  author = {NVIDIA},
  year = {2024}
}


@misc{mlperf_submissions,
  title = {{MLPerf Datacenter Inference 2024}},
  howpublished = {\url{https://mlcommons.org/benchmarks/inference-datacenter/}},
 author = {MLPerf Datacenter Inference},
  year = {2024},
}

@misc{meta_h100_infra,
  title = {{Meta H100 Infrastructure 2024}},
  howpublished = {\url{https://engineering.fb.com/2024/03/12/datacenter-engineering/building-metas-genai-infrastructure/}},
 author = {Meta},
  year = {2024},
}


@misc{amazon_h100,
  title = {{Amazon EC2-P5 H100 Instance}},
  howpublished = {\url{https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/}}, author = {Amazon},
  year = {2024}
}



@misc{ncu_profiling_limitation,
  title = {{NCU profiling limitation}},
  howpublished = {\url{https://forums.developer.nvidia.com/t/ncu-profiling-with-cache-control/246113/}},
  author = {NVIDIA},
  year = {2024}
}

@misc{mlperf_dlrm,
  title = {{MLPerf DLRM}},
  howpublished = {\url{https://github.com/mlcommons/inference/tree/master/recommendation/dlrm_v2/pytorch}}, 
  author = {MLPerf DLRM},
  year = {2024}
}

@misc{pytorch_dlrm,
  title = {{PyTorch DLRM}},
  howpublished = {\url{https://github.com/facebookresearch/dlrm/blob/639e3d25a59b35e6b703506a5764e611cdfe8bea/dlrm_s_pytorch.py#L590}},
author = {Meta},
  year = {2024}
}

@misc{param_embedding,
  author = {Meta},
  year = {2024},
  title = {Embedding uBenchmark in PARAM},
  howpublished = {"\url{https://github.com/facebookresearch/param/blob/0a073429d2139b5947212863b32b222a09239cd3/train/compute/pt/pytorch_emb.py#L49}"}
}



@misc{dlrm-dataset,
  title = {{Embedding lookup Production dataset}},
  howpublished = {\url{https://github.com/facebookresearch/dlrm_datasets}},
author = {Meta},
  year = {2023},
}


@misc{homogeneous-dlrm-dataset,
  title = {{Homogeneous Production Traces}},
  howpublished = {\url{https://github.com/rishucoding/reproduce_isca23_cpu_DLRM_inference}},
author = {Rishabh Jain},
  year = {2023},
}


@misc{emb_bag_cuda_kernel,
  title = {{Embedding Bag CUDA Kernel in PyTorch}},
  howpublished = {\url{https://github.com/pytorch/pytorch/blob/da7db5d345a10ffb5092b26c5159f56faec1d0ea/aten/src/ATen/native/cuda/EmbeddingBag.cu#L115
}},   author = {PyTorch},
  year = {2024},
}

@misc{meta_purchasing_gpu,
  title = {{Meta's 2024 ML Infrastructure}},
  howpublished = {\url{https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/
}}, 
author = {Meta},
  year = {2024},
}


@misc{ptx_prefetch,
  title = {{Parallel Thread Execution ISA Version 8.4}},
  howpublished = {\url{https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prefetch-prefetchu
}},  author = {NVIDIA},
  year = {2024},
}


@misc{local_memory,
  author = {StackOverflow},
  year = {2024},
  title = {{Where does Local Memory reside?}},
  howpublished = {\url{https://stackoverflow.com/questions/72381905/seeking-a-better-understanding-of-local-memory-in-cuda-where-does-it-live-how
}}
}


@misc{cpu_prefetch_intrinsics,
  author = {GCC},
  year = {2024},
  title = {{Data Prefetching with GCC}},
  howpublished = {\url{https://gcc.gnu.org/projects/prefetch.html
}}
}


@misc{nvcc_compiler,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia’s NVCC compiler}},
  howpublished = {\url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#opt-level-n-o
}}
}

@misc{ncu_tool,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia’s Nsight Compute Tool}},
  howpublished = {\url{https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html
}}
}


@misc{h100_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{Nvidia Hopper GPU WhitePaper}},
  howpublished = {\url{https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper
}}
}



@misc{a100_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{A100 GPU White Paper}},
  howpublished = {\url{https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf
}}
}


@misc{why_gpu,
  author = {Nvidia},
  year = {2024},
  title = {{CUDA Programming Guide: The benefits of using GPUs}},
  howpublished = {\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/#the-benefits-of-using-gpus
}}
}

@misc{l2_residency,
  author = {Nvidia},
  year = {2024},
  title = {{L2 cache residency control}},
  howpublished = {\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-l2-access-management
}}
}

%%%%%%%%%%%% use cases of Recommendation
@misc{fb_recsys,
  author = {Meta},
  year = {2024},
  title = {{Facebook Recommendation System}},
  url = {https://ai.meta.com/blog/ai-unconnected-content-recommendations-facebook-instagram/
}
}


@misc{tiktok_recsys,
  author = {TikTok},
  year = {2024},
  title = {{TikTok Recommendation System}},
  howpublished = {\url{https://www.tiktok.com/transparency/en-us/recommendation-system/
}}
}

@misc{amazon_recsys,
  author = {Amazon},
  year = {2024},
  title = {{Amazon Recommendation System}},
  howpublished = {\url{https://aws.amazon.com/personalize/ 
}}
}

@misc{netflix_recsys,
  author = {Netflix},
  year = {2024},
  title = {{Netflix Recommendation System}},
  howpublished = {\url{https://research.netflix.com/research-area/recommendations
}}
}

@misc{hulu_recsys,
  author = {Hulu},
  year = {2024},
  title = {{Hulu Recommendation System}},
  howpublished = {\url{https://help.hulu.com/article/hulu-personalized-recommendations#:~:text=While%20you're%20looking%20for,get%20to%20know%20you%20better.
}}
}

@misc{instagram_recsys,
  author = {Meta},
  year = {2024},
  title = {{Instagram Recommendation System}},
  howpublished = {\url{https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/
}}
}


@misc{ebay_recsys,
  author = {eBay},
  year = {2024},
  title = {{eBay Recommendation System}},
  howpublished = {\url{https://innovation.ebayinc.com/tech/engineering/building-a-deep-learning-based-retrieval-system-for-personalized-recommendations/
}}
}

@misc{alibaba_recsys,
  author = {AliBaba},
  year = {2024},
  title = {{eBay Recommendation System}},
  howpublished = {\url{https://www.alibabacloud.com/blog/getting-started-with-recommendation-system_597740
}}
}

@misc{A100,
title={{NVIDIA A100 Tensor Core GPU.}},
note={\url{https://www.nvidia.com/en-us/data-center/a100/}},
year={2024},
}


@misc{H100,
title={{NVIDIA H100 Tensor Core GPU}},
note={\url{https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/}},
year=2024
}

@misc{nvidia_nsight_systems,
title={{NVIDIA Nsight Systems}},
note={\url{https://docs.nvidia.com/nsight-systems/UserGuide/index.html}},
year=2024
}

@misc{nvcc_maxrregcount,
title={{nvcc maxrregcount flag}},
note={\url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#maxrregcount-amount-maxrregcount}},
year=2024
}


@misc{gpgpu-sim_scoreboard,
title={{Scoreboarding in GPUs}},
note={\url{http://gpgpu-sim.org/manual/index.php/Main_Page#Scoreboard}},
year=2024
}


@article{nair2024parallelization,
  title={Parallelization Strategies for DLRM Embedding Bag Operator on AMD CPUs},
  author={Nair, Krishnakumar and Pandey, Avinash-Chandra and Karabannavar, Siddappa and Arunachalam, Meena and Kalamatianos, John and Agrawal, Varun and Gupta, Saurabh and Sirasao, Ashish and Delaye, Elliott and Reinhardt, Steve and others},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}


@inproceedings{firoozshahian2023mtia,
author = {Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and Hutchin, Adam and Diril, Utku and Nair, Krishnakumar and Aredestani, Ehsan K. and Schatz, Martin and Hao, Yuchen and Komuravelli, Rakesh and Ho, Kunming and Abu Asal, Sameer and Shajrawi, Joe and Quinn, Kevin and Sreedhara, Nagesh and Kansal, Pankaj and Wei, Willie and Jayaraman, Dheepak and Cheng, Linda and Chopda, Pritam and Wang, Eric and Bikumandla, Ajay and Karthik Sengottuvel, Arun and Thottempudi, Krishna and Narasimha, Ashwin and Dodds, Brian and Gao, Cao and Zhang, Jiyuan and Al-Sanabani, Mohammed and Zehtabioskuie, Ana and Fix, Jordan and Yu, Hangchen and Li, Richard and Gondkar, Kaustubh and Montgomery, Jack and Tsai, Mike and Dwarakapuram, Saritha and Desai, Sanjay and Avidan, Nili and Ramani, Poorvaja and Narayanan, Karthik and Mathews, Ajit and Gopal, Sethu and Naumov, Maxim and Rao, Vijay and Noru, Krishna and Reddy, Harikrishna and Venkatapuram, Prahlad and Bjorlin, Alexis},
title = {MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589348},
doi = {10.1145/3579371.3589348},
abstract = {Meta has traditionally relied on using CPU-based servers for running inference workloads, specifically Deep Learning Recommendation Models (DLRM), but the increasing compute and memory requirements of these models have pushed the company towards using specialized solutions such as GPUs or other hardware accelerators. This paper describes the company's effort in constructing its first silicon specifically designed for recommendation systems; it describes the accelerator architecture and platform design, the software stack for enabling and optimizing PyTorch-based models and provides an initial performance evaluation. With our emerging software stack, we have made significant progress towards reaching the same or higher efficiency as the GPU: We averaged 0.9x perf/W across various DLRMs, and benchmarks show operators such as GEMMs reaching 2x perf/W. Finally, the paper describes the lessons we learned during this journey which can improve the performance and programmability of future generations of architecture.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {80},
numpages = {13},
keywords = {programmability, performance, recommendation systems, inference, machine learning, accelerators},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@article{naumov2019deep,
  author       = {Maxim Naumov and
                  Dheevatsa Mudigere and
                  Hao{-}Jun Michael Shi and
                  Jianyu Huang and
                  Narayanan Sundaraman and
                  Jongsoo Park and
                  Xiaodong Wang and
                  Udit Gupta and
                  Carole{-}Jean Wu and
                  Alisson G. Azzolini and
                  Dmytro Dzhulgakov and
                  Andrey Mallevich and
                  Ilia Cherniavskii and
                  Yinghai Lu and
                  Raghuraman Krishnamoorthi and
                  Ansha Yu and
                  Volodymyr Kondratenko and
                  Stephanie Pereira and
                  Xianjie Chen and
                  Wenlin Chen and
                  Vijay Rao and
                  Bill Jia and
                  Liang Xiong and
                  Misha Smelyanskiy},
  title        = {Deep Learning Recommendation Model for Personalization and Recommendation
                  Systems},
  journal      = {CoRR},
  volume       = {abs/1906.00091},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.00091},
  eprinttype    = {arXiv},
  eprint       = {1906.00091},
  timestamp    = {Wed, 15 Apr 2020 18:01:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1906-00091.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{gupta2020architectural,
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and Lee, Hsien-Hsin S. and Malevich, Andrey and Mudigere, Dheevatsa and Smelyanskiy, Mikhail and Xiong, Liang and Zhang, Xuan},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={The Architectural Implications of Facebook's DNN-Based Personalized Recommendation}, 
  year={2020},
  volume={},
  number={},
  pages={488-501},
  keywords={Computational modeling;Data centers;Computer architecture;Throughput;Optimization;Artificial intelligence;Videos},
  doi={10.1109/HPCA47549.2020.00047}}




@inproceedings{jain2023optimizing,
author = {Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and Das, Chita R.},
title = {Optimizing CPU Performance for Recommendation Systems At-Scale},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589112},
doi = {10.1145/3579371.3589112},
abstract = {Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {77},
numpages = {15},
keywords = {CPU, hyperthreading, prefetching, reuse distance, irregular memory accesses, embeddings, recommendation systems},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@INPROCEEDINGS{ke2020recnmp,
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S. and Li, Meng and Maher, Bert and Mudigere, Dheevatsa and Naumov, Maxim and Schatz, Martin and Smelyanskiy, Mikhail and Wang, Xiaodong and Reagen, Brandon and Wu, Carole-Jean and Hempstead, Mark and Zhang, Xuan},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing}, 
  year={2020},
  volume={},
  number={},
  pages={790-803},
  keywords={Systematics;Limiting;Scheduling algorithms;Energy conservation;Random access memory;Production;Parallel processing},
  doi={10.1109/ISCA45697.2020.00070}}


@ARTICLE{ke2021near,
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and Kim, KyungSoo and Jung, Jin and Yun, Ilkwon and Park, Sung Joo and Park, Hyunsun and Song, Joonho and Cho, Jeonghyeon and Sohn, Kyomin and Kim, Nam Sung and Lee, Hsien-Hsin S.},
  journal={IEEE Micro}, 
  title={Near-Memory Processing in Action: Accelerating Personalized Recommendation With AxDIMM}, 
  year={2022},
  volume={42},
  number={1},
  pages={116-127},
  keywords={Random access memory;Bandwidth;Throughput;Computational modeling;Hardware;Production;Field programmable gate arrays},
  doi={10.1109/MM.2021.3097700}}


@inproceedings {lai2023adaembed,
author = {Fan Lai and Wei Zhang and Rui Liu and William Tsai and Xiaohan Wei and Yuxi Hu and Sabin Devkota and Jianyu Huang and Jongsoo Park and Xing Liu and Zeliang Chen and Ellie Wen and Paul Rivera and Jie You and Chun-cheng Jason Chen and Mosharaf Chowdhury},
title = {$\{$AdaEmbed$\}$: Adaptive Embedding for $\{$Large-Scale$\}$ Recommendation Models},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {817--831},
url = {https://www.usenix.org/conference/osdi23/presentation/lai},
publisher = {USENIX Association},
month = jul
}

@inproceedings{mudigere2022software,
author = {Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and Yang, Jie (Amy) and Gao, Leon and Ivchenko, Dmytro and Basant, Aarti and Hu, Yuxi and Yang, Jiyan and Ardestani, Ehsan K. and Wang, Xiaodong and Komuravelli, Rakesh and Chu, Ching-Hsiang and Yilmaz, Serhat and Li, Huayu and Qian, Jiyuan and Feng, Zhuobo and Ma, Yinbin and Yang, Junjie and Wen, Ellie and Li, Hong and Yang, Lin and Sun, Chonglin and Zhao, Whitney and Melts, Dimitry and Dhulipala, Krishna and Kishore, KR and Graf, Tyler and Eisenman, Assaf and Matam, Kiran Kumar and Gangidi, Adi and Chen, Guoqiang Jerry and Krishnan, Manoj and Nayak, Avinash and Nair, Krishnakumar and Muthiah, Bharath and khorashadi, Mahmoud and Bhattacharya, Pallab and Lapukhov, Petr and Naumov, Maxim and Mathews, Ajit and Qiao, Lin and Smelyanskiy, Mikhail and Jia, Bill and Rao, Vijay},
title = {Software-hardware co-design for fast and scalable training of deep learning recommendation models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533727},
doi = {10.1145/3470496.3533727},
abstract = {Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\texttimes{} for training 12-trillion-parameter DLRM models deployed in production.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {993–1011},
numpages = {19},
location = {New York, New York},
series = {ISCA '22}
}



@inproceedings{gupta2020deeprecsys,
  title={Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference},
  author={Gupta, Udit and Hsia, Samuel and Saraph, Vikram and Wang, Xiaodong and Reagen, Brandon and Wei, Gu-Yeon and Lee, Hsien-Hsin S and Brooks, David and Wu, Carole-Jean},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={982--995},
  year={2020},
  organization={IEEE}
}

@inproceedings{ke2022hercules,
  title={Hercules: Heterogeneity-aware inference serving for at-scale personalized recommendation},
  author={Ke, Liu and Gupta, Udit and Hempstead, Mark and Wu, Carole-Jean and Lee, Hsien-Hsin S and Zhang, Xuan},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={141--154},
  year={2022},
  organization={IEEE}
}


@inproceedings{mudigere2022software_old,
  title={Software-hardware co-design for fast and scalable training of deep learning recommendation models},
  author={Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and others},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={993--1011},
  year={2022}
}

@inproceedings{firoozshahian2023mtia_old,
  title={MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
  author={Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--13},
  year={2023}
}

@inproceedings{jain2023optimizing_old,
  title={Optimizing CPU Performance for Recommendation Systems At-Scale},
  author={Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}

@inproceedings{gupta2021recpipe,
  title={Recpipe: Co-designing models and hardware to jointly optimize recommendation quality and performance},
  author={Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={870--884},
  year={2021}
}

@inproceedings{hsia2020cross,
  title={Cross-stack workload characterization of deep recommendation systems},
  author={Hsia, Samuel and Gupta, Udit and Wilkening, Mark and Wu, Carole-Jean and Wei, Gu-Yeon and Brooks, David},
  booktitle={2020 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={157--168},
  year={2020},
  organization={IEEE}
}

@inproceedings{lai2023adaembed_old,
  title={$\{$AdaEmbed$\}$: Adaptive Embedding for $\{$Large-Scale$\}$ Recommendation Models},
  author={Lai, Fan and Zhang, Wei and Liu, Rui and Tsai, William and Wei, Xiaohan and Hu, Yuxi and Devkota, Sabin and Huang, Jianyu and Park, Jongsoo and Liu, Xing and others},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={817--831},
  year={2023}
}

@inproceedings{lin2022building,
  title={Building a performance model for deep learning recommendation model training on gpus},
  author={Lin, Zhongyi and Feng, Louis and Ardestani, Ehsan K and Lee, Jaewon and Lundell, John and Kim, Changkyu and Kejariwal, Arun and Owens, John D},
  booktitle={2022 IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC)},
  pages={48--58},
  year={2022},
  organization={IEEE}
}




@inproceedings{gupta2020architectural_old,
  title={The architectural implications of facebook's dnn-based personalized recommendation},
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={488--501},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{zhou2019deep,
  title={Deep interest evolution network for click-through rate prediction},
  author={Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5941--5948},
  year={2019}
}


@article{naumov2019deep_old,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@inproceedings{kal2021space,
  title={Space: locality-aware processing in heterogeneous memory for personalized recommendations},
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={679--691},
  year={2021},
  organization={IEEE}
}

@article{luo2024benchmarking,
  title={Benchmarking and Dissecting the Nvidia Hopper GPU Architecture},
  author={Luo, Weile and Fan, Ruibo and Li, Zeyu and Du, Dayou and Wang, Qiang and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2402.13499},
  year={2024}
}



@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}






@inproceedings{lee2021merci,
  title={MERCI: efficient embedding reduction on commodity hardware via sub-query memoization},
  author={Lee, Yejin and Seo, Seong Hoon and Choi, Hyunji and Sul, Hyoung Uk and Kim, Soosung and Lee, Jae W and Ham, Tae Jun},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={302--313},
  year={2021}
}


@article{emma2005exploring,
  title={Exploring the limits of prefetching},
  author={Emma, Philip G and Hartstein, Allan and Puzak, Thomas R and Srinivasan, Viji},
  journal={IBM Journal of Research and Development},
  volume={49},
  number={1},
  pages={127--144},
  year={2005},
  publisher={IBM}
}

@misc{pytorch_2.1,
  author = {PyTorch},
  year = {2024},
  title = {{PyTorch 2.1.0}},
  howpublished = {\url{https://github.com/pytorch/pytorch/tree/v2.1.0
}}
}




@book{falsafi2022primer,
  title={A primer on hardware prefetching},
  author={Falsafi, Babak and Wenisch, Thomas F},
  year={2022},
  publisher={Springer Nature}
}


@article{lee2012prefetching,
  title={When prefetching works, when it doesn’t, and why},
  author={Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={1},
  pages={1--29},
  year={2012},
  publisher={ACM New York, NY, USA}
}



%-----------------------------------------------

@article{fu2023autoscratch,
  title={AutoScratch: ML-Optimized Cache Management for Inference-Oriented GPUs},
  author={Fu, Yaosheng and Bolotin, Evgeny and Jaleel, Aamer and Dalal, Gal and Mannor, Shie and Subag, Jacob and Korem, Noam and Behar, Michael and Nellans, David},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{zhang2023perks,
  title={PERKS: a Locality-Optimized Execution Model for Iterative Memory-bound GPU Applications},
  author={Zhang, Lingqi and Wahib, Mohamed and Chen, Peng and Meng, Jintao and Wang, Xiao and Endo, Toshio and Matsuoka, Satoshi},
  booktitle={Proceedings of the 37th International Conference on Supercomputing},
  pages={167--179},
  year={2023}
}


@inproceedings{sethi2022recshard,
  title={RecShard: statistical feature-based memory optimization for industry-scale neural recommendation},
  author={Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={344--358},
  year={2022}
}

@inproceedings{lui2021understanding,
  title={Understanding capacity-driven scale-out neural recommendation inference},
  author={Lui, Michael and Yetim, Yavuz and {\"O}zkan, {\"O}zg{\"u}r and Zhao, Zhuoran and Tsai, Shin-Yeh and Wu, Carole-Jean and Hempstead, Mark},
  booktitle={2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  pages={162--171},
  year={2021},
  organization={IEEE}
}

@article{ke2022disaggrec,
  title={DisaggRec: Architecting Disaggregated Systems for Large-Scale Personalized Recommendation},
  author={Ke, Liu and Zhang, Xuan and Lee, Benjamin and Suh, G Edward and Lee, Hsien-Hsin S},
  journal={arXiv preprint arXiv:2212.00939},
  year={2022}
}

@inproceedings{hwang2020centaur,
  title={Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations},
  author={Hwang, Ranggi and Kim, Taehun and Kwon, Youngeun and Rhu, Minsoo},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={968--981},
  year={2020},
  organization={IEEE}
}

@article{ke2021near_old,
  title={Near-memory processing in action: Accelerating personalized recommendation with axdimm},
  author={Ke, Liu and Zhang, Xuan and So, Jinin and Lee, Jong-Geon and Kang, Shin-Haeng and Lee, Sukhan and Han, Songyi and Cho, YeonGon and Kim, Jin Hyun and Kwon, Yongsuk and others},
  journal={IEEE Micro},
  volume={42},
  number={1},
  pages={116--127},
  year={2021},
  publisher={IEEE}
}

@inproceedings{kwon2019tensordimm,
  title={Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning},
  author={Kwon, Youngeun and Lee, Yunjae and Rhu, Minsoo},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={740--753},
  year={2019}
}

@inproceedings{ke2020recnmp_old,
  title={Recnmp: Accelerating personalized recommendation with near-memory processing},
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={790--803},
  year={2020},
  organization={IEEE}
}

@inproceedings{ye2023grace,
  title={GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference},
  author={Ye, Haojie and Vedula, Sanketh and Chen, Yuhan and Yang, Yichen and Bronstein, Alex and Dreslinski, Ronald and Mudge, Trevor and Talati, Nishil},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={282--301},
  year={2023}
}

@article{adnan2021accelerating,
  title={Accelerating recommendation system training by leveraging popular choices},
  author={Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J},
  journal={arXiv preprint arXiv:2103.00686},
  year={2021}
}

%-------------------------------------------------

@inproceedings{kwon2022training,
  title={Training personalized recommendation systems from (GPU) scratch: Look forward not backwards},
  author={Kwon, Youngeun and Rhu, Minsoo},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={860--873},
  year={2022}
}

%%%%%%%%%%%% GPU hardware and resources




@inproceedings{adufu2023l2,
  title={L2 Cache Access Pattern Analysis using Static Profiling of an Application},
  author={Adufu, Theodora and Kim, Yoonhee},
  booktitle={2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)},
  pages={97--102},
  year={2023},
  organization={IEEE}
}

%---------------------------

@inproceedings{adufu2023optimizing,
  title={Optimizing Performance Using GPU Cache Data Residency Based on Application’s Access Patterns},
  author={Adufu, Theodora and Kim, Yoonhee},
  booktitle={2023 24st Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  pages={42--47},
  year={2023},
  organization={IEEE}
}



%-----------------------------------------recsys missing; till here, alignment correct

%%%%%%%%%%%% related works on prefetching on GPUs

@inproceedings{jog2013orchestrated,
  title={Orchestrated scheduling and prefetching for GPGPUs},
  author={Jog, Adwait and Kayiran, Onur and Mishra, Asit K and Kandemir, Mahmut T and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R},
  booktitle={Proceedings of the 40th Annual International Symposium on Computer Architecture},
  pages={332--343},
  year={2013}
}

@inproceedings{sethia2013apogee,
  title={APOGEE: Adaptive prefetching on GPUs for energy efficiency},
  author={Sethia, Ankit and Dasika, Ganesh and Samadi, Mehrzad and Mahlke, Scott},
  booktitle={Proceedings of the 22nd international conference on Parallel architectures and compilation techniques},
  pages={73--82},
  year={2013},
  organization={IEEE}
}

@article{oh2018adaptive,
  title={Adaptive cooperation of prefetching and warp scheduling on gpus},
  author={Oh, Yunho and Kim, Keunsoo and Yoon, Myung Kuk and Park, Jong Hyun and Park, Yongjun and Annavaram, Murali and Ro, Won Woo},
  journal={IEEE Transactions on Computers},
  volume={68},
  number={4},
  pages={609--616},
  year={2018},
  publisher={IEEE}
}

@inproceedings{wu2011pacman,
  title={PACMan: prefetch-aware cache management for high performance caching},
  author={Wu, Carole-Jean and Jaleel, Aamer and Martonosi, Margaret and Steely Jr, Simon C and Emer, Joel},
  booktitle={Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={442--453},
  year={2011}
}

%%%%%%%%%%%% related works on multi-threading (warp level parallelism) on GPUs

@article{jog2013owl,
  title={OWL: cooperative thread array aware scheduling techniques for improving GPGPU performance},
  author={Jog, Adwait and Kayiran, Onur and Chidambaram Nachiappan, Nachiappan and Mishra, Asit K and Kandemir, Mahmut T and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R},
  journal={ACM SIGPLAN Notices},
  volume={48},
  number={4},
  pages={395--406},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kayiran2013neither,
  title={Neither more nor less: Optimizing thread-level parallelism for GPGPUs},
  author={Kay{\i}ran, Onur and Jog, Adwait and Kandemir, Mahmut T and Das, Chita R},
  booktitle={Proceedings of the 22nd international conference on Parallel architectures and compilation techniques},
  pages={157--166},
  year={2013},
  organization={IEEE}
}

@inproceedings{song2023ugache,
  title={UGACHE: A Unified GPU Cache for Embedding-based Deep Learning},
  author={Song, Xiaoniu and Zhang, Yiwen and Chen, Rong and Chen, Haibo},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={627--641},
  year={2023}
}

@article{yuan2023everest,
  title={Everest: GPU-Accelerated System For Mining Temporal Motifs},
  author={Yuan, Yichao and Ye, Haojie and Kaza, Sanketh Vedula Wynn and Talati, Nishil},
  journal={arXiv preprint arXiv:2310.02800},
  year={2023}
}

@inproceedings{sethia2015mascar,
  title={Mascar: Speeding up GPU warps by reducing memory pitstops},
  author={Sethia, Ankit and Jamshidi, D Anoushe and Mahlke, Scott},
  booktitle={2015 IEEE 21st International symposium on high performance computer architecture (HPCA)},
  pages={174--185},
  year={2015},
  organization={IEEE}
}

%%%%%%%%%%%% related works on register file virtualization

@inproceedings{jeon2015gpu,
  title={GPU register file virtualization},
  author={Jeon, Hyeran and Ravi, Gokul Subramanian and Kim, Nam Sung and Annavaram, Murali},
  booktitle={Proceedings of the 48th International Symposium on Microarchitecture},
  pages={420--432},
  year={2015}
}

@article{voitsechov2018software,
  title={Software-directed techniques for improved gpu register file utilization},
  author={Voitsechov, Dani and Zulfiqar, Arslan and Stephenson, Mark and Gebhart, Mark and Keckler, Stephen W},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={15},
  number={3},
  pages={1--23},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{oh2018finereg,
  title={FineReg: Fine-grained register file management for augmenting GPU throughput},
  author={Oh, Yunho and Yoon, Myung Kuk and Song, William J and Ro, Won Woo},
  booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={364--376},
  year={2018},
  organization={IEEE}
}

@INPROCEEDINGS{nebula,
  author={Singh, Sonali and Sarma, Anup and Jao, Nicholas and Pattnaik, Ashutosh and Lu, Sen and Yang, Kezhou and Sengupta, Abhronil and Narayanan, Vijaykrishnan and Das, Chita R.},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={NEBULA: A Neuromorphic Spin-Based Ultra-Low Power Architecture for SNNs and ANNs}, 
  year={2020},
  volume={},
  number={},
  pages={363-376},
  keywords={Neural nets;low power design;domain-specific architectures;memory technologies},
  doi={10.1109/ISCA45697.2020.00039}}

@INPROCEEDINGS{skipper,
  author={Singh, Sonali and Sarma, Anup and Lu, Sen and Sengupta, Abhronil and Kandemir, Mahmut T. and Neftci, Emre and Narayanan, Vijaykrishnan and Das, Chita R.},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Skipper: Enabling efficient SNN training through activation-checkpointing and time-skipping}, 
  year={2022},
  volume={},
  number={},
  pages={565-581},
  keywords={Training;Checkpointing;Costs;Microarchitecture;Neuromorphics;Memory management;Neural networks;SNN;BPTT;compute and memory},
  doi={10.1109/MICRO56248.2022.00047}}

@INPROCEEDINGS{stash,
  author={Sharma, Aakash and Bhasi, Vivek M. and Singh, Sonali and Jain, Rishabh and Gunasekaran, Jashwant Raj and Mitra, Subrata and Kandemir, Mahmut Taylan and Kesidis, George and Das, Chita R.},
  booktitle={2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Stash: A Comprehensive Stall-Centric Characterization of Public Cloud VMs for Distributed Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  keywords={Training;Deep learning;Cloud computing;Costs;Image recognition;Graphics processing units;Training data;Public cloud GPU;Cloud computing;Characterization;Distributed deep learning},
  doi={10.1109/ICDCS57875.2023.00023}}


@misc{arxiv_stash,
      title={Analysis of Distributed Deep Learning in the Cloud}, 
      author={Aakash Sharma and Vivek M. Bhasi and Sonali Singh and Rishabh Jain and Jashwant Raj Gunasekaran and Subrata Mitra and Mahmut Taylan Kandemir and George Kesidis and Chita R. Das},
      year={2022},
      eprint={2208.14344},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.14344}, 
}

@misc{LLM_survey,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06196}, 
}

@inproceedings{jha2024mem,
  title={Mem-Rec: Memory Efficient Recommendation System using Alternative Representation},
  author={Jha, Gopi Krishna and Thomas, Anthony and Jain, Nilesh and Gobriel, Sameh and Rosing, Tajana and Iyer, Ravi},
  booktitle={Asian Conference on Machine Learning},
  pages={518--533},
  year={2024},
  organization={PMLR}
}

@inproceedings{matam2024quickupdate,
  title={$\{$QuickUpdate$\}$: a $\{$Real-Time$\}$ Personalization System for $\{$Large-Scale$\}$ Recommendation Models},
  author={Matam, Kiran Kumar and Ramezani, Hani and Wang, Fan and Chen, Zeliang and Dong, Yue and Ding, Maomao and Zhao, Zhiwei and Zhang, Zhengyu and Wen, Ellie and Eisenman, Assaf},
  booktitle={21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={731--744},
  year={2024}
}

@article{zha2022dreamshard,
  title={Dreamshard: Generalizable embedding table placement for recommender systems},
  author={Zha, Daochen and Feng, Louis and Tan, Qiaoyu and Liu, Zirui and Lai, Kwei-Herng and Bhushanam, Bhargav and Tian, Yuandong and Kejariwal, Arun and Hu, Xia},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15190--15203},
  year={2022}
}

@misc{amd-zendnn,
  author = {AMD},
  year = {2023},
  title = {{AMD Zen Deep Neural Network}},
  howpublished = {"\url{https://www.amd.com/en/developer/zendnn.html}"}
}

@misc{fbgemm,
  author = {Facebook},
  year = {2023},
  title = {{Facebook Generalized Matrix Multiplication}},
  howpublished = {"\url{https://github.com/pytorch/FBGEMM}"}
}

@misc{ipex,
  author = {Intel},
  year = {2023},
  title = {{Intel® Extension for PyTorch*}},
  howpublished = {"\url{https://github.com/intel/intel-extension-for-pytorch}"}
}

@misc{dlrm-dataset,
  author = {Meta},
  year = {2023},
  title = {{Embedding lookup Production dataset}},
  howpublished = {"\url{https://github.com/facebookresearch/dlrm_datasets}"}
}

@misc{aws,
  author = {Amazon},
  year = {2024},
  title = {{Amazon EC2 Instance Types – AWS}},
  howpublished = {"\url{https://aws.amazon.com/ec2/instance-types/}"}
}

@misc{gcp,
  author = {Google},
  year = {2024},
  title = {{Machine families resource and comparison guide | Compute Engine Documentation | Google Cloud}},
  howpublished = {"\url{https://cloud.google.com/compute/docs/machine-resource}"}
}

@misc{azure,
  author = {Microsoft},
  year = {2024},
  title = {{Cloud Computing Services | Microsoft Azure}},
  howpublished = {"\url{https://azure.microsoft.com/en-us/}"}
}



@inproceedings{Paszke_PyTorch_An_Imperative_2019,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'Alché-Buc, F. and Fox, E. and Garnett, R.},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}

@inproceedings{ardestani2022supporting,
  title={Supporting massive DLRM inference through software defined memory},
  author={Ardestani, Ehsan K and Kim, Changkyu and Lee, Seung Jae and Pan, Luoshang and Axboe, Jens and Rampersad, Valmiki and Agrawal, Banit and Yu, Fuxun and Yu, Ansha and Le, Trung and others},
  booktitle={2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)},
  pages={302--312},
  year={2022},
  organization={IEEE}
}

@inproceedings{gupta2020architectural,
  title={The architectural implications of facebook's dnn-based personalized recommendation},
  author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={488--501},
  year={2020},
  organization={IEEE}
}

@INPROCEEDINGS {acun2021understanding,
author = {B. Acun and M. Murphy and X. Wang and J. Nie and C. Wu and K. Hazelwood},
booktitle = {2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
title = {Understanding Training Efficiency of Deep Learning Recommendation Models at Scale},
year = {2021},
volume = {},
issn = {},
pages = {802-814},
abstract = {The use of GPUs has proliferated for machine learning workflows and is now considered mainstream for many deep learning models. Meanwhile, when training state-of-the-art personal recommendation models, which consume the highest number of compute cycles at our large-scale datacenters, the use of GPUs came with various challenges due to having both compute-intensive and memory-intensive components. GPU performance and efficiency of these recommendation models are largely affected by model architecture configurations such as dense and sparse features, MLP dimensions. Furthermore, these models often contain large embedding tables that do not fit into limited GPU memory. The goal of this paper is to explain the intricacies of using GPUs for training recommendation models, factors affecting hardware efficiency at scale, and learnings from a new scale-up GPU server design, Zion.},
keywords = {training;deep learning;computational modeling;memory management;graphics processing units;production;throughput},
doi = {10.1109/HPCA51647.2021.00072},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA51647.2021.00072},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@inproceedings {eisenman2022check,
author = {Assaf Eisenman and Kiran Kumar Matam and Steven Ingram and Dheevatsa Mudigere and Raghuraman Krishnamoorthi and Krishnakumar Nair and Misha Smelyanskiy and Murali Annavaram},
title = {{Check-N-Run}: a Checkpointing System for Training Deep Learning Recommendation Models},
booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
year = {2022},
isbn = {978-1-939133-27-4},
address = {Renton, WA},
pages = {929--943},
url = {https://www.usenix.org/conference/nsdi22/presentation/eisenman},
publisher = {USENIX Association},
month = apr
}

@INPROCEEDINGS{gupta2020deeprecsys,
  author={Gupta, Udit and Hsia, Samuel and Saraph, Vikram and Wang, Xiaodong and Reagen, Brandon and Wei, Gu-Yeon and Lee, Hsien-Hsin S. and Brooks, David and Wu, Carole-Jean},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference}, 
  year={2020},
  volume={},
  number={},
  pages={982-995},
  doi={10.1109/ISCA45697.2020.00084}}

@inproceedings{gupta2021recpipe,
author = {Gupta, Udit and Hsia, Samuel and Zhang, Jeff and Wilkening, Mark and Pombra, Javin and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Wu, Carole-Jean and Brooks, David},
title = {RecPipe: Co-Designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480127},
doi = {10.1145/3466752.3480127},
abstract = {Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 \texttimes{} and 6 \texttimes{}.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {870–884},
numpages = {15},
keywords = {deep learning, datacenter, hardware accelerator, personalized recommendation},
location = {Virtual Event, Greece},
series = {MICRO '21}
}


@INPROCEEDINGS{kalamkar2020optimizing,
  author={Kalamkar, Dhiraj and Georganas, Evangelos and Srinivasan, Sudarshan and Chen, Jianping and Shiryaev, Mikhail and Heinecke, Alexander},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={Optimizing Deep Learning Recommender Systems Training on CPU Cluster Architectures}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/SC41405.2020.00047}}

@inproceedings{kwon2022training,
author = {Kwon, Youngeun and Rhu, Minsoo},
title = {Training Personalized Recommendation Systems from (GPU) Scratch: Look Forward Not Backwards},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527386},
doi = {10.1145/3470496.3527386},
abstract = {Personalized recommendation models (RecSys) are one of the most popular machine learning workload serviced by hyperscalers. A critical challenge of training RecSys is its high memory capacity requirements, reaching hundreds of GBs to TBs of model size. In RecSys, the so-called embedding layers account for the majority of memory usage so current systems employ a hybrid CPU-GPU design to have the large CPU memory store the memory hungry embedding layers. Unfortunately, training embeddings involve several memory bandwidth intensive operations which is at odds with the slow CPU memory, causing performance overheads. Prior work proposed to cache frequently accessed embeddings inside GPU memory as means to filter down the embedding layer traffic to CPU memory, but this paper observes several limitations with such cache design. In this work, we present a fundamentally different approach in designing embedding caches for RecSys. Our proposed ScratchPipe architecture utilizes unique properties of RecSys training to develop an embedding cache that not only sees the past but also the "future" cache accesses. ScratchPipe exploits such property to guarantee that the active working set of embedding layers can "always" be captured inside our proposed cache design, enabling embedding layer training to be conducted at GPU memory speed.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {860–873},
numpages = {14},
keywords = {systems for machine learning, recommendation system, neural network, memory architecture, graphics processing unit (GPU)},
location = {New York, New York},
series = {ISCA '22}
}

@INPROCEEDINGS {ke2022hercules,
author = {L. Ke and U. Gupta and M. Hempstead and C. Wu and H. S. Lee and X. Zhang},
booktitle = {2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
title = {Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation},
year = {2022},
volume = {},
issn = {},
pages = {141-154},
abstract = {Personalized recommendation is an important class of deep-learning applications that powers a large collection of internet services and consumes a considerable amount of datacenter resources. As the scale of production-grade recommendation systems continues to grow, optimizing their serving performance and efficiency in a heterogeneous datacenter is important and can translate into infrastructure capacity saving. In this paper, we propose Hercules, an optimized framework for personalized recommendation inference serving that targets diverse industry-representative models and cloud-scale heterogeneous systems. Hercules performs a two-stage optimization procedure — offline profiling and online serving. The first stage searches the large under-explored task scheduling space with a gradient-based search algorithm achieving up to 9.0× latency-bounded throughput improvement on individual servers; it also identifies the optimal heterogeneous server architecture for each recommendation workload. The second stage performs heterogeneity-aware cluster provisioning to optimize resource mapping and allocation in response to fluctuating diurnal loads. The proposed cluster scheduler in Hercules achieves 47.7% cluster capacity saving and reduces the provisioned power by 23.7% over a state-of-the-art greedy scheduler.},
keywords = {job shop scheduling;web and internet services;computer architecture;throughput;real-time systems;servers;resource management;task analysis;recommender systems;optimization},
doi = {10.1109/HPCA53966.2022.00019},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA53966.2022.00019},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {apr}
}

@inproceedings{mudigere2022software,
author = {Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and Yang, Jie (Amy) and Gao, Leon and Ivchenko, Dmytro and Basant, Aarti and Hu, Yuxi and Yang, Jiyan and Ardestani, Ehsan K. and Wang, Xiaodong and Komuravelli, Rakesh and Chu, Ching-Hsiang and Yilmaz, Serhat and Li, Huayu and Qian, Jiyuan and Feng, Zhuobo and Ma, Yinbin and Yang, Junjie and Wen, Ellie and Li, Hong and Yang, Lin and Sun, Chonglin and Zhao, Whitney and Melts, Dimitry and Dhulipala, Krishna and Kishore, KR and Graf, Tyler and Eisenman, Assaf and Matam, Kiran Kumar and Gangidi, Adi and Chen, Guoqiang Jerry and Krishnan, Manoj and Nayak, Avinash and Nair, Krishnakumar and Muthiah, Bharath and khorashadi, Mahmoud and Bhattacharya, Pallab and Lapukhov, Petr and Naumov, Maxim and Mathews, Ajit and Qiao, Lin and Smelyanskiy, Mikhail and Jia, Bill and Rao, Vijay},
title = {Software-Hardware Co-Design for Fast and Scalable Training of Deep Learning Recommendation Models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533727},
doi = {10.1145/3470496.3533727},
abstract = {Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\texttimes{} for training 12-trillion-parameter DLRM models deployed in production.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {993–1011},
numpages = {19},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{sethi2022recshard,
author = {Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
title = {RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale Neural Recommendation},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507777},
doi = {10.1145/3503222.3507777},
abstract = {We propose RecShard, a fine-grained embedding table (EMB) partitioning and placement technique for deep learning recommendation models (DLRMs). RecShard is designed based on two key observations. First, not all EMBs are equal, nor all rows within an EMB are equal in terms of access patterns. EMBs exhibit distinct memory characteristics, providing performance optimization opportunities for intelligent EMB partitioning and placement across a tiered memory hierarchy. Second, in modern DLRMs, EMBs function as hash tables. As a result, EMBs display interesting phenomena, such as the birthday paradox, leaving EMBs severely under-utilized. RecShard determines an optimal EMB sharding strategy for a set of EMBs based on training data distributions and model characteristics, along with the bandwidth characteristics of the underlying tiered memory hierarchy. In doing so, RecShard achieves over 6 times higher EMB training throughput on average for capacity constrained DLRMs. The throughput increase comes from improved EMB load balance by over 12 times and from the reduced access to the slower memory by over 87 times.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {344–358},
numpages = {15},
keywords = {Deep learning recommendation models, AI training systems, Neural networks, Memory optimization},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@INPROCEEDINGS{sun2022rmssd,
  author={Sun, Xuan and Wan, Hu and Li, Qiao and Yang, Chia-Lin and Kuo, Tei-Wei and Xue, Chun Jason},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={RM-SSD: In-Storage Computing for Large-Scale Recommendation Inference}, 
  year={2022},
  volume={},
  number={},
  pages={1056-1070},
  doi={10.1109/HPCA53966.2022.00081}}


@inproceedings{wilkening2021recssd,
author = {Wilkening, Mark and Gupta, Udit and Hsia, Samuel and Trippel, Caroline and Wu, Carole-Jean and Brooks, David and Wei, Gu-Yeon},
title = {RecSSD: Near Data Processing for Solid State Drive Based Recommendation Inference},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446763},
doi = {10.1145/3445814.3446763},
abstract = {Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2\texttimes{} compared to using COTS SSDs across eight industry-representative models.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {717–729},
numpages = {13},
keywords = {solid state drives, neural networks, near data processing},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{zhao2022understanding,
author = {Zhao, Mark and Agarwal, Niket and Basant, Aarti and Gedik, Bu\u{g}ra and Pan, Satadru and Ozdal, Mustafa and Komuravelli, Rakesh and Pan, Jerry and Bao, Tianshu and Lu, Haowei and Narayanan, Sundaram and Langman, Jack and Wilfong, Kevin and Rastogi, Harsha and Wu, Carole-Jean and Kozyrakis, Christos and Pol, Parik},
title = {Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training: Industrial Product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533044},
doi = {10.1145/3470496.3533044},
abstract = {Datacenter-scale AI training clusters consisting of thousands of domain-specific accelerators (DSA) are used to train increasingly-complex deep learning models. These clusters rely on a data storage and ingestion (DSI) pipeline, responsible for storing exabytes of training data and serving it at tens of terabytes per second. As DSAs continue to push training efficiency and throughput, the DSI pipeline is becoming the dominating factor that constrains the overall training performance and capacity. Innovations that improve the efficiency and performance of DSI systems and hardware are urgent, demanding a deep understanding of DSI characteristics and infrastructure at scale.This paper presents Meta's end-to-end DSI pipeline, composed of a central data warehouse built on distributed storage and a Data PreProcessing Service that scales to eliminate data stalls. We characterize how hundreds of models are collaboratively trained across geo-distributed datacenters via diverse and continuous training jobs. These training jobs read and heavily filter massive and evolving datasets, resulting in popular features and samples used across training jobs. We measure the intense network, memory, and compute resources required by each training job to preprocess samples during training. Finally, we synthesize key takeaways based on our production infrastructure characterization. These include identifying hardware bottlenecks, discussing opportunities for heterogeneous DSI hardware, motivating research in datacenter scheduling and benchmark datasets, and assimilating lessons learned in optimizing DSI infrastructure.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1042–1057},
numpages = {16},
keywords = {data ingestion, distributed systems, data storage, machine learning systems, databases},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{shi2020compositional,
author = {Shi, Hao-Jun Michael and Mudigere, Dheevatsa and Naumov, Maxim and Yang, Jiyan},
title = {Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403059},
doi = {10.1145/3394486.3403059},
abstract = {Modern deep learning-based recommendation systems exploit hundreds to thousands of different categorical features, each with millions of different categories ranging from clicks to posts. To respect the natural diversity within the categorical data, embeddings map each category to a unique dense representation within an embedded space. Since each categorical feature could take on as many as tens of millions of different possible categories, the embedding tables form the primary memory bottleneck during both training and inference. We propose a novel approach for reducing the embedding size in an end-to-end fashion by exploiting complementary partitions of the category set to produce a unique embedding vector for each category without explicit definition. By storing multiple smaller embedding tables based on each complementary partition and combining embeddings from each table, we define a unique embedding for each category at smaller cost. This approach may be interpreted as using a specific fixed codebook to ensure uniqueness of each category's representation. Our experimental results demonstrate the effectiveness of our approach over the hashing trick for reducing the size of the embedding tables in terms of model loss and accuracy, while retaining a similar reduction in the number of parameters.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {165–175},
numpages = {11},
keywords = {recommendation systems, model compression, embeddings},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{liu2023accelerating,
author = {Liu, Haifeng and Zheng, Long and Huang, Yu and Liu, Chaoqiang and Ye, Xiangyu and Yuan, Jingrui and Liao, Xiaofei and Jin, Hai and Xue, Jingling},
title = {Accelerating Personalized Recommendation with Cross-Level Near-Memory Processing},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589101},
doi = {10.1145/3579371.3589101},
abstract = {The memory-intensive embedding layers of the personalized recommendation systems are the performance bottleneck as they demand large memory bandwidth and exhibit irregular and sparse memory access patterns. Recent studies propose near memory processing (NMP) to accelerate memory-bound embedding operations. However, due to the load imbalance caused by the skewed access frequency of the embedding data, existing NMP solutions that exploit fine-grained memory parallelism fail to translate the increasingly massive internal bandwidth to performance improvements, leading to resource underutilization and hardware overhead.We propose an efficient yet practical fine-grained NMP accelerator for embedding operations. We architect ReCross, a cross-level NMP architecture that exploits rank, bank-group, and subarray-level memory parallelism in a unified DIMM-based memory system by supporting rank, bank-group, and bank-level NMP to accommodate various bandwidth requirements of embedding data. In addition, we present a novel embedding partitioning technique to quantify the bandwidth requirements of embedding tables and allocate them to appropriate NMP levels. ReCross innovatively collaborates the data and architecture characteristics for the NMP embedding layer acceleration, achieving high resource utilization and performance. Our evaluation shows that ReCross outperforms a state-of-the-art bank-group level NMP solution, TRiM-G, by 2.5\texttimes{} with nearly the same area overhead and bank-level NMP solution, TRiM-B, by 1.8\texttimes{} with an area overhead reduction of 4\texttimes{}.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {66},
numpages = {13},
keywords = {DRAM, memory system, near-memory-processing, DIMM},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{rishabh_isca23,
author = {Jain, Rishabh and Cheng, Scott and Kalagi, Vishwas and Sanghavi, Vrushabh and Kaul, Samvit and Arunachalam, Meena and Maeng, Kiwan and Jog, Adwait and Sivasubramaniam, Anand and Kandemir, Mahmut Taylan and Das, Chita R.},
title = {Optimizing CPU Performance for Recommendation Systems At-Scale},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589112},
doi = {10.1145/3579371.3589112},
abstract = {Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {77},
numpages = {15},
keywords = {reuse distance, hyperthreading, prefetching, CPU, recommendation systems, irregular memory accesses, embeddings},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@inproceedings{ibrahim2022efficient,
author = {Ibrahim, Mohamed Assem and Kayiran, Onur and Aga, Shaizeen},
title = {Efficient Cache Utilization via Model-Aware Data Placement for Recommendation Models},
year = {2022},
isbn = {9781450385701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488423.3519317},
doi = {10.1145/3488423.3519317},
abstract = {Deep neural network (DNN) based recommendation models (RMs) represent a class of critical workloads that are broadly used in social media, entertainment content, and online businesses. Given their pervasive usage, understanding the memory subsystem behavior of these models is crucial, particularly from the perspective of future memory subsystem design. To this end, in this work, we first do an in-depth memory footprint and traffic analysis of emerging RMs. We observe that emerging RMs will severely stress future (and possibly larger) caches and memories. To address this challenge, we make the key observation that a data placement strategy that is aware of the components within these models (as opposed to one that considers the entire model as a whole) stands a better chance of relieving the stress on the memory subsystem. Specifically, of the two key components of these models, namely, embedding tables and multi-layer perceptron layers, we show how we can exploit the locality of memory accesses to embedding tables to come up with a more nuanced data placement scheme. We demonstrate how our proposed data placement strategy can reduce overall memory traffic (approximately 32\%) while improving performance (up to 1.99 \texttimes{}). We argue that memory subsystems that are more amenable to residency controls stand a better chance to address the needs of emerging models.},
booktitle = {Proceedings of the International Symposium on Memory Systems},
articleno = {2},
numpages = {11},
keywords = {Neural Networks, DLRM, Embedding Tables, Caches, Recommendation Models, MLPs},
location = {Washington DC, DC, USA},
series = {MEMSYS '21}
}

@INPROCEEDINGS{kal2021space,
  author={Kal, Hongju and Lee, Seokmin and Ko, Gun and Ro, Won Woo},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={SPACE: Locality-Aware Processing in Heterogeneous Memory for Personalized Recommendations}, 
  year={2021},
  volume={},
  number={},
  pages={679-691},
  doi={10.1109/ISCA52012.2021.00059}}


@inproceedings{ke2020recnmp,
author = {Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S. and Li, Meng and Maher, Bert and Mudigere, Dheevatsa and Naumov, Maxim and Schatz, Martin and Smelyanskiy, Mikhail and Wang, Xiaodong and Reagen, Brandon and Wu, Carole-Jean and Hempstead, Mark and Zhang, Xuan},
title = {RecNMP: Accelerating Personalized Recommendation with near-Memory Processing},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00070},
doi = {10.1109/ISCA45697.2020.00070},
abstract = {Personalized recommendation systems leverage deep learning models and account for the majority of data center AI cycles. Their performance is dominated by memory-bound sparse embedding operations with unique irregular memory access patterns that pose a fundamental challenge to accelerate. This paper proposes a lightweight, commodity DRAM compliant, near-memory processing solution to accelerate personalized recommendation inference. The in-depth characterization of production-grade recommendation models shows that embedding operations with high model-, operator- and data-level parallelism lead to memory bandwidth saturation, limiting recommendation inference performance. We propose RecNMP which provides a scalable solution to improve system throughput, supporting a broad range of sparse embedding models. RecNMP is specifically tailored to production environments with heavy co-location of operators on a single server. Several hardware/software co-optimization techniques such as memory-side caching, table-aware packet scheduling, and hot entry profiling are studied, providing up to 9.8x memory latency speedup over a highly-optimized baseline. Overall, RecNMP offers 4.2x throughput improvement and 45.8\% memory energy savings.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {790–803},
numpages = {14},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{lee2021merci,
author = {Lee, Yejin and Seo, Seong Hoon and Choi, Hyunji and Sul, Hyoung Uk and Kim, Soosung and Lee, Jae W. and Ham, Tae Jun},
title = {MERCI: Efficient Embedding Reduction on Commodity Hardware via Sub-Query Memoization},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446717},
doi = {10.1145/3445814.3446717},
abstract = {Deep neural networks (DNNs) with embedding layers are widely adopted to capture complex relationships among entities within a dataset. Embedding layers aggregate multiple embeddings — a dense vector used to represent the complicated nature of a data feature— into a single embedding; such operation is called embedding reduction. Embedding reduction spends a significant portion of its runtime on reading embeddings from memory and thus is known to be heavily memory-bandwidth-bound. Recent works attempt to accelerate this critical operation, but they often require either hardware modifications or emerging memory technologies, which makes it hardly deployable on commodity hardware. Thus, we propose MERCI, Memoization for Embedding Reduction with ClusterIng, a novel memoization framework for efficient embedding reduction. MERCI provides a mechanism for memoizing partial aggregation of correlated embeddings and retrieving the memoized partial result at a low cost. MERCI substantially reduces the number of memory accesses by 44\% (29\%), leading to 102\% (74\%) throughput improvement on real machines and 40.2\% (28.6\%) energy savings at the expense of 8\texttimes{}(1\texttimes{}) additional memory usage.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {302–313},
numpages = {12},
keywords = {Memoization, Recommender Systems, Embedding Lookup},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{gupta2021training,
author = {Gupta, Vipul and Choudhary, Dhruv and Tang, Peter and Wei, Xiaohan and Wang, Xing and Huang, Yuzhen and Kejariwal, Arun and Ramchandran, Kannan and Mahoney, Michael W.},
title = {Training Recommender Systems at Scale: Communication-Efficient Model and Data Parallelism},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467080},
doi = {10.1145/3447548.3467080},
abstract = {In this paper, we consider hybrid parallelism---a paradigm that employs both Data Parallelism (DP) and Model Parallelism (MP)---to scale distributed training of large recommendation models. We propose a compression framework called Dynamic Communication Thresholding (DCT) for communication-efficient hybrid training. DCT filters the entities to be communicated across the network through a simple hard-thresholding function, allowing only the most relevant information to pass through. For communication efficient DP, DCT compresses the parameter gradients sent to the parameter server during model synchronization. The threshold is updated only once every few thousand iterations to reduce the computational overhead of compression. For communication efficient MP, DCT incorporates a novel technique to compress the activations and gradients sent across the network during the forward and backward propagation, respectively. This is done by identifying and updating only the most relevant neurons of the neural network for each training sample in the data. We evaluate DCT on publicly available natural language processing and recommender models and datasets, as well as recommendation systems used in production at Facebook. DCT reduces communication by at least 100x and 20x during DP and MP, respectively. The algorithm has been deployed in production, and it improves end-to-end training time for a state-of-the-art industrial recommender model by 37\%, without any loss in performance.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {2928–2936},
numpages = {9},
keywords = {recommender systems, neural networks, hybrid parallelism, distributed training},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{wang2022elrec,
author = {Wang, Zheng and Wang, Yuke and Feng, Boyuan and Mudigere, Dheevatsa and Muthiah, Bharath and Ding, Yufei},
title = {EL-Rec: Efficient Large-Scale Recommendation Model Training via Tensor-Train Embedding Table},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {Deep learning Recommendation Models (DLRMs) plays an important role in various application domains. However, existing DLRM training systems require a large number of GPUs due to the memory-intensive embedding tables. To this end, we propose EL-Rec, an efficient computing framework harnessing the Tensor-train (TT) technique to democratize the training of large-scale DLRMs with limited GPU resources. Specifically, EL-Rec optimizes TT decomposition based on key computation primitives of embedding tables and implements a high-performance compressed embedding table which is a drop-in replacement of Pytorch API. EL-Rec introduces an index reordering technique to harvest the performance gains from both local and global information of training inputs. EL-Rec also highlights a pipeline training paradigm to eliminate the communication overhead between the host memory and the training worker. Comprehensive experiments demonstrate that EL-Rec can handle the largest publicly available DLRM dataset with a single GPU and achieves 3\texttimes{} speedup over the state-of-the-art DLRM frameworks.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {70},
numpages = {14},
keywords = {recommender systems, high performance computing, deep learning},
location = {Dallas, Texas},
series = {SC '22}
}

@inproceedings{balasubramanian2021cdlrm,
author = {Balasubramanian, Keshav and Alshabanah, Abdulla and Choe, Joshua D and Annavaram, Murali},
title = {CDLRM: Look Ahead Caching for Scalable Training of Recommendation Models},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474246},
doi = {10.1145/3460231.3474246},
abstract = {Deep learning recommendation models (DLRMs) are typically composed of two sets of parameters: large embedding tables to handle sparse categorical inputs, and neural networks such as multi-layer perceptrons (MLPs) to handle dense non-categorical inputs. Current DLRM training practices keep both these parameters in GPU memory. But as the size of the embedding tables grow, this practice of storing model parameters in GPU memory requires dozens or even hundreds of GPUs. This is an unsustainable trend with severe environmental consequences. Furthermore, such a design forces only a few conglomerates to be the gate keepers of model training. In this work, we propose cDLRM which democratizes recommendation model training by allowing a user to train on a single GPU regardless of the size of embedding tables by storing all embedding tables in CPU memory. A CPU based pre-processor analyzes training batches to prefetch embedding table slices accessed by those batches and caches them in GPU memory just-in-time. An associated caching protocol on the GPU enables efficiently updating the cached embedding table parameters. cDLRM decouples the embedding table size demands from the number of GPUs needed for compute. We first demonstrate that with cDLRM it is possible to train a large recommendation model using a single GPU regardless of model size. We then demonstrate that with its unique caching strategy, cDLRM enables pure data parallel training. We use two publicly available datasets to show that a cDLRM achieves identical model accuracy compared to a baseline trained completely on GPUs, while benefiting from large reduction in GPU demand.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {263–272},
numpages = {10},
keywords = {distributed data parallel training, caching, efficient training, prefetching, Recommendation models},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@INPROCEEDINGS{rashidi2020scalable,
  author={Rashidi, Saeed and Shurpali, Pallavi and Sridharan, Srinivas and Hassani, Naader and Mudigere, Dheevatsa and Nair, Krishnakumar and Smelyanski, Misha and Krishna, Tushar},
  booktitle={2020 IEEE Symposium on High-Performance Interconnects (HOTI)}, 
  title={Scalable Distributed Training of Recommendation Models: An ASTRA-SIM + NS3 case-study with TCP/IP transport}, 
  year={2020},
  volume={},
  number={},
  pages={33-42},
  doi={10.1109/HOTI51249.2020.00020}}

@inproceedings{hildebrand2023efficient,
author = {Hildebrand, Mark and Lowe-Power, Jason and Akella, Venkatesh},
title = {Efficient Large Scale DLRM Implementation on Heterogeneous Memory Systems},
year = {2023},
isbn = {978-3-031-32040-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-32041-5_3},
doi = {10.1007/978-3-031-32041-5_3},
abstract = {We propose a new data structure called CachedEmbeddings for training large scale deep learning recommendation models (DLRM) efficiently on heterogeneous (DRAM + non-volatile) memory platforms. CachedEmbeddings implements an implicit software-managed cache and data movement optimization that is integrated with the Julia programming framework to optimize the implementation of large scale DLRM implementations with multiple sparse embedded tables operations. In particular we show an implementation that is 1.4X to 2X better than the best known Intel CPU based implementations on state-of-the-art DLRM benchmarks on a real heterogeneous memory platform from Intel, and 1.32X to 1.45X improvement over Intel’s 2LM implementation that treats the DRAM as a hardware managed cache.},
booktitle = {High Performance Computing: 38th International Conference, ISC High Performance 2023, Hamburg, Germany, May 21–25, 2023, Proceedings},
pages = {42–61},
numpages = {20},
location = {Hamburg, Germany}
}

@article{wang2023emsi,
author = {Wang, Yitu and Li, Shiyu and Zheng, Qilin and Chang, Andrew and Li, Hai and Chen, Yiran},
title = {
EMS-i: An Efficient Memory System Design with Specialized Caching Mechanism for Recommendation Inference},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609384},
doi = {10.1145/3609384},
abstract = {Recommendation systems have been widely embedded into many Internet services. For example, Meta’s deep learning recommendation model (DLRM) shows high prefictive accuracy of click-through rate in processing large-scale embedding tables. The SparseLengthSum (SLS) kernel of the DLRM dominates the inference time of the DLRM due to intensive irregular memory accesses to the embedding vectors. Some prior works directly adopt near data processing (NDP) solutions to obtain higher memory bandwidth to accelerate SLS. However, their inferior memory hierarchy induces low performance-cost ratio and fails to fully exploit the data locality. Although some software-managed cache policies were proposed to improve the cache hit rate, the incurred cache miss penalty is unacceptable considering the high overheads of executing the corresponding programs and the communication between the host and the accelerator. To address the issues aforementioned, we propose EMS-i, an efficient memory system design that integrates Solide State Drive (SSD) into the memory hierarchy using Compute Express Link (CXL) for recommendation system inference. We specialize the caching mechanism according to the characteristics of various DLRM workloads and propose a novel prefetching mechanism to further improve the performance. In addition, we delicately design the inference kernel and develop a customized mapping scheme for SLS operation, considering the multi-level parallelism in SLS and the data locality within a batch of queries. Compared to the state-of-the-art NDP solutions, EMS-i achieves up to 10.9\texttimes{} speedup over RecSSD and the performance comparable to RecNMP with 72\% energy savings. EMS-i also saves up to 8.7\texttimes{} and 6.6 \texttimes{} memory cost w.r.t. RecSSD and RecNMP, respectively.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {100},
numpages = {22},
keywords = {Recommendation system, compute express link}
}

@inproceedings{Nagrecha2023InTune,
author = {Nagrecha, Kabir and Liu, Lingyi and Delgado, Pablo and Padmanabhan, Prasanna},
title = {InTune: Reinforcement Learning-Based Data Pipeline Optimization for Deep Recommendation Models},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608778},
doi = {10.1145/3604915.3608778},
abstract = {Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- \& time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning (DL) training jobs are dominated by model execution times, the most important factor in DLRM training performance is often online data ingestion.In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into the specific bottlenecks and challenges of the DLRM training pipeline at scale. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to both observe the performance impacts of online ingestion and to identify shortfalls in existing data pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune. InTune&nbsp;employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data-loading and improve throughput. Our experiments show that InTune&nbsp;can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune&nbsp;achieves significantly higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune&nbsp;to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus current state-of-the-art data pipeline optimizers while also improving both CPU \& GPU utilization.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {430–442},
numpages = {13},
keywords = {data processing, recommendation systems, resource allocation, deep learning, parallel computing},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{fang2022frequency,
  author       = {Jiarui Fang and
                  Geng Zhang and
                  Jiatong Han and
                  Shenggui Li and
                  Zhengda Bian and
                  Yongbin Li and
                  Jin Liu and
                  Yang You},
  title        = {A Frequency-aware Software Cache for Large Recommendation System Embeddings},
  journal      = {CoRR},
  volume       = {abs/2208.05321},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2208.05321},
  doi          = {10.48550/ARXIV.2208.05321},
  eprinttype    = {arXiv},
  eprint       = {2208.05321},
  timestamp    = {Sat, 17 Dec 2022 01:15:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2208-05321.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Agarwal2023Bagpipe,
author = {Agarwal, Saurabh and Yan, Chengpo and Zhang, Ziyi and Venkataraman, Shivaram},
title = {Bagpipe: Accelerating Deep Recommendation Model Training},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613142},
doi = {10.1145/3600006.3613142},
abstract = {Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\% of embeddings representing more than 92\% of total accesses. Further, we also observe that during offline training we can lookahead at future batches to determine which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to overlap remote embedding accesses with the computation. We design an Oracle Cacher, a new component that uses a lookahead algorithm to generate optimal cache update decisions while providing strong consistency guarantees against staleness. We also design a logically replicated, physically partitioned cache and show that our design can reduce synchronization overheads in a distributed setting. Finally, we propose a disaggregated system architecture and show that our design can enable low-overhead fault tolerance. Our experiments using three datasets and four models show that Bagpipe provides a speed up of up to 5.6x compared to state of the art baselines, while providing the same convergence and reproducibility guarantees as synchronous training.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {348–363},
numpages = {16},
keywords = {distributed training, recommendation models},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@INPROCEEDINGS {Jiang2023MixRec,
author = {J. Jiang and R. Tian and J. Du and D. Huang and Y. Lu},
booktitle = {2023 IEEE 41st International Conference on Computer Design (ICCD)},
title = {MixRec: Orchestrating Concurrent Recommendation Model Training on CPU-GPU platform},
year = {2023},
volume = {},
issn = {},
pages = {366-374},
abstract = {The development of deep learning recommendation models (DLRM) and recommendation systems has significantly improved the precision of information matching. Due to distinct computation, data access, and memory usage characteristics of recommendation models, they may suffer from low resource utilization on prevalent heterogeneous CPU-GPU hardware platforms. Existing concurrent training solutions cannot be directly applied to DLRM due to various factors, such as insufficient fine-grained memory management and the lack of collaborative CPU-GPU scheduling. In this paper, we introduce MixRec, a scheduling framework that addresses these challenges by pro-viding an efficient job management and scheduling mechanism for DLRM training jobs on heterogeneous CPU-GPU platforms. To facilitate training co-location, we first estimate the peak memory consumption of each job. Additionally, we track and collect resource utilization for DLRM training jobs. Based on the information of resource usage, a batched job dispatcher with dynamic resource-complementary scheduling policy is proposed to co-locate DLRM training jobs on CPU-GPU platform. Experimental results demonstrate that our implementation achieved up to 4.42× higher throughput and 3.97× higher resource utilization for training jobs involving various recommendation models.},
keywords = {training;processor scheduling;computational modeling;memory management;dynamic scheduling;throughput;hardware},
doi = {10.1109/ICCD58817.2023.00062},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCD58817.2023.00062},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov}
}

@article{Punniyamurthy2023GPUinitiated,
  author       = {Kishore Punniyamurthy and
                  Bradford M. Beckmann and
                  Khaled Hamidouche},
  title        = {GPU-initiated Fine-grained Overlap of Collective Communication with
                  Computation},
  journal      = {CoRR},
  volume       = {abs/2305.06942},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.06942},
  doi          = {10.48550/ARXIV.2305.06942},
  eprinttype    = {arXiv},
  eprint       = {2305.06942},
  timestamp    = {Wed, 17 May 2023 15:47:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-06942.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{Lin2022Building,
  author={Lin, Zhongyi and Feng, Louis and Ardestani, Ehsan K. and Lee, Jaewon and Lundell, John and Kim, Changkyu and Kejariwal, Arun and Owens, John D.},
  booktitle={2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Building a Performance Model for Deep Learning Recommendation Model Training on GPUs}, 
  year={2022},
  volume={},
  number={},
  pages={227-229},
  doi={10.1109/ISPASS55109.2022.00030}}

@inproceedings{Xiao2023Gmeta,
author = {Xiao, Youshao and Zhao, Shangchun and Zhou, Zhenglei and Huan, Zhaoxin and Ju, Lin and Zhang, Xiaolu and Wang, Lin and Zhou, Jun},
title = {G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615208},
doi = {10.1145/3583780.3615208},
abstract = {Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the G PU cluster, namely G -Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental results show that G-Meta achieves notable training speed without loss of statistical performance. Since early 2022, G-Meta has been deployed in Alipay's core advertising and recommender system, shrinking the continuous delivery of models by four times. It also obtains 6.48\% improvement in Conversion Rate (CVR) and 1.06\% increase in CPM (Cost Per Mille) in Alipay's homepage display advertising, with the benefit of larger training samples and tasks.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4365–4369},
numpages = {5},
keywords = {deep meta learning, recommender system, distributed training},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{Wei2022GPUSpecialized,
author = {Wei, Yingcan and Langer, Matthias and Yu, Fan and Lee, Minseok and Liu, Jie and Shi, Ji and Wang, Zehuan},
title = {A GPU-Specialized Inference Parameter Server for Large-Scale Deep Recommendation Models},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3546765},
doi = {10.1145/3523227.3546765},
abstract = {Recommendation systems are of crucial importance for a variety of modern apps and web services, such as news feeds, social networks, e-commerce, search, etc. To achieve peak prediction accuracy, modern recommendation models combine deep learning with terabyte-scale embedding tables to obtain a fine-grained representation of the underlying data. Traditional inference serving architectures require deploying the whole model to standalone servers, which is infeasible at such massive scale. In this paper, we provide insights into the intriguing and challenging inference domain of online recommendation systems. We propose the HugeCTR Hierarchical Parameter Server (HPS), an industry-leading distributed recommendation inference framework, that combines a high-performance GPU embedding cache with an hierarchical storage architecture, to realize low-latency retrieval of embeddings for online model inference tasks. Among other things, HPS features (1) a redundant hierarchical storage system, (2) a novel high-bandwidth cache to accelerate parallel embedding lookup on NVIDIA GPUs, (3) online training support and (4) light-weight APIs for easy integration into existing large-scale recommendation workflows. To demonstrate its capabilities, we conduct extensive studies using both synthetically engineered and public datasets. We show that our HPS can dramatically reduce end-to-end inference latency, achieving 5~62x speedup (depending on the batch size) over CPU baseline implementations for popular recommendation models. Through multi-GPU concurrent deployment, the HPS can also greatly increase the inference QPS.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {408–419},
numpages = {12},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{Desai2022tradeoffs,
 author = {Desai, Aditya and Shrivastava, Anshumali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {33961--33972},
 publisher = {Curran Associates, Inc.},
 title = {The trade-offs of model size in large recommendation models : 100GB to 10MB Criteo-tb DLRM model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/dbae915128892556134f1c5375855590-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{Adnan2022Heterogeneous,
  author       = {Muhammad Adnan and
                  Yassaman Ebrahimzadeh Maboud and
                  Divya Mahajan and
                  Prashant J. Nair},
  title        = {Heterogeneous Acceleration Pipeline for Recommendation System Training},
  journal      = {CoRR},
  volume       = {abs/2204.05436},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2204.05436},
  doi          = {10.48550/ARXIV.2204.05436},
  eprinttype    = {arXiv},
  eprint       = {2204.05436},
  timestamp    = {Mon, 26 Jun 2023 07:38:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2204-05436.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ye2023GRACE,
author = {Ye, Haojie and Vedula, Sanketh and Chen, Yuhan and Yang, Yichen and Bronstein, Alex and Dreslinski, Ronald and Mudge, Trevor and Talati, Nishil},
title = {GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582029},
doi = {10.1145/3582016.3582029},
abstract = {The high memory bandwidth demand of sparse embedding layers continues to be a critical challenge in scaling the performance of recommendation models. While prior works have exploited heterogeneous memory system designs and partial embedding sum memoization techniques, they offer limited benefits. This is because prior designs either target a very small subset of embeddings to simplify their analysis or incur a high processing cost to account for all embeddings, which does not scale with the large sizes of modern embedding tables. This paper proposes GRACE-a lightweight and scalable graph-based algorithm-system co-design framework to significantly improve the embedding layer performance of recommendation models. GRACE proposes a novel Item Co-occurrence Graph (ICG) that scalably records item co-occurrences. GRACE then presents a new system-aware ICG clustering algorithm to find frequently accessed item combinations of arbitrary lengths to compute and memoize their partial sums. High-frequency partial sums are stored in a software-managed cache space to reduce memory traffic and improve the throughput of computing sparse features. We further present a cache data layout and low-cost address computation logic to efficiently lookup item embeddings and their partial sums. Our evaluation shows that GRACE significantly outperforms the state-of-the-art techniques SPACE and MERCI by 1.5x and 1.4x, respectively.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {282–301},
numpages = {20},
keywords = {Embedding Reduction, DLRM, Algorithm-System Co-Design},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{Huang2021Hierarchical,
author = {Huang, Yuzhen and Wei, Xiaohan and Wang, Xing and Yang, Jiyan and Su, Bor-Yiing and Bharuka, Shivam and Choudhary, Dhruv and Jiang, Zewei and Zheng, Hai and Langman, Jack},
title = {Hierarchical Training: Scaling Deep Recommendation Models on Large CPU Clusters},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467084},
doi = {10.1145/3447548.3467084},
abstract = {Neural network based recommendation models are widely used to power many internet-scale applications including product recommendation and feed ranking. As the models become more complex and more training data is required during training, improving the training scalability of these recommendation models becomes an urgent need. However, improving the scalability without sacrificing the model quality is challenging. In this paper, we conduct an in-depth analysis of the scalability bottleneck in existing training architecture on large scale CPU clusters. Based on these observations, we propose a new training architecture called Hierarchical Training, which exploits both data parallelism and model parallelism for the neural network part of the model within a group. We implement hierarchical training with a two-layer design: a tagging system that decides the operator placement and a net transformation system that materializes the training plans, and integrate hierarchical training into existing training stack. We propose several optimizations to improve the scalability of hierarchical training including model architecture optimization, communication compression, and various system-level improvements. Extensive experiments at massive scale demonstrate that hierarchical training can speed up distributed recommendation model training by 1.9x without model quality drop.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages = {3050–3058},
numpages = {9},
keywords = {system for machine learning, optimization, distributed training},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{GenoaHotChips,
  author={Troester, Kai and Bhargava, Ravi},
  booktitle={2023 IEEE Hot Chips 35 Symposium (HCS)}, 
  title={AMD Next Generation “Zen 4” Core and 4th Gen AMD EPYC™ 9004 Server CPU}, 
  year={2023},
  volume={},
  number={},
  pages={1-25},
  doi={10.1109/HCS59251.2023.10254726}}


@misc{GenoaLaunch,
  author = {AMD},
  year = {2023},
  title = {{AMD Zen Deep Neural Network}},
  howpublished = {"\url{https://www.amd.com/en/developer/zendnn.html}"}
}

@misc{GenoaSpec,
  author = {AMD},
  year = {2022},
  title = {{AMD Epyc 9654 CPU, code-named Genoa}},
  howpublished = {"\url{https://www.amd.com/en/product/12191}"}
}

@misc{GenoaXSpec,
  author = {AMD},
  year = {2022},
  title = {{AMD Epyc 9684X CPU, code-named Genoa X}},
  howpublished = {"https://www.amd.com/en/products/cpu/amd-epyc-9684x"}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
    CTLuse_article_number = "yes",
    CTLuse_paper = "yes",
    CTLuse_forced_etal = "no",
    CTLmax_names_forced_etal = "2",
    CTLnames_show_etal = "2",
    CTLalt_stretch_factor = "4",
    CTLdash_repeated_names = "no",
}
@ARTICLE{smartDust,
  author={B. {Warneke} and M. {Last} and B. {Liebowitz} and K. S. J. {Pister}},
  journal={Computer}, 
  title={Smart Dust: communicating with a cubic-millimeter computer}, 
  year={2001},
  volume={34},
  number={1},
  pages={44-51},}

@misc{AppleECG,
    key         = {Apple Watch - ECG},
    title       = {Taking an ECG with the ECG app on Apple Watch Series 4 or later},
    note        = {https://support.apple.com/en-us/HT208955},
    year        = 2020
}

@misc{AppleFall,
    key         = {Apple Watch - Fall Detection},
    title       = {Use fall detection with Apple Watch},
    note        = {https://support.apple.com/en-us/HT208944},
    year        = 2020
}

@InProceedings{netadapt,
	author = {Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
	title = {NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications},
	booktitle = {ECCV},
	month = {September},
	year = {2018}
}

@misc{Google-Assistant-Watch,
    key         = {Google Assistant for Wearables},
    title       = {Google Assistant for Wearables},
    note        = {https://assistant.google.com/platforms/wearables/},
    year        = 2020
}

@INPROCEEDINGS{ResiRCA,
  author={K. {Qiu} and N. {Jao} and M. {Zhao} and C. S. {Mishra} and G. {Gudukbay} and S. {Jose} and J. {Sampson} and M. T. {Kandemir} and V. {Narayanan}},
  booktitle={2020 HPCA}, 
  title={ResiRCA: A Resilient Energy Harvesting ReRAM Crossbar-Based Accelerator for Intelligent Embedded Processors}, 
  year={2020},
  volume={},
  number={},
  pages={315-327},}
  
  @INPROCEEDINGS{spendthrift,
  author={K. {Ma} and X. {Li} and S. R. {Srinivasa} and Y. {Liu} and J. {Sampson} and Y. {Xie} and V. {Narayanan}},
  booktitle={2017 (ASP-DAC)}, 
  title={Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors}, 
  year={2017},
  volume={},
  number={},
  pages={678-683},}
  
@inproceedings{IntBeyondEdge,
author = {Gobieski, Graham and Lucia, Brandon and Beckmann, Nathan},
title = {Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems},
publisher = {ACM},
booktitle = {ASPLOS},
year={2019}
}

@inproceedings {chinchilla,
author = {Kiwan Maeng and Brandon Lucia},
title = {Adaptive Dynamic Checkpointing for Safe Efficient Intermittent Computing},
booktitle = {OSDI},
year = {2018},
publisher = {{USENIX} Association},
}

@ARTICLE{NVPMa,
  author={K. {Ma} and X. {Li} and S. {Li} and Y. {Liu} and J. J. {Sampson} and Y. {Xie} and V. {Narayanan}},
  journal={IEEE Micro}, 
  title={Nonvolatile Processor Architecture Exploration for Energy-Harvesting Applications}, 
  year={2015},
  volume={35},
  number={5},
  pages={32-40},}
  
  @INPROCEEDINGS{incidental,
  author={K. {Ma} and X. {Li} and J. {Li} and Y. {Liu} and Y. {Xie} and J. {Sampson} and M. T. {Kandemir} and V. {Narayanan}},
  booktitle={MICRO}, 
  year={2017},
  title={Incidental Computing on IoT Nonvolatile Processors}, 
}
  
 
@inproceedings{IntermittentChallange,
  author    = {Brandon Lucia and
               Vignesh Balaji and
               Alexei Colin and
               Kiwan Maeng and
               Emily Ruppel},
  editor    = {Benjamin S. Lerner and
               Rastislav Bod{\'{\i}}k and
               Shriram Krishnamurthi},
  title     = {Intermittent Computing: Challenges and Opportunities},
  booktitle = {2nd Summit on Advances in Programming Languages, {SNAPL} 2017, May
               7-10, 2017, Asilomar, CA, {USA}},
  series    = {LIPIcs},
  volume    = {71},
  pages     = {8:1--8:14},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  year      = {2017},
  url       = {https://doi.org/10.4230/LIPIcs.SNAPL.2017.8},
  doi       = {10.4230/LIPIcs.SNAPL.2017.8},
  timestamp = {Tue, 11 Feb 2020 15:52:14 +0100},
  biburl    = {https://dblp.org/rec/conf/snapl/LuciaBCMR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{NVPMicro,
  author    = {Kaisheng Ma and
               Xueqing Li and
               Karthik Swaminathan and
               Yang Zheng and
               Shuangchen Li and
               Yongpan Liu and
               Yuan Xie and
               John (Jack) Morgan Sampson and
               Vijaykrishnan Narayanan},
  title     = {Nonvolatile Processor Architectures: Efficient, Reliable Progress
               with Unstable Power},
  journal   = {{IEEE} Micro},
  volume    = {36},
  number    = {3},
  pages     = {72--83},
  year      = {2016},
  url       = {https://doi.org/10.1109/MM.2016.35},
  doi       = {10.1109/MM.2016.35},
  timestamp = {Sat, 20 May 2017 00:26:03 +0200},
  biburl    = {https://dblp.org/rec/journals/micro/MaLSZLLXSN16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LuciaCheckpoint,
  author    = {Kiwan Maeng and
               Brandon Lucia},
  editor    = {Andrea C. Arpaci{-}Dusseau and
               Geoff Voelker},
  title     = {Adaptive Dynamic Checkpointing for Safe Efficient Intermittent Computing},
  booktitle = {OSDI},
  publisher = {{USENIX} Association},
  year      = {2018},
}

@inproceedings{intelligenceBeyondEdge,
  author    = {Graham Gobieski and
               Brandon Lucia and
               Nathan Beckmann},
  editor    = {Iris Bahar and
               Maurice Herlihy and
               Emmett Witchel and
               Alvin R. Lebeck},
  title     = {Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems},
  booktitle = {ASPLOS},
  publisher = {{ACM}},
  year      = {2019},
}

@misc{iWatchBattery,
    key         = {Apple Watch},
    title       = {Apple Watch Series 5 teardown reveals bigger battery},
    note        = {https://appleinsider.com/articles/19/09/25/apple-watch-series-5-teardown-reveals-bigger-battery},
    year        = 2019
}

@INPROCEEDINGS{LeNet,
    author = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}

@article{HARHompel,
  author    = {Fernando Moya Rueda and
               Ren{\'{e}} Grzeszick and
               Gernot A. Fink and
               Sascha Feldhorst and
               Michael ten Hompel},
  title     = {Convolutional Neural Networks for Human Activity Recognition Using
               Body-Worn Sensors},
  journal   = {Informatics},
  year      = {2018},
}

@INPROCEEDINGS{HARHaChoi,
  author={S. {Ha} and S. {Choi}},
  booktitle={IJCNN},
  year={2016},
  title={Convolutional neural networks for human activity recognition using multiple accelerometer and gyroscope sensors}, 
}
  
@article{PolikarEnsemble,
  author    = {Robi Polikar},
  title     = {Ensemble learning},
  journal   = {Scholarpedia},
  volume    = {4},
  number    = {1},
  pages     = {2776},
  year      = {2009},
  url       = {https://doi.org/10.4249/scholarpedia.2776},
  doi       = {10.4249/scholarpedia.2776},
  timestamp = {Thu, 23 May 2019 15:09:49 +0200},
  biburl    = {https://dblp.org/rec/journals/scholarpedia/Polikar09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mHealthDroid,
  author    = {Oresti Ba{\~{n}}os and
               Rafael Garc{\'{\i}}a and
               Juan Antonio Holgado Terriza and
               Miguel Damas and
               H{\'{e}}ctor Pomares and
               Ignacio Rojas Ruiz and
               Alejandro Saez and
               Claudia Villalonga},

  title     = {mHealthDroid: {A} Novel Framework for Agile Development of Mobile
               Health Applications},
  booktitle = {IWAAL},
  year={2014},
  publisher = {Springer},
}

@article{mHealth,
author = {Banos, Oresti and Villalonga, Claudia and García, Rafael and Saez, Alejandro and Damas, Miguel and Holgado-Terriza, Juan and Lee, Sungyong and Pomares, Hector and Rojas, Ignacio},
year = {2015},
title = {Design, implementation and validation of a novel open framework for agile development of mobile health applications},
journal = {BioMedical Engineering OnLine},
}

@inproceedings{EAPruning,
  author    = {Tien{-}Ju Yang and
               Yu{-}Hsin Chen and
               Vivienne Sze},
  title     = {Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware
               Pruning},
  booktitle = {CVPR},
  year={2017},
  publisher = {{IEEE}},
}

@misc{GoogleFit,
    key         = {Google Fit},
    title       = {Google APIs for Android},
    url         = "https://developers.google.com/android/reference/com/google/android/gms/location/DetectedActivity",
    year        = 2020
}

@misc{AppleHealth,
    key         = {Apple Developer: CMMotionActivity},
    title       = {Apple Developer: CMMotionActivity},
    url        = "https://developer.apple.com/documentation/coremotion/cmmotionactivity\#topics",
    year        = 2020
}

@online{keras,
  title={Keras},
  author={Chollet, Francois and others},
  year={2015},
  publisher={GitHub},
  url={https://github.com/fchollet/keras},
}

@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{PAMAP21,
  author    = {Attila Reiss and
               Didier Stricker},
  title     = {Introducing a New Benchmarked Dataset for Activity Monitoring},
  booktitle = {ISWC},
  publisher = {{IEEE}},
  year      = {2012},
}

@inproceedings{PAMAP22,
  author    = {Attila Reiss and
               Didier Stricker},
  editor    = {Fillia Makedon},
  title     = {Creating and benchmarking a new dataset for physical activity monitoring},
  booktitle = {PETRA},
  year={2012},
  publisher = {{ACM}},

}

@INPROCEEDINGS{NIP,
  author={F. {Su} and W. {Chen} and L. {Xia} and C. {Lo} and T. {Tang} and Z. {Wang} and K. {Hsu} and M. {Cheng} and J. {Li} and Y. {Xie} and Y. {Wang} and M. {Chang} and H. {Yang} and Y. {Liu}},
  booktitle={2017 Symposium on VLSI Technology}, 
  title={A 462GOPs/J RRAM-based nonvolatile intelligent processor for energy harvesting IoE system featuring nonvolatile logics and processing-in-memory}, 
  year={2017},
  volume={},
  number={},
  pages={T260-T261},}
  
  @article{Jordao:2018,
author    = {Artur Jordao,
Antonio Carlos Nazare,
Jessica Sena and
William Robson Schwartz},
title     = {Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art},
journal   = {arXiv},
year      = {2018},
eprint    = {1806.05226},
}

@inproceedings{USCHAD,
  author    = {Mi Zhang and
               Alexander A. Sawchuk},
  editor    = {Anind K. Dey and
               Hao{-}Hua Chu and
               Gillian R. Hayes},
  title     = {{USC-HAD:} a daily activity dataset for ubiquitous activity recognition
               using wearable sensors},
  booktitle = {The 2012 {ACM} Conference on Ubiquitous Computing, Ubicomp '12, Pittsburgh,
               PA, USA, September 5-8, 2012},
  pages     = {1036--1043},
  publisher = {{ACM}},
  year      = {2012},
  url       = {https://doi.org/10.1145/2370216.2370438},
  doi       = {10.1145/2370216.2370438},
  timestamp = {Tue, 06 Nov 2018 16:58:39 +0100},
  biburl    = {https://dblp.org/rec/conf/huc/ZhangS12a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{smartDust,
  author={B. {Warneke} and M. {Last} and B. {Liebowitz} and K. S. J. {Pister}},
  journal={Computer}, 
  title={Smart Dust: communicating with a cubic-millimeter computer}, 
  year={2001},
  volume={34},
  number={1},
  pages={44-51},}

@misc{AppleECG,
    key         = {Apple Watch - ECG},
    title       = {Taking an ECG with the ECG app on Apple Watch Series 4 or later},
    note        = {https://support.apple.com/en-us/HT208955},
    year        = 2020
}

@misc{AppleFall,
    key         = {Apple Watch - Fall Detection},
    title       = {Use fall detection with Apple Watch},
    note        = {https://support.apple.com/en-us/HT208944},
    year        = 2020
}

@InProceedings{netadapt,
	author = {Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
	title = {NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications},
	booktitle = {ECCV},
	month = {September},
	year = {2018}
}

@misc{Google-Assistant-Watch,
    key         = {Google Assistant for Wearables},
    title       = {Google Assistant for Wearables},
    note        = {https://assistant.google.com/platforms/wearables/},
    year        = 2020
}

@INPROCEEDINGS{ResiRCA,
  author={K. {Qiu} and N. {Jao} and M. {Zhao} and C. S. {Mishra} and G. {Gudukbay} and S. {Jose} and J. {Sampson} and M. T. {Kandemir} and V. {Narayanan}},
  booktitle={2020 HPCA}, 
  title={ResiRCA: A Resilient Energy Harvesting ReRAM Crossbar-Based Accelerator for Intelligent Embedded Processors}, 
  year={2020},
  volume={},
  number={},
  pages={315-327},}
  
  @INPROCEEDINGS{spendthrift,
  author={K. {Ma} and X. {Li} and S. R. {Srinivasa} and Y. {Liu} and J. {Sampson} and Y. {Xie} and V. {Narayanan}},
  booktitle={2017 (ASP-DAC)}, 
  title={Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors}, 
  year={2017},
  volume={},
  number={},
  pages={678-683},}
  
@inproceedings{IntBeyondEdge,
author = {Gobieski, Graham and Lucia, Brandon and Beckmann, Nathan},
title = {Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems},
publisher = {ACM},
booktitle = {ASPLOS},
year={2019}
}

@inproceedings {chinchilla,
author = {Kiwan Maeng and Brandon Lucia},
title = {Adaptive Dynamic Checkpointing for Safe Efficient Intermittent Computing},
booktitle = {OSDI},
year = {2018},
publisher = {{USENIX} Association},
}

@ARTICLE{NVPMa,
  author={K. {Ma} and X. {Li} and S. {Li} and Y. {Liu} and J. J. {Sampson} and Y. {Xie} and V. {Narayanan}},
  journal={IEEE Micro}, 
  title={Nonvolatile Processor Architecture Exploration for Energy-Harvesting Applications}, 
  year={2015},
  volume={35},
  number={5},
  pages={32-40},}
  
  @INPROCEEDINGS{incidental,
  author={K. {Ma} and X. {Li} and J. {Li} and Y. {Liu} and Y. {Xie} and J. {Sampson} and M. T. {Kandemir} and V. {Narayanan}},
  booktitle={MICRO}, 
  year={2017},
  title={Incidental Computing on IoT Nonvolatile Processors}, 
}
  
 
@inproceedings{IntermittentChallange,
  author    = {Brandon Lucia and
               Vignesh Balaji and
               Alexei Colin and
               Kiwan Maeng and
               Emily Ruppel},
  editor    = {Benjamin S. Lerner and
               Rastislav Bod{\'{\i}}k and
               Shriram Krishnamurthi},
  title     = {Intermittent Computing: Challenges and Opportunities},
  booktitle = {2nd Summit on Advances in Programming Languages, {SNAPL} 2017, May
               7-10, 2017, Asilomar, CA, {USA}},
  series    = {LIPIcs},
  volume    = {71},
  pages     = {8:1--8:14},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  year      = {2017},
  url       = {https://doi.org/10.4230/LIPIcs.SNAPL.2017.8},
  doi       = {10.4230/LIPIcs.SNAPL.2017.8},
  timestamp = {Tue, 11 Feb 2020 15:52:14 +0100},
  biburl    = {https://dblp.org/rec/conf/snapl/LuciaBCMR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{NVPMicro,
  author    = {Kaisheng Ma and
               Xueqing Li and
               Karthik Swaminathan and
               Yang Zheng and
               Shuangchen Li and
               Yongpan Liu and
               Yuan Xie and
               John (Jack) Morgan Sampson and
               Vijaykrishnan Narayanan},
  title     = {Nonvolatile Processor Architectures: Efficient, Reliable Progress
               with Unstable Power},
  journal   = {{IEEE} Micro},
  volume    = {36},
  number    = {3},
  pages     = {72--83},
  year      = {2016},
  url       = {https://doi.org/10.1109/MM.2016.35},
  doi       = {10.1109/MM.2016.35},
  timestamp = {Sat, 20 May 2017 00:26:03 +0200},
  biburl    = {https://dblp.org/rec/journals/micro/MaLSZLLXSN16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LuciaCheckpoint,
  author    = {Kiwan Maeng and
               Brandon Lucia},
  editor    = {Andrea C. Arpaci{-}Dusseau and
               Geoff Voelker},
  title     = {Adaptive Dynamic Checkpointing for Safe Efficient Intermittent Computing},
  booktitle = {OSDI},
  publisher = {{USENIX} Association},
  year      = {2018},
}

@inproceedings{intelligenceBeyondEdge,
  author    = {Graham Gobieski and
               Brandon Lucia and
               Nathan Beckmann},
  editor    = {Iris Bahar and
               Maurice Herlihy and
               Emmett Witchel and
               Alvin R. Lebeck},
  title     = {Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems},
  booktitle = {ASPLOS},
  publisher = {{ACM}},
  year      = {2019},
}

@misc{iWatchBattery,
    key         = {Apple Watch},
    title       = {Apple Watch Series 5 teardown reveals bigger battery},
    note        = {https://appleinsider.com/articles/19/09/25/apple-watch-series-5-teardown-reveals-bigger-battery},
    year        = 2019
}

@INPROCEEDINGS{LeNet,
    author = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}

@article{HARHompel,
  author    = {Fernando Moya Rueda and
               Ren{\'{e}} Grzeszick and
               Gernot A. Fink and
               Sascha Feldhorst and
               Michael ten Hompel},
  title     = {Convolutional Neural Networks for Human Activity Recognition Using
               Body-Worn Sensors},
  journal   = {Informatics},
  year      = {2018},
}

@INPROCEEDINGS{HARHaChoi,
  author={S. {Ha} and S. {Choi}},
  booktitle={IJCNN},
  year={2016},
  title={Convolutional neural networks for human activity recognition using multiple accelerometer and gyroscope sensors}, 
}
  
@article{PolikarEnsemble,
  author    = {Robi Polikar},
  title     = {Ensemble learning},
  journal   = {Scholarpedia},
  volume    = {4},
  number    = {1},
  pages     = {2776},
  year      = {2009},
  url       = {https://doi.org/10.4249/scholarpedia.2776},
  doi       = {10.4249/scholarpedia.2776},
  timestamp = {Thu, 23 May 2019 15:09:49 +0200},
  biburl    = {https://dblp.org/rec/journals/scholarpedia/Polikar09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mHealthDroid,
  author    = {Oresti Ba{\~{n}}os and
               Rafael Garc{\'{\i}}a and
               Juan Antonio Holgado Terriza and
               Miguel Damas and
               H{\'{e}}ctor Pomares and
               Ignacio Rojas Ruiz and
               Alejandro Saez and
               Claudia Villalonga},

  title     = {mHealthDroid: {A} Novel Framework for Agile Development of Mobile
               Health Applications},
  booktitle = {IWAAL},
  year={2014},
  publisher = {Springer},
}

@article{mHealth,
author = {Banos, Oresti and Villalonga, Claudia and García, Rafael and Saez, Alejandro and Damas, Miguel and Holgado-Terriza, Juan and Lee, Sungyong and Pomares, Hector and Rojas, Ignacio},
year = {2015},
title = {Design, implementation and validation of a novel open framework for agile development of mobile health applications},
journal = {BioMedical Engineering OnLine},
}

@inproceedings{EAPruning,
  author    = {Tien{-}Ju Yang and
               Yu{-}Hsin Chen and
               Vivienne Sze},
  title     = {Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware
               Pruning},
  booktitle = {CVPR},
  year={2017},
  publisher = {{IEEE}},
}

@misc{GoogleFit,
    key         = {Google Fit},
    title       = {Google APIs for Android},
    url         = "https://developers.google.com/android/reference/com/google/android/gms/location/DetectedActivity",
    year        = 2020
}

@misc{AppleHealth,
    key         = {Apple Developer: CMMotionActivity},
    title       = {Apple Developer: CMMotionActivity},
    url        = "https://developer.apple.com/documentation/coremotion/cmmotionactivity\#topics",
    year        = 2020
}

@misc{keras,
  title={Keras},
  author={Chollet, Francois and others},
  year={2015},
  publisher={GitHub},
  url={https://github.com/fchollet/keras},
}

@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{PAMAP21,
  author    = {Attila Reiss and
               Didier Stricker},
  title     = {Introducing a New Benchmarked Dataset for Activity Monitoring},
  booktitle = {ISWC},
  publisher = {{IEEE}},
  year      = {2012},
}

@inproceedings{PAMAP22,
  author    = {Attila Reiss and
               Didier Stricker},
  editor    = {Fillia Makedon},
  title     = {Creating and benchmarking a new dataset for physical activity monitoring},
  booktitle = {PETRA},
  year={2012},
  publisher = {{ACM}},

}

@INPROCEEDINGS{NIP,
  author={F. {Su} and W. {Chen} and L. {Xia} and C. {Lo} and T. {Tang} and Z. {Wang} and K. {Hsu} and M. {Cheng} and J. {Li} and Y. {Xie} and Y. {Wang} and M. {Chang} and H. {Yang} and Y. {Liu}},
  booktitle={2017 Symposium on VLSI Technology}, 
  title={A 462GOPs/J RRAM-based nonvolatile intelligent processor for energy harvesting IoE system featuring nonvolatile logics and processing-in-memory}, 
  year={2017},
  volume={},
  number={},
  pages={T260-T261},}
  
  @article{Jordao:2018,
author    = {Artur Jordao,
Antonio Carlos Nazare,
Jessica Sena and
William Robson Schwartz},
title     = {Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art},
journal   = {arXiv},
year      = {2018},
eprint    = {1806.05226},
}

@inproceedings{USCHAD,
  author    = {Mi Zhang and
               Alexander A. Sawchuk},
  editor    = {Anind K. Dey and
               Hao{-}Hua Chu and
               Gillian R. Hayes},
  title     = {{USC-HAD:} a daily activity dataset for ubiquitous activity recognition
               using wearable sensors},
  booktitle = {The 2012 {ACM} Conference on Ubiquitous Computing, Ubicomp '12, Pittsburgh,
               PA, USA, September 5-8, 2012},
  pages     = {1036--1043},
  publisher = {{ACM}},
  year      = {2012},
  url       = {https://doi.org/10.1145/2370216.2370438},
  doi       = {10.1145/2370216.2370438},
  timestamp = {Tue, 06 Nov 2018 16:58:39 +0100},
  biburl    = {https://dblp.org/rec/conf/huc/ZhangS12a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{Ting-He-ArXiv,
      title={Sharing Models or Coresets: A Study based on Membership Inference Attack}, 
      author={Hanlin Lu and Changchang Liu and Ting He and Shiqiang Wang and Kevin S. Chan},
      year={2020},
      eprint={2007.02977},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{Ting-He-IEEE,  author={H. {Lu} and C. {Liu} and S. {Wang} and T. {He} and V. {Narayanan} and K. S. {Chan} and S. {Pasteris}},  booktitle={2020 IFIP Networking Conference (Networking)},   title={Joint Coreset Construction and Quantization for Distributed Machine Learning},   year={2020},  volume={},  number={},}

@article{practicalcoresets,
  title={Practical coreset constructions for machine learning},
  author={Bachem, Olivier and Lucic, Mario and Krause, Andreas},
  journal={arXiv preprint arXiv:1703.06476},
  year={2017}
}

@InProceedings{coresets-traning-pmlr, 
title = {Coresets for Data-efficient Training of Machine Learning Models}, 
author = {Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure}, 
booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
pages = {6950--6960}, 
year = {2020}, 
editor = {Hal Daumé III and Aarti Singh}, 
volume = {119}, 
series = {Proceedings of Machine Learning Research}, 
address = {Virtual}, month = {13--18 Jul}, publisher = {PMLR}
}

@article{coresets-traning-GMM,
author = {Lucic, Mario and Faulkner, Matthew and Krause, Andreas and Feldman, Dan},
title = {Training Gaussian Mixture Models at Scale via Coresets},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {How can we train a statistical mixture model on a massive data set? In this work we show how to construct coresets for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real-world data sets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5885–5909},
numpages = {25},
keywords = {streaming and distributed computation, gaussian mixture models, coresets}
}

@article{coresets-compression,
  title={Data-dependent coresets for compressing neural networks with applications to generalization bounds},
  author={Baykal, Cenk and Liebenwein, Lucas and Gilitschenski, Igor and Feldman, Dan and Rus, Daniela},
  journal={arXiv preprint arXiv:1804.05345},
  year={2018}
}

@inproceedings{coresets-compression-ECCV,
  title={Coreset-based neural network compression},
  author={Dubey, Abhimanyu and Chatterjee, Moitreya and Ahuja, Narendra},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={454--470},
  year={2018}
}

@article{EHWSN-learning,
  title={Learning to Optimize Energy Efficiency in Energy Harvesting Wireless Sensor Networks},
  author={Ghosh, Debamita and Hanawal, Manjesh K and Zlatanov, Nikola},
  journal={arXiv preprint arXiv:2012.15203},
  year={2020}
}

@article{EHWSN-transmission,
  title={Uniform thresholding based transmission policy for energy harvesting wireless sensor nodes in fading channel},
  author={Jaitawat, Arpita and Singh, Arun Kumar},
  journal={Wireless Networks},
  pages={1--10},
  year={2020},
  publisher={Springer}
}

@inproceedings{EHWSN-adaptive,
  title={Adaptive control of duty cycling in energy-harvesting wireless sensor networks},
  author={Vigorito, Christopher M and Ganesan, Deepak and Barto, Andrew G},
  booktitle={2007 4th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks},
  pages={21--30},
  year={2007},
  organization={IEEE}
}

@article{EHWSN-review,
  title={A review on energy management schemes in energy harvesting wireless sensor networks},
  author={Babayo, Aliyu Aliyu and Anisi, Mohammad Hossein and Ali, Ihsan},
  journal={Renewable and Sustainable Energy Reviews},
  volume={76},
  pages={1176--1184},
  year={2017},
  publisher={Elsevier}
}


@article{kang2017neurosurgeon,
  title={Neurosurgeon: Collaborative intelligence between the cloud and mobile edge},
  author={Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
  journal={ACM SIGARCH Computer Architecture News},
  volume={45},
  number={1},
  pages={615--629},
  year={2017},
  publisher={ACM New York, NY, USA}
}


@article{taylor2018adaptive,
  title={Adaptive deep learning model selection on embedded systems},
  author={Taylor, Ben and Marco, Vicent Sanz and Wolff, Willy and Elkhatib, Yehia and Wang, Zheng},
  journal={ACM SIGPLAN Notices},
  volume={53},
  number={6},
  pages={31--43},
  year={2018},
  publisher={ACM New York, NY, USA}
}


@article{zhao2018deepthings,
  title={DeepThings: Distributed adaptive deep learning inference on resource-constrained IoT edge clusters},
  author={Zhao, Zhuoran and Barijough, Kamyar Mirzazad and Gerstlauer, Andreas},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={37},
  number={11},
  pages={2348--2359},
  year={2018},
  publisher={IEEE}
}


@inproceedings{eshratifar2018energy,
  title={Energy and performance efficient computation offloading for deep neural networks in a mobile cloud computing environment},
  author={Eshratifar, Amir Erfan and Pedram, Massoud},
  booktitle={Proceedings of the 2018 on Great Lakes Symposium on VLSI},
  pages={111--116},
  year={2018}
}

@inproceedings{drq-isca2020,
  title={{DRQ}: dynamic region-based quantization for deep neural network acceleration},
  author={Song, Zhuoran and Fu, Bangqi and Wu, Feiyang and Jiang, Zhaoming and Jiang, Li and Jing, Naifeng and Liang, Xiaoyao},
  booktitle={ISCA},
  year={2020},
  organization={IEEE}
}

@article{deepcompression,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{compression-kimura2005survey,
  title={A survey on data compression in wireless sensor networks},
  author={Kimura, Naoto and Latifi, Shahram},
  booktitle={International Conference on Information Technology: Coding and Computing (ITCC'05)-Volume II},
  volume={2},
  pages={8--13},
  year={2005},
  organization={IEEE}
}

@article{compression-hung2012evaluation,
  title={An evaluation of model-based approaches to sensor data compression},
  author={Hung, Nguyen Quoc Viet and Jeung, Hoyoung and Aberer, Karl},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={25},
  number={11},
  pages={2434--2447},
  year={2012},
  publisher={IEEE}
}

@article{compression-marcelloni2008simple,
  title={A simple algorithm for data compression in wireless sensor networks},
  author={Marcelloni, Francesco and Vecchio, Massimo},
  journal={IEEE communications letters},

  year={2008},
  publisher={IEEE}
}

@InProceedings{Origin, 
title = {Origin: Enabling On-Device Intelligence for Human Activity Recognition Using Energy Harvesting Wireless Sensor Networks}, 
author = {Mishra, Cyan Subhra and Sampson, Jack and Kandemir, Mahmut Taylan and Narayanan, Vijaykrishnan}, 
booktitle = {DATE}, 

year = {2021}
}

@misc{Cisco-report,
  title={Cisco annual internet report (2018--2023) white paper},
  author={Cisco, U},
  year={2020}
}

@inproceedings{bachem2015coresets,
  title={Coresets for nonparametric estimation-the case of DP-means},
  author={Bachem, Olivier and Lucic, Mario and Krause, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={209--217},
  year={2015},
  organization={PMLR}
}

@inproceedings{smartglass,
  title={Wearable smart glass: Features, applications, current progress and challenges},
  author={Kumar, Nallapaneni Manoj and Singh, Neeraj Kumar and Peddiny, VK},
  booktitle={2018 Second International Conference on Green Computing and Internet of Things (ICGCIoT)},
  pages={577--582},
  year={2018},
  organization={IEEE}
}

@article{smarthome,
  title={A systematic review of the smart home literature: A user perspective},
  author={Marikyan, Davit and Papagiannidis, Savvas and Alamanos, Eleftherios},
  journal={Technological Forecasting and Social Change},
  volume={138},
  pages={139--154},
  year={2019},
  publisher={Elsevier}
}

@article{industry40,
  title={Predictive maintenance in the Industry 4.0: A systematic literature review},
  author={Zonta, Tiago and da Costa, Cristiano Andr{\'e} and da Rosa Righi, Rodrigo and de Lima, Miromar Jose and da Trindade, Eduardo Silveira and Li, Guann Pyng},
  journal={Computers \& Industrial Engineering},
  volume={150},
  pages={106889},
  year={2020},
  publisher={Elsevier}
}

@article{orbitaledge,
  title={Orbital edge computing: Machine inference in space},
  author={Denby, Bradley and Lucia, Brandon},
  journal={IEEE Computer Architecture Letters},
  volume={18},
  number={1},
  pages={59--62},
  year={2019},
  publisher={IEEE}
}

@article{EHuse,
  title={Survey on wireless sensor network applications and energy efficient routing protocols},
  author={Mohamed, Reem E and Saleh, Ahmed I and Abdelrazzak, Maher and Samra, Ahmed S},
  journal={Wireless Personal Communications},
  volume={101},
  number={2},
  pages={1019--1055},
  year={2018},
  publisher={Springer}
}

@inproceedings {batteryfree,
author = {Kai Geissdoerfer and Marco Zimmerling},
title = {Learning to Communicate Effectively Between Battery-free Devices},
booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
year = {2022},
isbn = {978-1-939133-27-4},
address = {Renton, WA},
pages = {419--435},
url = {https://www.usenix.org/conference/nsdi22/presentation/geissdoerfer},
publisher = {USENIX Association},
month = apr,
}

@article{EHSurvey,
  title={Energy harvesting sensor nodes: Survey and implications},
  author={Sudevalayam, Sujesha and Kulkarni, Purushottam},
  journal={IEEE communications surveys \& tutorials},
  volume={13},
  number={3},
  pages={443--461},
  year={2010},
  publisher={IEEE}
}


@misc{TIEH,
    key         = {Texas Instrument Micro-controller with FRAM},
    title       = {16 MHz MCU with 64KB FRAM, 2KB SRAM, AES, 12-bit ADC, comparator, DMA, UART/SPI/I2C, timer},
    note        = {https://www.ti.com/product/MSP430FR5969},
    year        = 2022
}

@article{WISP,
  title={Design of an RFID-based battery-free programmable sensing platform},
  author={Sample, Alanson P and Yeager, Daniel J and Powledge, Pauline S and Mamishev, Alexander V and Smith, Joshua R},
  journal={IEEE transactions on instrumentation and measurement},
  volume={57},
  number={11},
  pages={2608--2615},
  year={2008},
  publisher={IEEE}
}


@inproceedings{quantcompression,
  title={Distributed compression for sensor networks},
  author={Kusuma, Julius and Doherty, Lance and Ramchandran, Kannan},
  booktitle={Proceedings 2001 International Conference on Image Processing (Cat. No. 01CH37205)},
  volume={1},
  pages={82--85},
  year={2001},
  organization={IEEE}
}

@inproceedings{6907021,
  author={Paul, Rohan and Feldman, Dan and Rus, Daniela and Newman, Paul},
  booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Visual precis generation using coresets}, 
  year={2014},
  volume={},
  number={},
  pages={1304-1311},
  doi={10.1109/ICRA.2014.6907021}
  }
  
@article{coresetDNN,
  title={Data-independent neural pruning via coresets},
  author={Mussay, Ben and Osadchy, Margarita and Braverman, Vladimir and Zhou, Samson and Feldman, Dan},
  journal={arXiv preprint arXiv:1907.04018},
  year={2019}
}

@inproceedings{infocommDNNpart,
  title={Dynamic adaptive DNN surgery for inference acceleration on the edge},
  author={Hu, Chuang and Bao, Wei and Wang, Dan and Liu, Fengming},
  booktitle={IEEE INFOCOM 2019-IEEE Conference on Computer Communications},
  pages={1423--1431},
  year={2019},
  organization={IEEE}
}

@inproceedings{talhaDNN,
  title={Distributed inference acceleration with adaptive DNN partitioning and offloading},
  author={Mohammed, Thaha and Joe-Wong, Carlee and Babbar, Rohit and Di Francesco, Mario},
  booktitle={IEEE INFOCOM 2020-IEEE Conference on Computer Communications},
  pages={854--863},
  year={2020},
  organization={IEEE}
}

@misc{SynDC,
    key         = {Design Compiler: 
Concurrent Timing, Area, Power, and Test Optimization},
    title       = {Design Compiler:
Concurrent Timing, Area, Power, and Test Optimization},
    note        = {https://www.synopsys.com/implementation-and-signoff/rtl-synthesis-test/dc-ultra.html},
    year        = 2022
}

@inproceedings{wu2016cloud,
  title={Cloud-based machine learning for predictive analytics: Tool wear prediction in milling},
  author={Wu, Dazhong and Jennings, Connor and Terpenny, Janis and Kumara, Soundar},
  booktitle={2016 IEEE International Conference on Big Data (Big Data)},
  pages={2062--2069},
  year={2016},
  organization={IEEE}
}

@article{bearing,
  title={Bearing fault detection and diagnosis using case western reserve university dataset with deep learning approaches: A review},
  author={Neupane, Dhiraj and Seok, Jongwon},
  journal={IEEE Access},
  volume={8},
  pages={93155--93178},
  year={2020},
  publisher={IEEE}
}

@article{industry4,
  title={Industry 4.0},
  author={Lasi, Heiner and Fettke, Peter and Kemper, Hans-Georg and Feld, Thomas and Hoffmann, Michael},
  journal={Business \& information systems engineering},
  volume={6},
  number={4},
  pages={239--242},
  year={2014},
  publisher={Springer}
}

@article{XBSIM,
  title={XB-SIM: A simulation framework for modeling and exploration of ReRAM-based CNN acceleration design},
  author={Fei, Xiang and Zhang, Youhui and Zheng, Weimin},
  journal={Tsinghua Science and Technology},
  volume={26},
  number={3},
  pages={322--334},
  year={2020},
  publisher={TUP}
}


@inproceedings{ulpComm1,
  title={Polymorphic radios: A new design paradigm for ultra-low power communication},
  author={Rostami, Mohammad and Gummeson, Jeremy and Kiaghadi, Ali and Ganesan, Deepak},
  booktitle={Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
  pages={446--460},
  year={2018}
}

@inproceedings{olszewski2017realistic,
  title={Realistic dynamic facial textures from a single image using gans},
  author={Olszewski, Kyle and Li, Zimo and Yang, Chao and Zhou, Yi and Yu, Ronald and Huang, Zeng and Xiang, Sitao and Saito, Shunsuke and Kohli, Pushmeet and Li, Hao},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5429--5438},
  year={2017}
}


@article{eren2019generic,
  title={A generic intelligent bearing fault diagnosis system using compact adaptive 1D CNN classifier},
  author={Eren, Levent and Ince, Turker and Kiranyaz, Serkan},
  journal={Journal of Signal Processing Systems},
  volume={91},
  number={2},
  pages={179--189},
  year={2019},
  publisher={Springer}
}

@inproceedings{hoang2017convolutional,
  title={Convolutional neural network based bearing fault diagnosis},
  author={Hoang, Duy-Tang and Kang, Hee-Jun},
  booktitle={International conference on intelligent computing},
  pages={105--111},
  year={2017},
  organization={Springer}
}

@article{oribitalEH1,
  title={Tartan Artibeus: A Batteryless, Computational Satellite Research Platform},
  author={Denby, Brad and Ruppel, Emily and Singh, Vaibhav and Che, Shize and Taylor, Chad and Zaidi, Fayyaz and Kumar, Swarun and Manchester, Zac and Lucia, Brandon},
  year={2022}
}

@article{smartfarmEH1,
  title={AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
  author={Saadane, Rachid and Chehri, Abdellah and Jeon, Seunggil and others},
  journal={Sustainable Energy Technologies and Assessments},
  volume={52},
  pages={102093},
  year={2022},
  publisher={Elsevier}
}

@article{smartfarmsolar,
  title={Maximization of wireless sensor network lifetime using solar energy harvesting for smart agriculture monitoring},
  author={Sharma, Himanshu and Haque, Ahteshamul and Jaffery, Zainul Abdin},
  journal={Ad Hoc Networks},
  volume={94},
  pages={101966},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{smartfarmsolar1,
  title={Solar energy harvesting for smart farming using nanomaterial and machine learning},
  author={Vatti, Rambabu and Vatti, Nagarjuna and Mahender, K and Vatti, Prasanna Lakshmi and Krishnaveni, B},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={981},
  number={3},
  pages={032009},
  year={2020},
  organization={IOP Publishing}
}

@misc{adafruitm4,
    key         = {Adafruit M4 Express},
    title       = {Adafruit Feather M4 Express Featuring ATSAMD51 ATSAMD51 Cortex M4},
    note        = {https://www.adafruit.com/product/3857},
    year        = {2022}
}

@inproceedings{ganerror,
  title={Seeing what a gan cannot generate},
  author={Bau, David and Zhu, Jun-Yan and Wulff, Jonas and Peebles, William and Strobelt, Hendrik and Zhou, Bolei and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4502--4511},
  year={2019}
}

@misc{adafruitI,
    key         = {Adafruit ItsyBitsy nRF52840 Express Bluetooth LE},
    title       = {Adafruit ItsyBitsy nRF52840 Express Bluetooth LE},
    note        = {https://www.adafruit.com/product/4481},
    year        = {2022}
}

@misc{CircuitPy,
    key         = {Circuit Python},
    title       = {Circuit Python},
    note        = {https://circuitpython.org/},
    year        = {2022}
}

@misc{MuE,
    key         = {Mu Editor},
    title       = {Code with Mu a simple Python editor for beginner programmers.},
    note        = {https://codewith.mu/en/},
    year        = {2022}
}
@book{predictive,
  title={An introduction to predictive maintenance},
  author={Mobley, R Keith},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{BearingCNN1,
  title={Convolutional neural net and bearing fault analysis},
  author={Lee, Dean and Siu, Vincent and Cruz, Rick and Yetman, Charles},
  booktitle={Proceedings of the International Conference on Data Science (ICDATA)},
  pages={194},
  year={2016},
  organization={The Steering Committee of The World Congress in Computer Science, Computer~…}
}

@article{BearingCNN2,
  title={A generic intelligent bearing fault diagnosis system using compact adaptive 1D CNN classifier},
  author={Eren, Levent and Ince, Turker and Kiranyaz, Serkan},
  journal={Journal of Signal Processing Systems},
  volume={91},
  number={2},
  pages={179--189},
  year={2019},
  publisher={Springer}
}

@inproceedings{BearingCNN3,
  title={Convolutional neural network based bearing fault diagnosis},
  author={Hoang, Duy-Tang and Kang, Hee-Jun},
  booktitle={International conference on intelligent computing},
  pages={105--111},
  year={2017},
  organization={Springer}
}

@article{BearingCNN4,
  title={An weighted CNN ensemble model with small amount of data for bearing fault diagnosis},
  author={Han, Seungmin and Jeong, Jongpil},
  journal={Procedia Computer Science},
  volume={175},
  pages={88--95},
  year={2020},
  publisher={Elsevier}
}

@misc{grindtime,
    key         = {Automating the Grinding Process},
    title       = {Automating the Grinding Process},
    note        = {\\https://www.sme.org/technologies/articles/2013/january/automating-the-grinding-process/},
    year        = 2013
}
@ARTICLE{smartDust,
  author={B. {Warneke} and M. {Last} and B. {Liebowitz} and K. S. J. {Pister}},
  journal={Computer}, 
  title={Smart Dust: communicating with a cubic-millimeter computer}, 
  year={2001},
  volume={34},
  number={1},
  pages={44-51},}

@misc{AppleECG,
    key         = {Apple Watch - ECG},
    title       = {Taking an ECG with the ECG app on Apple Watch Series 4 or later},
    note        = {https://support.apple.com/en-us/HT208955},
    year        = 2020
}

@misc{AppleFall,
    key         = {Apple Watch - Fall Detection},
    title       = {Use fall detection with Apple Watch},
    note        = {https://support.apple.com/en-us/HT208944},
    year        = 2020
}

@InProceedings{netadapt,
	author = {Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
	title = {NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications},
	booktitle = {ECCV},
	month = {September},
	year = {2018}
}

@misc{Google-Assistant-Watch,
    key         = {Google Assistant for Wearables},
    title       = {Google Assistant for Wearables},
    note        = {https://assistant.google.com/platforms/wearables/},
    year        = 2020
}

@INPROCEEDINGS{ResiRCA,
  author={K. {Qiu} and N. {Jao} and M. {Zhao} and C. S. {Mishra} and G. {Gudukbay} and S. {Jose} and J. {Sampson} and M. T. {Kandemir} and V. {Narayanan}},
  booktitle={2020 HPCA}, 
  title={ResiRCA: A Resilient Energy Harvesting ReRAM Crossbar-Based Accelerator for Intelligent Embedded Processors}, 
  year={2020},
  volume={},
  number={},
  pages={315-327},}
  
  @INPROCEEDINGS{spendthrift,
  author={K. {Ma} and X. {Li} and S. R. {Srinivasa} and Y. {Liu} and J. {Sampson} and Y. {Xie} and V. {Narayanan}},
  booktitle={2017 (ASP-DAC)}, 
  title={Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors}, 
  year={2017},
  volume={},
  number={},
  pages={678-683},}
  
@inproceedings{IntBeyondEdge,
author = {Gobieski, Graham and Lucia, Brandon and Beckmann, Nathan},
title = {Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems},
publisher = {ACM},
booktitle = {ASPLOS},
year={2019}
}

@inproceedings {chinchilla,
author = {Kiwan Maeng and Brandon Lucia},
title = {Adaptive Dynamic Checkpointing for Safe Efficient Intermittent Computing},
booktitle = {OSDI},
year = {2018},
publisher = {{USENIX} Association},
}

@ARTICLE{NVPMa,
  author={K. {Ma} and X. {Li} and S. {Li} and Y. {Liu} and J. J. {Sampson} and Y. {Xie} and V. {Narayanan}},
  journal={IEEE Micro}, 
  title={Nonvolatile Processor Architecture Exploration for Energy-Harvesting Applications}, 
  year={2015},
  volume={35},
  number={5},
  pages={32-40},}
  
  @INPROCEEDINGS{incidental,
  author={K. {Ma} and X. {Li} and J. {Li} and Y. {Liu} and Y. {Xie} and J. {Sampson} and M. T. {Kandemir} and V. {Narayanan}},
  booktitle={MICRO}, 
  year={2017},
  title={Incidental Computing on IoT Nonvolatile Processors}, 
}
  
 
@inproceedings{IntermittentChallange,
  author    = {Brandon Lucia and
               Vignesh Balaji and
               Alexei Colin and
               Kiwan Maeng and
               Emily Ruppel},
  editor    = {Benjamin S. Lerner and
               Rastislav Bod{\'{\i}}k and
               Shriram Krishnamurthi},
  title     = {Intermittent Computing: Challenges and Opportunities},
  booktitle = {2nd Summit on Advances in Programming Languages, {SNAPL} 2017, May
               7-10, 2017, Asilomar, CA, {USA}},
  series    = {LIPIcs},
  volume    = {71},
  pages     = {8:1--8:14},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  year      = {2017},
  url       = {https://doi.org/10.4230/LIPIcs.SNAPL.2017.8},
  doi       = {10.4230/LIPIcs.SNAPL.2017.8},
  timestamp = {Tue, 11 Feb 2020 15:52:14 +0100},
  biburl    = {https://dblp.org/rec/conf/snapl/LuciaBCMR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{NVPMicro,
  author    = {Kaisheng Ma and
               Xueqing Li and
               Karthik Swaminathan and
               Yang Zheng and
               Shuangchen Li and
               Yongpan Liu and
               Yuan Xie and
               John (Jack) Morgan Sampson and
               Vijaykrishnan Narayanan},
  title     = {Nonvolatile Processor Architectures: Efficient, Reliable Progress
               with Unstable Power},
  journal   = {{IEEE} Micro},
  volume    = {36},
  number    = {3},
  pages     = {72--83},
  year      = {2016},
  url       = {https://doi.org/10.1109/MM.2016.35},
  doi       = {10.1109/MM.2016.35},
  timestamp = {Sat, 20 May 2017 00:26:03 +0200},
  biburl    = {https://dblp.org/rec/journals/micro/MaLSZLLXSN16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LuciaCheckpoint,
  author    = {Kiwan Maeng and
               Brandon Lucia},
  editor    = {Andrea C. Arpaci{-}Dusseau and
               Geoff Voelker},
  title     = {Adaptive Dynamic Checkpointing for Safe Efficient Intermittent Computing},
  booktitle = {OSDI},
  publisher = {{USENIX} Association},
  year      = {2018},
}

@inproceedings{intelligenceBeyondEdge,
  author    = {Graham Gobieski and
               Brandon Lucia and
               Nathan Beckmann},
  editor    = {Iris Bahar and
               Maurice Herlihy and
               Emmett Witchel and
               Alvin R. Lebeck},
  title     = {Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems},
  booktitle = {ASPLOS},
  publisher = {{ACM}},
  year      = {2019},
}

@misc{iWatchBattery,
    key         = {Apple Watch},
    title       = {Apple Watch Series 5 teardown reveals bigger battery},
    note        = {https://appleinsider.com/articles/19/09/25/apple-watch-series-5-teardown-reveals-bigger-battery},
    year        = 2019
}

@INPROCEEDINGS{LeNet,
    author = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}

@article{HARHompel,
  author    = {Fernando Moya Rueda and
               Ren{\'{e}} Grzeszick and
               Gernot A. Fink and
               Sascha Feldhorst and
               Michael ten Hompel},
  title     = {Convolutional Neural Networks for Human Activity Recognition Using
               Body-Worn Sensors},
  journal   = {Informatics},
  year      = {2018},
}

@INPROCEEDINGS{HARHaChoi,
  author={S. {Ha} and S. {Choi}},
  booktitle={IJCNN},
  year={2016},
  title={Convolutional neural networks for human activity recognition using multiple accelerometer and gyroscope sensors}, 
}
  
@article{PolikarEnsemble,
  author    = {Robi Polikar},
  title     = {Ensemble learning},
  journal   = {Scholarpedia},
  volume    = {4},
  number    = {1},
  pages     = {2776},
  year      = {2009},
  url       = {https://doi.org/10.4249/scholarpedia.2776},
  doi       = {10.4249/scholarpedia.2776},
  timestamp = {Thu, 23 May 2019 15:09:49 +0200},
  biburl    = {https://dblp.org/rec/journals/scholarpedia/Polikar09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mHealthDroid,
  author    = {Oresti Ba{\~{n}}os and
               Rafael Garc{\'{\i}}a and
               Juan Antonio Holgado Terriza and
               Miguel Damas and
               H{\'{e}}ctor Pomares and
               Ignacio Rojas Ruiz and
               Alejandro Saez and
               Claudia Villalonga},

  title     = {mHealthDroid: {A} Novel Framework for Agile Development of Mobile
               Health Applications},
  booktitle = {IWAAL},
  year={2014},
  publisher = {Springer},
}

@article{mHealth,
author = {Banos, Oresti and Villalonga, Claudia and García, Rafael and Saez, Alejandro and Damas, Miguel and Holgado-Terriza, Juan and Lee, Sungyong and Pomares, Hector and Rojas, Ignacio},
year = {2015},
title = {Design, implementation and validation of a novel open framework for agile development of mobile health applications},
journal = {BioMedical Engineering OnLine},
}

@inproceedings{EAPruning,
  author    = {Tien{-}Ju Yang and
               Yu{-}Hsin Chen and
               Vivienne Sze},
  title     = {Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware
               Pruning},
  booktitle = {CVPR},
  year={2017},
  publisher = {{IEEE}},
}

@misc{GoogleFit,
    key         = {Google Fit},
    title       = {Google APIs for Android},
    url         = "https://developers.google.com/android/reference/com/google/android/gms/location/DetectedActivity",
    year        = 2020
}

@misc{AppleHealth,
    key         = {Apple Developer: CMMotionActivity},
    title       = {Apple Developer: CMMotionActivity},
    url        = "https://developer.apple.com/documentation/coremotion/cmmotionactivity\#topics",
    year        = 2020
}

@misc{keras,
  title={Keras},
  author={Chollet, Francois and others},
  year={2015},
  publisher={GitHub},
  url={https://github.com/fchollet/keras},
}

@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{PAMAP21,
  author    = {Attila Reiss and
               Didier Stricker},
  title     = {Introducing a New Benchmarked Dataset for Activity Monitoring},
  booktitle = {ISWC},
  publisher = {{IEEE}},
  year      = {2012},
}

@inproceedings{PAMAP22,
  author    = {Attila Reiss and
               Didier Stricker},
  editor    = {Fillia Makedon},
  title     = {Creating and benchmarking a new dataset for physical activity monitoring},
  booktitle = {PETRA},
  year={2012},
  publisher = {{ACM}},

}

@INPROCEEDINGS{NIP,
  author={F. {Su} and W. {Chen} and L. {Xia} and C. {Lo} and T. {Tang} and Z. {Wang} and K. {Hsu} and M. {Cheng} and J. {Li} and Y. {Xie} and Y. {Wang} and M. {Chang} and H. {Yang} and Y. {Liu}},
  booktitle={2017 Symposium on VLSI Technology}, 
  title={A 462GOPs/J RRAM-based nonvolatile intelligent processor for energy harvesting IoE system featuring nonvolatile logics and processing-in-memory}, 
  year={2017},
  volume={},
  number={},
  pages={T260-T261},}
  
  @article{Jordao:2018,
author    = {Artur Jordao,
Antonio Carlos Nazare,
Jessica Sena and
William Robson Schwartz},
title     = {Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art},
journal   = {arXiv},
year      = {2018},
eprint    = {1806.05226},
}

@inproceedings{USCHAD,
  author    = {Mi Zhang and
               Alexander A. Sawchuk},
  editor    = {Anind K. Dey and
               Hao{-}Hua Chu and
               Gillian R. Hayes},
  title     = {{USC-HAD:} a daily activity dataset for ubiquitous activity recognition
               using wearable sensors},
  booktitle = {The 2012 {ACM} Conference on Ubiquitous Computing, Ubicomp '12, Pittsburgh,
               PA, USA, September 5-8, 2012},
  pages     = {1036--1043},
  publisher = {{ACM}},
  year      = {2012},
  url       = {https://doi.org/10.1145/2370216.2370438},
  doi       = {10.1145/2370216.2370438},
  timestamp = {Tue, 06 Nov 2018 16:58:39 +0100},
  biburl    = {https://dblp.org/rec/conf/huc/ZhangS12a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{Ting-He-ArXiv,
      title={Sharing Models or Coresets: A Study based on Membership Inference Attack}, 
      author={Hanlin Lu and Changchang Liu and Ting He and Shiqiang Wang and Kevin S. Chan},
      year={2020},
      eprint={2007.02977},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{Ting-He-IEEE,  author={H. {Lu} and C. {Liu} and S. {Wang} and T. {He} and V. {Narayanan} and K. S. {Chan} and S. {Pasteris}},  booktitle={2020 IFIP Networking Conference (Networking)},   title={Joint Coreset Construction and Quantization for Distributed Machine Learning},   year={2020},  volume={},  number={},}

@article{practicalcoresets,
  title={Practical coreset constructions for machine learning},
  author={Bachem, Olivier and Lucic, Mario and Krause, Andreas},
  journal={arXiv preprint arXiv:1703.06476},
  year={2017}
}

@InProceedings{coresets-traning-pmlr, 
title = {Coresets for Data-efficient Training of Machine Learning Models}, 
author = {Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure}, 
booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
pages = {6950--6960}, 
year = {2020}, 
editor = {Hal Daumé III and Aarti Singh}, 
volume = {119}, 
series = {Proceedings of Machine Learning Research}, 
address = {Virtual}, month = {13--18 Jul}, publisher = {PMLR}
}

@article{coresets-traning-GMM,
author = {Lucic, Mario and Faulkner, Matthew and Krause, Andreas and Feldman, Dan},
title = {Training Gaussian Mixture Models at Scale via Coresets},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {How can we train a statistical mixture model on a massive data set? In this work we show how to construct coresets for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real-world data sets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5885–5909},
numpages = {25},
keywords = {streaming and distributed computation, gaussian mixture models, coresets}
}

@article{coresets-compression,
  title={Data-dependent coresets for compressing neural networks with applications to generalization bounds},
  author={Baykal, Cenk and Liebenwein, Lucas and Gilitschenski, Igor and Feldman, Dan and Rus, Daniela},
  journal={arXiv preprint arXiv:1804.05345},
  year={2018}
}

@inproceedings{coresets-compression-ECCV,
  title={Coreset-based neural network compression},
  author={Dubey, Abhimanyu and Chatterjee, Moitreya and Ahuja, Narendra},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={454--470},
  year={2018}
}

@article{EHWSN-learning,
  title={Learning to Optimize Energy Efficiency in Energy Harvesting Wireless Sensor Networks},
  author={Ghosh, Debamita and Hanawal, Manjesh K and Zlatanov, Nikola},
  journal={arXiv preprint arXiv:2012.15203},
  year={2020}
}

@article{EHWSN-transmission,
  title={Uniform thresholding based transmission policy for energy harvesting wireless sensor nodes in fading channel},
  author={Jaitawat, Arpita and Singh, Arun Kumar},
  journal={Wireless Networks},
  pages={1--10},
  year={2020},
  publisher={Springer}
}

@inproceedings{EHWSN-adaptive,
  title={Adaptive control of duty cycling in energy-harvesting wireless sensor networks},
  author={Vigorito, Christopher M and Ganesan, Deepak and Barto, Andrew G},
  booktitle={2007 4th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks},
  pages={21--30},
  year={2007},
  organization={IEEE}
}

@article{EHWSN-review,
  title={A review on energy management schemes in energy harvesting wireless sensor networks},
  author={Babayo, Aliyu Aliyu and Anisi, Mohammad Hossein and Ali, Ihsan},
  journal={Renewable and Sustainable Energy Reviews},
  volume={76},
  pages={1176--1184},
  year={2017},
  publisher={Elsevier}
}


@article{kang2017neurosurgeon,
  title={Neurosurgeon: Collaborative intelligence between the cloud and mobile edge},
  author={Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
  journal={ACM SIGARCH Computer Architecture News},
  volume={45},
  number={1},
  pages={615--629},
  year={2017},
  publisher={ACM New York, NY, USA}
}


@article{taylor2018adaptive,
  title={Adaptive deep learning model selection on embedded systems},
  author={Taylor, Ben and Marco, Vicent Sanz and Wolff, Willy and Elkhatib, Yehia and Wang, Zheng},
  journal={ACM SIGPLAN Notices},
  volume={53},
  number={6},
  pages={31--43},
  year={2018},
  publisher={ACM New York, NY, USA}
}


@article{zhao2018deepthings,
  title={DeepThings: Distributed adaptive deep learning inference on resource-constrained IoT edge clusters},
  author={Zhao, Zhuoran and Barijough, Kamyar Mirzazad and Gerstlauer, Andreas},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={37},
  number={11},
  pages={2348--2359},
  year={2018},
  publisher={IEEE}
}


@inproceedings{eshratifar2018energy,
  title={Energy and performance efficient computation offloading for deep neural networks in a mobile cloud computing environment},
  author={Eshratifar, Amir Erfan and Pedram, Massoud},
  booktitle={Proceedings of the 2018 on Great Lakes Symposium on VLSI},
  pages={111--116},
  year={2018}
}

@inproceedings{drq-isca2020,
  title={{DRQ}: dynamic region-based quantization for deep neural network acceleration},
  author={Song, Zhuoran and Fu, Bangqi and Wu, Feiyang and Jiang, Zhaoming and Jiang, Li and Jing, Naifeng and Liang, Xiaoyao},
  booktitle={ISCA},
  year={2020},
  organization={IEEE}
}

@article{deepcompression,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{compression-kimura2005survey,
  title={A survey on data compression in wireless sensor networks},
  author={Kimura, Naoto and Latifi, Shahram},
  booktitle={International Conference on Information Technology: Coding and Computing (ITCC'05)-Volume II},
  volume={2},
  pages={8--13},
  year={2005},
  organization={IEEE}
}

@article{compression-hung2012evaluation,
  title={An evaluation of model-based approaches to sensor data compression},
  author={Hung, Nguyen Quoc Viet and Jeung, Hoyoung and Aberer, Karl},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={25},
  number={11},
  pages={2434--2447},
  year={2012},
  publisher={IEEE}
}

@article{compression-marcelloni2008simple,
  title={A simple algorithm for data compression in wireless sensor networks},
  author={Marcelloni, Francesco and Vecchio, Massimo},
  journal={IEEE communications letters},

  year={2008},
  publisher={IEEE}
}

@InProceedings{Origin, 
title = {Origin: Enabling On-Device Intelligence for Human Activity Recognition Using Energy Harvesting Wireless Sensor Networks}, 
author = {Mishra, Cyan Subhra and Sampson, Jack and Kandemir, Mahmut Taylan and Narayanan, Vijaykrishnan}, 
booktitle = {DATE}, 

year = {2021}
}

@misc{Cisco-report,
  title={Cisco annual internet report (2018--2023) white paper},
  author={Cisco, U},
  year={2020}
}

@inproceedings{bachem2015coresets,
  title={Coresets for nonparametric estimation-the case of DP-means},
  author={Bachem, Olivier and Lucic, Mario and Krause, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={209--217},
  year={2015},
  organization={PMLR}
}

@inproceedings{smartglass,
  title={Wearable smart glass: Features, applications, current progress and challenges},
  author={Kumar, Nallapaneni Manoj and Singh, Neeraj Kumar and Peddiny, VK},
  booktitle={2018 Second International Conference on Green Computing and Internet of Things (ICGCIoT)},
  pages={577--582},
  year={2018},
  organization={IEEE}
}

@article{smarthome,
  title={A systematic review of the smart home literature: A user perspective},
  author={Marikyan, Davit and Papagiannidis, Savvas and Alamanos, Eleftherios},
  journal={Technological Forecasting and Social Change},
  volume={138},
  pages={139--154},
  year={2019},
  publisher={Elsevier}
}

@article{industry40,
  title={Predictive maintenance in the Industry 4.0: A systematic literature review},
  author={Zonta, Tiago and da Costa, Cristiano Andr{\'e} and da Rosa Righi, Rodrigo and de Lima, Miromar Jose and da Trindade, Eduardo Silveira and Li, Guann Pyng},
  journal={Computers \& Industrial Engineering},
  volume={150},
  pages={106889},
  year={2020},
  publisher={Elsevier}
}

@article{orbitaledge,
  title={Orbital edge computing: Machine inference in space},
  author={Denby, Bradley and Lucia, Brandon},
  journal={IEEE Computer Architecture Letters},
  volume={18},
  number={1},
  pages={59--62},
  year={2019},
  publisher={IEEE}
}

@article{EHuse,
  title={Survey on wireless sensor network applications and energy efficient routing protocols},
  author={Mohamed, Reem E and Saleh, Ahmed I and Abdelrazzak, Maher and Samra, Ahmed S},
  journal={Wireless Personal Communications},
  volume={101},
  number={2},
  pages={1019--1055},
  year={2018},
  publisher={Springer}
}

@inproceedings {batteryfree,
author = {Kai Geissdoerfer and Marco Zimmerling},
title = {Learning to Communicate Effectively Between Battery-free Devices},
booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
year = {2022},
isbn = {978-1-939133-27-4},
address = {Renton, WA},
pages = {419--435},
url = {https://www.usenix.org/conference/nsdi22/presentation/geissdoerfer},
publisher = {USENIX Association},
month = apr,
}

@article{EHSurvey,
  title={Energy harvesting sensor nodes: Survey and implications},
  author={Sudevalayam, Sujesha and Kulkarni, Purushottam},
  journal={IEEE communications surveys \& tutorials},
  volume={13},
  number={3},
  pages={443--461},
  year={2010},
  publisher={IEEE}
}


@misc{TIEH,
    key         = {Texas Instrument Micro-controller with FRAM},
    title       = {16 MHz MCU with 64KB FRAM, 2KB SRAM, AES, 12-bit ADC, comparator, DMA, UART/SPI/I2C, timer},
    note        = {https://www.ti.com/product/MSP430FR5969},
    year        = 2022
}

@article{WISP,
  title={Design of an RFID-based battery-free programmable sensing platform},
  author={Sample, Alanson P and Yeager, Daniel J and Powledge, Pauline S and Mamishev, Alexander V and Smith, Joshua R},
  journal={IEEE transactions on instrumentation and measurement},
  volume={57},
  number={11},
  pages={2608--2615},
  year={2008},
  publisher={IEEE}
}


@inproceedings{quantcompression,
  title={Distributed compression for sensor networks},
  author={Kusuma, Julius and Doherty, Lance and Ramchandran, Kannan},
  booktitle={Proceedings 2001 International Conference on Image Processing (Cat. No. 01CH37205)},
  volume={1},
  pages={82--85},
  year={2001},
  organization={IEEE}
}

@inproceedings{6907021,
  author={Paul, Rohan and Feldman, Dan and Rus, Daniela and Newman, Paul},
  booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Visual precis generation using coresets}, 
  year={2014},
  volume={},
  number={},
  pages={1304-1311},
  doi={10.1109/ICRA.2014.6907021}
  }
  
@article{coresetDNN,
  title={Data-independent neural pruning via coresets},
  author={Mussay, Ben and Osadchy, Margarita and Braverman, Vladimir and Zhou, Samson and Feldman, Dan},
  journal={arXiv preprint arXiv:1907.04018},
  year={2019}
}

@inproceedings{infocommDNNpart,
  title={Dynamic adaptive DNN surgery for inference acceleration on the edge},
  author={Hu, Chuang and Bao, Wei and Wang, Dan and Liu, Fengming},
  booktitle={IEEE INFOCOM 2019-IEEE Conference on Computer Communications},
  pages={1423--1431},
  year={2019},
  organization={IEEE}
}

@inproceedings{talhaDNN,
  title={Distributed inference acceleration with adaptive DNN partitioning and offloading},
  author={Mohammed, Thaha and Joe-Wong, Carlee and Babbar, Rohit and Di Francesco, Mario},
  booktitle={IEEE INFOCOM 2020-IEEE Conference on Computer Communications},
  pages={854--863},
  year={2020},
  organization={IEEE}
}

@misc{SynDC,
    key         = {Design Compiler: 
Concurrent Timing, Area, Power, and Test Optimization},
    title       = {Design Compiler:
Concurrent Timing, Area, Power, and Test Optimization},
    note        = {https://www.synopsys.com/implementation-and-signoff/rtl-synthesis-test/dc-ultra.html},
    year        = 2022
}

@inproceedings{wu2016cloud,
  title={Cloud-based machine learning for predictive analytics: Tool wear prediction in milling},
  author={Wu, Dazhong and Jennings, Connor and Terpenny, Janis and Kumara, Soundar},
  booktitle={2016 IEEE International Conference on Big Data (Big Data)},
  pages={2062--2069},
  year={2016},
  organization={IEEE}
}

@article{bearing,
  title={Bearing fault detection and diagnosis using case western reserve university dataset with deep learning approaches: A review},
  author={Neupane, Dhiraj and Seok, Jongwon},
  journal={IEEE Access},
  volume={8},
  pages={93155--93178},
  year={2020},
  publisher={IEEE}
}

@article{industry4,
  title={Industry 4.0},
  author={Lasi, Heiner and Fettke, Peter and Kemper, Hans-Georg and Feld, Thomas and Hoffmann, Michael},
  journal={Business \& information systems engineering},
  volume={6},
  number={4},
  pages={239--242},
  year={2014},
  publisher={Springer}
}

@article{XBSIM,
  title={XB-SIM: A simulation framework for modeling and exploration of ReRAM-based CNN acceleration design},
  author={Fei, Xiang and Zhang, Youhui and Zheng, Weimin},
  journal={Tsinghua Science and Technology},
  volume={26},
  number={3},
  pages={322--334},
  year={2020},
  publisher={TUP}
}


@inproceedings{ulpComm1,
  title={Polymorphic radios: A new design paradigm for ultra-low power communication},
  author={Rostami, Mohammad and Gummeson, Jeremy and Kiaghadi, Ali and Ganesan, Deepak},
  booktitle={Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
  pages={446--460},
  year={2018}
}

@inproceedings{olszewski2017realistic,
  title={Realistic dynamic facial textures from a single image using gans},
  author={Olszewski, Kyle and Li, Zimo and Yang, Chao and Zhou, Yi and Yu, Ronald and Huang, Zeng and Xiang, Sitao and Saito, Shunsuke and Kohli, Pushmeet and Li, Hao},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5429--5438},
  year={2017}
}


@article{eren2019generic,
  title={A generic intelligent bearing fault diagnosis system using compact adaptive 1D CNN classifier},
  author={Eren, Levent and Ince, Turker and Kiranyaz, Serkan},
  journal={Journal of Signal Processing Systems},
  volume={91},
  number={2},
  pages={179--189},
  year={2019},
  publisher={Springer}
}

@inproceedings{hoang2017convolutional,
  title={Convolutional neural network based bearing fault diagnosis},
  author={Hoang, Duy-Tang and Kang, Hee-Jun},
  booktitle={International conference on intelligent computing},
  pages={105--111},
  year={2017},
  organization={Springer}
}

@article{oribitalEH1,
  title={Tartan Artibeus: A Batteryless, Computational Satellite Research Platform},
  author={Denby, Brad and Ruppel, Emily and Singh, Vaibhav and Che, Shize and Taylor, Chad and Zaidi, Fayyaz and Kumar, Swarun and Manchester, Zac and Lucia, Brandon},
  year={2022}
}

@article{smartfarmEH1,
  title={AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
  author={Saadane, Rachid and Chehri, Abdellah and Jeon, Seunggil and others},
  journal={Sustainable Energy Technologies and Assessments},
  volume={52},
  pages={102093},
  year={2022},
  publisher={Elsevier}
}

@article{smartfarmsolar,
  title={Maximization of wireless sensor network lifetime using solar energy harvesting for smart agriculture monitoring},
  author={Sharma, Himanshu and Haque, Ahteshamul and Jaffery, Zainul Abdin},
  journal={Ad Hoc Networks},
  volume={94},
  pages={101966},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{smartfarmsolar1,
  title={Solar energy harvesting for smart farming using nanomaterial and machine learning},
  author={Vatti, Rambabu and Vatti, Nagarjuna and Mahender, K and Vatti, Prasanna Lakshmi and Krishnaveni, B},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={981},
  number={3},
  pages={032009},
  year={2020},
  organization={IOP Publishing}
}

@misc{adafruitm4,
    key         = {Adafruit M4 Express},
    title       = {Adafruit Feather M4 Express Featuring ATSAMD51 ATSAMD51 Cortex M4},
    note        = {https://www.adafruit.com/product/3857},
    year        = {2022}
}

@inproceedings{ganerror,
  title={Seeing what a gan cannot generate},
  author={Bau, David and Zhu, Jun-Yan and Wulff, Jonas and Peebles, William and Strobelt, Hendrik and Zhou, Bolei and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4502--4511},
  year={2019}
}

@misc{adafruitI,
    key         = {Adafruit ItsyBitsy nRF52840 Express Bluetooth LE},
    title       = {Adafruit ItsyBitsy nRF52840 Express Bluetooth LE},
    note        = {https://www.adafruit.com/product/4481},
    year        = {2022}
}

@misc{CircuitPy,
    key         = {Circuit Python},
    title       = {Circuit Python},
    note        = {https://circuitpython.org/},
    year        = {2022}
}

@misc{MuE,
    key         = {Mu Editor},
    title       = {Code with Mu a simple Python editor for beginner programmers.},
    note        = {https://codewith.mu/en/},
    year        = {2022}
}
@book{predictive,
  title={An introduction to predictive maintenance},
  author={Mobley, R Keith},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{BearingCNN1,
  title={Convolutional neural net and bearing fault analysis},
  author={Lee, Dean and Siu, Vincent and Cruz, Rick and Yetman, Charles},
  booktitle={Proceedings of the International Conference on Data Science (ICDATA)},
  pages={194},
  year={2016},
  organization={The Steering Committee of The World Congress in Computer Science, Computer~…}
}

@article{BearingCNN2,
  title={A generic intelligent bearing fault diagnosis system using compact adaptive 1D CNN classifier},
  author={Eren, Levent and Ince, Turker and Kiranyaz, Serkan},
  journal={Journal of Signal Processing Systems},
  volume={91},
  number={2},
  pages={179--189},
  year={2019},
  publisher={Springer}
}

@inproceedings{BearingCNN3,
  title={Convolutional neural network based bearing fault diagnosis},
  author={Hoang, Duy-Tang and Kang, Hee-Jun},
  booktitle={International conference on intelligent computing},
  pages={105--111},
  year={2017},
  organization={Springer}
}

@article{BearingCNN4,
  title={An weighted CNN ensemble model with small amount of data for bearing fault diagnosis},
  author={Han, Seungmin and Jeong, Jongpil},
  journal={Procedia Computer Science},
  volume={175},
  pages={88--95},
  year={2020},
  publisher={Elsevier}
}

@misc{grindtime,
    key         = {Automating the Grinding Process},
    title       = {Automating the Grinding Process},
    note        = {\\https://www.sme.org/technologies/articles/2013/january/automating-the-grinding-process/},
    year        = 2013
}
% ========================================================================
% Bibliography entries from Chapter 6 (Usas - HPCA24)
% ========================================================================

@book{ushas,
  title={Encyclopedia of hinduism},
  author={Jones, Constance and Ryan, James D},
  year={2006},
  publisher={Infobase publishing}
}

@inproceedings{ekya,
  title={Ekya: Continuous learning of video analytics models on edge compute servers},
  author={Bhardwaj, Romil and Xia, Zhengxu and Ananthanarayanan, Ganesh and Jiang, Junchen and Shu, Yuanchao and Karianakis, Nikolaos and Hsieh, Kevin and Bahl, Paramvir and Stoica, Ion},
  booktitle={19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
  pages={119--135},
  year={2022}
}

@techreport{bellevuereport,
  title={Traffic video analytics-case study report},
  author={Ananthanarayanan, Ganesh and Bahl, Victor and Shu, Yuanchao and Loewenherz, Franz and Lai, Daniel and Akers, Darcy and Cao, Peiwei and Xia, Fan and Zhang, Jiangbo and Song, Ashley},
  year={2019},
  institution={Technical Report MSR-TR-1970-3. Microsoft and City of Bellevue.}
}

@inproceedings{anomalyD,
  title={Towards anomaly detectors that learn continuously},
  author={Stocco, Andrea and Tonella, Paolo},
  booktitle={2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  pages={201--208},
  year={2020},
  organization={IEEE}
}

@article{DNNEdge,
  title={Deep learning with edge computing: A review},
  author={Chen, Jiasi and Ran, Xukan},
  journal={Proceedings of the IEEE},
  volume={107},
  number={8},
  pages={1655--1674},
  year={2019},
  publisher={IEEE}
}

@article{samplingbias,
  title={Sampling bias in deep active classification: An empirical study},
  author={Prabhu, Ameya and Dognin, Charles and Singh, Maneesh},
  journal={arXiv preprint arXiv:1909.09389},
  year={2019}
}

@inproceedings{student_teacher01,
  title={Continual learning in the teacher-student setup: Impact of task similarity},
  author={Lee, Sebastian and Goldt, Sebastian and Saxe, Andrew},
  booktitle={International Conference on Machine Learning},
  pages={6109--6119},
  year={2021},
  organization={PMLR}
}

@article{CForget01,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@inproceedings{exemplar01,
  title={Online continual learning for visual food classification},
  author={He, Jiangpeng and Zhu, Fengqing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2337--2346},
  year={2021}
}

@article{DNNenergy01,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{DNNcarbon01,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@inproceedings{DNNcarbon02,
  title={Measuring the carbon intensity of ai in cloud instances},
  author={Dodge, Jesse and Prewitt, Taylor and Tachet des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A and DeCario, Nicole and Buchanan, Will},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1877--1894},
  year={2022}
}

@inproceedings{mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={{M Sandler, A Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen  }},
  booktitle={CVPR},
  year={2018}
}

@article{opExCarbon,
  title={Chasing carbon: The elusive environmental footprint of computing},
  author={Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
  journal={IEEE Micro},
  volume={42},
  number={4},
  pages={37--47},
  year={2022},
  publisher={IEEE}
}

@article{NVPma,
  title={Nonvolatile processor architecture exploration for energy-harvesting applications},
  author={Ma, Kaisheng and Li, Xueqing and Li, Shuangchen and Liu, Yongpan and Sampson, Jack and Xie, Yuan and Narayanan, Vijaykrishnan},
  journal={IEEE Micro},
  volume={35},
  number={5},
  pages={32--40},
  year={2015},
  publisher={IEEE}
}

@inproceedings{resiRCA,
  title={ResiRCA: A resilient energy harvesting ReRAM crossbar-based accelerator for intelligent embedded processors},
  author={Qiu, Keni and Jao, Nicholas and Zhao, Mengying and Mishra, Cyan Subhra and Gudukbay, Gulsum and Jose, Sethu and Sampson, Jack and Kandemir, Mahmut Taylan and Narayanan, Vijaykrishnan},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={315--327},
  year={2020},
  organization={IEEE}
}

@inproceedings{dadiannao,
  title={Dadiannao: A machine-learning supercomputer},
  author={Chen, Yunji and Luo, Tao and Liu, Shaoli and Zhang, Shijin and He, Liqiang and Wang, Jia and Li, Ling and Chen, Tianshi and Xu, Zhiwei and Sun, Ninghui and others},
  booktitle={2014 47th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={609--622},
  year={2014},
  organization={IEEE}
}

@inproceedings{videoedge,
  title={Videoedge: Processing camera streams using hierarchical clusters},
  author={Hung, Chien-Chun and Ananthanarayanan, Ganesh and Bodik, Peter and Golubchik, Leana and Yu, Minlan and Bahl, Paramvir and Philipose, Matthai},
  booktitle={2018 IEEE/ACM Symposium on Edge Computing (SEC)},
  pages={115--131},
  year={2018},
  organization={IEEE}
}

@article{deepcompression,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{chameleon,
  title={Chameleon: Scalable Adaptation of Video Analytics},
  author={{Junchen Jiang, Ganesh Ananthanarayanan, Peter Bodík, Siddhartha Sen, Ion Stoica}},
  booktitle={ACM SIGCOMM},
  year={2018}
}

@inproceedings{TPU,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@article{scalesim,
  title={Scale-sim: Systolic cnn accelerator simulator},
  author={Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
  journal={arXiv preprint arXiv:1811.02883},
  year={2018}
}

@article{eyeriss2,
  title={Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices},
  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={2},
  pages={292--308},
  year={2019},
  publisher={IEEE}
}

@article{cacti,
  title={CACTI 6.0: A tool to model large caches},
  author={Muralimanohar, Naveen and Balasubramonian, Rajeev and Jouppi, Norman P},
  journal={HP laboratories},
  volume={27},
  pages={28},
  year={2009}
}

@article{dramsim3,
  title={DRAMsim3: a cycle-accurate, thermal-capable DRAM simulator},
  author={Li, Shang and Yang, Zhiyuan and Reddy, Dhiraj and Srivastava, Ankur and Jacob, Bruce},
  journal={IEEE Computer Architecture Letters},
  volume={19},
  number={2},
  pages={106--109},
  year={2020},
  publisher={IEEE}
}

@article{cnvlutin,
  title={Cnvlutin: Ineffectual-neuron-free deep neural network computing},
  author={Albericio, Jorge and Judd, Patrick and Hetherington, Tayler and Aamodt, Tor and Jerger, Natalie Enright and Moshovos, Andreas},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={1--13},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{intermittentLearning,
  title={Intermittent learning: On-device machine learning on intermittently powered system},
  author={Lee, Seulki and Islam, Bashima and Luo, Yubo and Nirjon, Shahriar},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={3},
  number={4},
  pages={1--30},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{flexblock,
  title={FlexBlock: A flexible DNN training accelerator with multi-mode block floating point support},
  author={Noh, Seock-Hwan and Koo, Jahyun and Lee, Seunghyun and Park, Jongse and Kung, Jaeha},
  journal={IEEE Transactions on Computers},
  year={2023},
  publisher={IEEE}
}

@article{flexsa,
  title={FlexSA: Flexible systolic array architecture for efficient pruned DNN model training},
  author={Lym, Sangkug and Erez, Mattan},
  journal={arXiv preprint arXiv:2004.13027},
  year={2020}
}

@article{DBSCAN,
  title={DBSCAN revisited, revisited: why and how you should (still) use DBSCAN},
  author={Schubert, Erich and Sander, J{\"o}rg and Ester, Martin and Kriegel, Hans Peter and Xu, Xiaowei},
  journal={ACM Transactions on Database Systems (TODS)},
  volume={42},
  number={3},
  pages={1--21},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{STTlife,
  title={Architecture design with STT-RAM: Opportunities and challenges},
  author={Chi, Ping and Li, Shuangchen and Cheng, Yuanqing and Lu, Yu and Kang, Seung H and Xie, Yuan},
  booktitle={2016 21st Asia and South Pacific design automation conference (ASP-DAC)},
  pages={109--114},
  year={2016},
  organization={IEEE}
}

@inproceedings{CL3,
  title={Towards open world object detection},
  author={Joseph, KJ and Khan, Salman and Khan, Fahad Shahbaz and Balasubramanian, Vineeth N},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5830--5840},
  year={2021}
}

@inproceedings{CL1,
  title={Der: Dynamically expandable representation for class incremental learning},
  author={Yan, Shipeng and Xie, Jiangwei and He, Xuming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3014--3023},
  year={2021}
}

@article{CL2,
  title={Efficient lifelong learning with a-gem},
  author={Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:1812.00420},
  year={2018}
}

@inproceedings{CL4,
  title={Gdumb: A simple approach that questions our progress in continual learning},
  author={Prabhu, Ameya and Torr, Philip HS and Dokania, Puneet K},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={524--540},
  year={2020},
  organization={Springer}
}

@inproceedings{mementos,
  title={Mementos: System support for long-running computation on RFID-scale devices},
  author={Ransford, Benjamin and Sorber, Jacob and Fu, Kevin},
  booktitle={Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
  pages={159--170},
  year={2011}
}

@inproceedings{chain,
  title={Chain: tasks and channels for reliable intermittent programs},
  author={Colin, Alexei and Lucia, Brandon},
  booktitle={Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  pages={514--530},
  year={2016}
}

@inproceedings{capybara,
  title={Reconfigurable energy storage architecture for energy-harvesting devices},
  author={Colin, Alexei and Harvey, Graham and Lucia, Brandon and Sample, Alanson P},
  booktitle={Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={767--781},
  year={2018}
}

@inproceedings{botoks,
  title={Botoks: Reliable batteryless wireless communication with a constant-quality service},
  author={Majid, Amjad Yousef and Bertrand, Gregory D and Mangal, Vaidyanath and Bakar, Nur A Abu and Lucia, Brandon and Gummeson, Jeremy},
  booktitle={Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
  pages={514--527},
  year={2020}
}

@misc{unicef-cobalt,
  title={Child Labour in Cobalt Mining in the Democratic Republic of the Congo},
  author={{UNICEF}},
  howpublished={\url{https://www.unicef.org/drc/en/child-labour-cobalt-mining}},
  year={2021}
}

