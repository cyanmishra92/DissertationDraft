T2:
% \subsection*{Remarks}

% - The proof relies on the convexity of the loss functions and Lipschitz continuity of the gradients to ensure the descent property and boundedness of the gradient norms.
% - The assumption that each sensor participates infinitely often ensures that the aggregated gradient \( \bar{g}^{(t)} \) approximates the true gradient \( \nabla L(\theta_G^{(t)}) \) over time.
% - In practice, participation may be random due to energy constraints. Incorporating probabilistic models of participation can extend the analysis to more general settings.



% \begin{thebibliography}{9}
% \bibitem{bottou2018optimization}
% Bottou, L., Curtis, F. E., \& Nocedal, J. (2018). Optimization methods for large-scale machine learning. \emph{SIAM Review}, 60(2), 223-311.

% \bibitem{stich2018local}
% Stich, S. U. (2018). Local SGD converges fast and communicates little. \emph{arXiv preprint arXiv:1805.09767}.

% \bibitem{li2019convergence}
% Li, X., Huang, K., Yang, W., Wang, S., \& Zhang, Z. (2019). On the convergence of FedAvg on non-IID data. \emph{arXiv preprint arXiv:1907.02189}.
% \end{thebibliography}