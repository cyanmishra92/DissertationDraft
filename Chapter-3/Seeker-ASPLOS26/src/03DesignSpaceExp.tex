\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{figs/ecosystem.pdf}
  \caption{An example of EH Sensor-Host ecosystem - the sensor transitions between multiple states and executes the compute as store and execute fashion~\cite{Origin}. The host receives the data in compressed form for the unfinished portion, decompresses it, runs inference and finally ensembles the results from multiple sensors to improve accuracy and robustness.}
  \label{Fig:ecosystem}
\end{figure}
Since data communication in a sensor host ecosystem (Figure~\ref{Fig:ecosystem}) consumes substantial power, we rely on coresets as an efficient way to lossily communicate the features with minimal information degradation. The coreset construction techniques need to be extremely lightweight while preserving key features to justify the computation-communication trade-offs in energy and latency. To this end, we explore two different kinds of coreset construction techniques. 

\subsection{Coreset Construction Techniques}
\textbf{\underline{Coreset Construction Using Importance Sampling:}~}An easy way to build a representation from a data 
distribution is to perform importance sampling~\cite{practicalcoresets, bachem2015coresets}, i.e. give more importance in choosing the data which are unique and, in our case, contribute significant to the inference (i.e. having a high enough magnitude in the frequency response of the sensor signal). The intuition is that any importance
sampling scheme produces an unbiased estimator~\cite{practicalcoresets}. To preserve the temporal and frequency features, we ensure sampling data which are far enough from each other to build a better representation. The entire process of importance sampling uses simple arithmetic operations and is therefore viable in energy-scarce situations.
The host can take the sub-sampled data and perform inference. The caveat is to have a model trained on the sub-sampled data, which can be done as an one-time step. Although the sub-sampling might lead to poor inference accuracy, in our experiments, with iso-compression ratio, importance sampling based coresets still outperforms classical compression techniques. Figure~\ref{Fig:Coresets} shows a toy example of importance sampling in a 2D data set. Observe that the selected points (in \textcolor{red}{red}) are approximating the original distribution.


\noindent \textbf{\underline{Coreset Construction Using Clustering:}~} Although importance sapling based coreset construction is computationally inexpensive, it suffers from accuracy loss because it doesn't explicitly preserve the intricate structure of the data points. To address this, we also utilize coreset construction using k-means clustering~\cite{Ting-He-ArXiv, Ting-He-IEEE, practicalcoresets}, which separates the data points into a set of k (or fewer) N-spherical clusters and represents the geometric shape of the data by using the cluster centers and cluster radii (Fig.~\ref{Fig:Coresets}). These are then communicated to the host device for inference. Since clustering better preserves the geometry of the distribution, we observe that inferences with coresets constructed using clustering are more accurate than using importance sampling, and therefore can be preferred over the former whenever there is enough energy.

\vspace{-8pt}



\begin{figure}
\centering 
\includegraphics[width=0.65\linewidth]{figs/43.pdf}
\caption{A toy example of the coreset construction techniques in \emph{Seeker}. Imp-sampling uses a probability based  importance sampling; clustering preserves the geometric shape of the original data. In each case, the points/values in \textcolor{red}{red} are communicated to the host.}
\label{Fig:Coresets}
\end{figure}

\subsection{Communication vs Accuracy}
We can tune the aforementioned coreset construction techniques allow a variable number of features depending on the available energy, i.e. for importance sampling, we can limit the number of points to choose, and similarly, for clustering we can limit both the number of clusters and the number of iterations. 
However, even after preserving important features, the constructed corests are  lossy representation of the original data. Therefore, when performing inference on the compressed coresets representation, the inference accuracy goes down, albeit not significant compared to other lossy compression methods (we can again refer to Table~\ref{tab:compression-table} for the relevant comparisons). 
This leaves an optimization space in trading between communication cost vs. accuracy, i.e. \textit{whether to construct strict and low-volume coresets and lose accuracy or to preserve maximum data points and pay for the communication cost}. We perform an analysis on the MHELATH~\cite{mHealth, mHealthDroid} data set (we take a overlapping moving window of 60 data points sampled at 50Hz from 3 different IMUs, overlap size: 30 data points) to find a trade-off between the coreset size (directly related to the communication cost) and the inference accuracy. Empirically, we observe that accurately preserving the features for each class requires \textbf{20 data points} using importance sampling or \textbf{12 clusters} (see Fig.~\ref{Fig:AAC-Accuracy}) using clustering based techniques. Going above 12 clusters did not significantly improve accuracy. This further motivates us to look for opportunities in the data distribution to improve the compression ratio.

As the DNN models were designed to infer on the full data, we retrain the DNN models to recognize the compressed representation of the data and infer directly from that (both from the importance sampling and clustering). As the coreset formation algorithms are fairly simple~\cite{bachem2015coresets, practicalcoresets, Ting-He-ArXiv, Ting-He-IEEE}, it does not take much latency or energy to convert the raw sensor data into the coreset form even while using a commercial-off-the-shelf micro-controller (like TI MSP430FR5969~\cite{TIEH}). This allows the EH-sensor to opt for coreset formation followed by data communication to the host device as an energy-viable alternative to local DNN inference on the original data. In our example case, transmitting the raw data (60 data points, 32bit floating point data type) needs \textbf{240 Byte}s of data transfer, and with coreset construction and quantization we can limit it to \textbf{36 Bytes} (for 12 clusters, each cluster center is represented by 2 Bytes of data, and radius represented by 1 Byte data), thereby \textit{reducing the data communication volume by 85\%}. The host runs inference on the compressed data to detect the activity (with an accuracy of 76\%). However, due to this reduced accuracy, the sensor only takes this option iff it does not have enough energy to perform the inference at the edge device (either in the 16bit or 12bit variant of the DNN - more details on DNN design is presented in Section~\ref{sec:4}). This raises a question: \textit{is it possible to generate a more \textbf{useful} approximation, via reconstruction, of the data that we lost while forming the coresets?} This problem has not been explored in details, as coresets are typically considered as an $\alpha-$approximate representation of the data ($\alpha$ being the error/approximation parameter)~\cite{bachem2015coresets} and never needed proper recovery. However, thanks to  the low dimensional nature of many sensor data, reconstruction of original data from coresets becomes an essential step.

\subsubsection{Data Memoization:}
Given our focus on ultra low power energy harvesting devices, any opportunities to reduce computation and communication can noticeably augment the performance and efficiency of the entire system. We look into data memoization as one such opportunity. For two instances of the same class, there should be a very high correlation in the sensor data. We empirically measure this by testing for correlation between the sensor signatures of different classes. Conservatively, we choose a correlation coefficient $\geq 0.95$ to predict that the two activities are the same, and hence skip the inference altogether and just communicate only the results to the host. We store ground truth sensor data pattern for all possible labels, and when new data arrives, we find the correlation of the sampled data against the ground truth data, and if any of the correlation coefficient comes out to be $\geq 0.95$, we choose to ignore further inference computation and only communicate the classification result to the host for further processing. Note that choosing the correlation threshold entirely depends on the application and user preference.
 
 \begin{figure}
  \centering
  %\dummyfig{Sensor Setup}
  \includegraphics[clip,width=0.65\linewidth]{figs/seekerDesign.pdf}
  \caption{Overall system design of Seeker}
  \label{Fig:system_design}
%\end{figure}
 \end{figure}


\subsubsection{Recoverable Coreset Construction:}\label{sec:recover} The primary reason the accuracy of inferring on coreset data is lower than that of the original model is the loss of features. Typically, the sensor data are low dimensional, and hence even with a good quality of coreset construction, it is difficult to preserve all the features. However, while inferring at the host, if we are able to recover the data or reconstruct it with minimum error, the accuracy can easily be increased. 

\noindent \textbf{\underline{Clustering Coreset Recovery:}} Clustering preserves the geometry of the original data by representing them as a set of N-spherical clusters represented with a center and a radius. In the process of coreset construction we only preserve the coordinates of the centers and the radii of the clusters, and hence miss the coordinates of the points inside the clusters. However, any random distribution of the lost points in the cluster could provide us with a $2r-$approximate representation of the original distribution (where $r$ is the radius of the cluster; refer Figure~\ref{Fig:clusterRecovery} for a toy example). However, to achieve this, we need some extra information about the clusters. The standard method of clustering-based coreset construction keeps the cluster center and cluster radius, which gives the geometrical shape of the entire data. Extending this with \textbf{the point count for each cluster} allows for reconstruction of data in the original form that can be processed by DNNs trained on full-size data. These reconstructed data sets can be synthesized simply by uniformly distributing the points within each cluster. Although the intra-cluster data distribution will be different from the original, it will still preserve the overall geometry with a certain degree of approximation which the DNN could learn to accommodate.

\begin{figure}
  \centering
  %\dummyfig{Sensor Setup} 
  \includegraphics[width=0.8\linewidth]{figs/17.pdf}
  \caption{Accuracy with different \#clusters (k).}
  \label{Fig:AAC-Accuracy}
  %\vspace{-3mm}
\end{figure}


Experimentally, on the MHELATH dataset, we observe that inferring on the synthesized reconstructions of cluster based coresets can achieve an accuracy of $\approx 85\%$. The reconstruction feature at the host comes with little to no overhead for the host (given the host devices have considerably more compute than the sensor nodes). The addition of the recovery parameter (number of points per cluster) needs \textbf{4 more bits} (in our experiments, we never observe any clusters having more than 16 data points) of data per cluster, bringing the total data communication volume to \textbf{42 Bytes}, which is still a significant $5.7\times$ less in comparison to the original 240 Bytes needed to communicate the raw data in our setup. However, since clustering based coreset construction is more expensive than the importance sampling based coreset construction, it is not always possible to build a recoverable coreset at the edge, unless we figure out a to recover the lost points while we perform importance sampling. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{figure} 
 \centering
    \subfloat[Recovering a cluster with uniform random re-distribution.]
    {%
    %\centering
     \includegraphics[clip,width=0.75\linewidth]{figs/clusterRecovery.pdf}%
    \label{Fig:clusterRecovery}
    }
    \hfill \\
    \subfloat[Recovering a sub-sampling with GAN.]
    {%
    %\centering
     \includegraphics[clip,width=0.75\linewidth]{figs/DPRecovery.pdf}%
    \label{Fig:DPRecovery}
    }
    \hfill
    \caption{Recovering data from the coresets.}
    \label{fig:AllRecovery}  
    %\vspace{-0.5cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\noindent \textbf{\underline{Importance Sampling Coreset Recovery:}} Unlike clustering, when we construct a coreset with importance sampling, we typically have no information regarding the lost data points. We hypothesize that the dropped sample should contain, although not important, sensor specific artifacts. And these artifact must have some pattern, if modeled correctly, could represent the lost data. Towards this, we designed and trained a generative adversarial network (GAN, see Figure~\ref{Fig:DPRecovery} for the structural details) to recover the lost samples of the importance sampling. As training parameters, we provide some statistical parameters (specifically mean and variance) of the signal and random noise to the generator, and the generator generates the lost signals. The discriminator tries to discriminate between the actual data and the synthesized data. We fine-tune the network until the discriminator is fooled sufficiently to distinguish between the original data and the recovered data.   
Considering the fact that we do have access to the sensor data to train the learning algorithm, we can use the same data to train the GAN and with sufficient data, the discriminator could generate the lost signal with minimum error. Our experiments show that the deviation from the original signal, in most cases, is limited to $\le15\%$. However, in some pathological cases, the error at times goes close to $60\%$, and we believe them to be generated artifacts which are common side effects of the GANs\cite{ganerror}.  Our experiments suggests that inference on the GAN recovered signal is almost as good as (about $2\%-4\%$ difference in accuracy) the inference on the recovered cluster signal. The recovery policy can be implemented as a simple generator network in the host. Although, the training of the GAN is complex and involves multiple networks as well as hyper-parameters tuning, the generator network itself is very small (\textit{few hundred thousands} of parameters depending on the sensor data). 

\vspace{-8pt}