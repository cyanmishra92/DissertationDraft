In this section, we provide a background of the current state-of-the-art in performing sensing and computations on EH-WSNs. We also describe the challenges in enabling complex compute on such devices and the need for hardware-software co-design to enable specialized intermittent computing in EH-WSNs. Finally, we define the scope of our work and focus on the problem specifics while alluding to probable solutions. 
Figure~\ref{Fig:EHPrimer} shows the basic building blocks of an energy harvesting sensing/computing unit. The harvested energy is typically stored in either an intermediate storage like a (super) capacitor~\cite{batteryfree}, or used for charging. For building scalable and sustainable infrastructure of battery-free EH-WSNs, the former is more feasible and will be our focus for this work. The fickle nature of harvested energy has posed a major challenge in performing any useful computation, as any useful forward progress gets lost when the traditional computing systems lose power. To tackle this, a significant amount of work has been done on check-pointing, and compiler level tweaks, which help maximize the forward progress on such devices~\cite{incidental, chinchilla, LuciaCheckpoint}. These solutions operate on augmented commercial off the shelf micro-controllers~\cite{TIEH}, or specialized products with traditional architecture~\cite{WISP}, and rely on their efficient prediction of power failure. While these software optimizations and judicious use of persistent storage works for smaller workloads like keyword spotting (e.g \textit{"Ok Google"} detection), they are inefficient for complex workloads (e.g. multi-sensor HAR, predictive maintenance etc.). 

These software-based solutions exhibit inefficiencies with respect to energy and time due to performing multiple save-and-restore cycles~\cite{intelligenceBeyondEdge,ResiRCA}: while some of these operations are necessary, unnecessary checkpoints will also be conservatively performed to ensure forward-progress. Therefore, recent works~\cite{NVPMa, incidental, NVPMicro, spendthrift, ResiRCA} propose the use of a NVP, where the non-volatility of the hardware itself takes care of saving and resuming the program execution. This reduces software overheads and latencies for handling power emergencies and hence can guarantee better QoS for complex and longer tasks even when power is deeply unreliable. Using an NVP and multiple harvested energy sources Qiu et al.~\cite{ResiRCA} demonstrates the possibility of performing complex DNN inference at the EH-Sensor itself. While an NVP ensures safe check-pointing for a given computation, current edge scenarios may require a device to be simultaneously performing multiple functionalities~\cite{iWatchBattery, AppleFall, AppleECG, Google-Assistant-Watch} and might be at energy scarcity. As a result, it is difficult to reliably run these complex tasks standalone on the edge device. Current devices adapt in one of three ways: \emph{1) Send all the sensor data to a connected host device, or cloud, to offload the compute and act only as a sensing and display device; 2) Process data on the device itself, potentially dropping or delaying tasks due to energy shortfalls; 3) A mix of the two models, where some computations do happen on the device while others are offloaded to balance compute, energy, and communication resources}; and typically, the latter is preferred, but it is non-trivial to find the optimal balance between what is to be done on the edge, what to be offloaded~\cite{kang2017neurosurgeon, taylor2018adaptive, zhao2018deepthings, eshratifar2018energy}, and \textit{how to efficiently offload}.


\begin{figure}
\centering
	\subfloat[ High-level overview of an energy harvesting system.]
	{
	\includegraphics[width=0.85\linewidth, height=2.5cm]{figs/EHPrimer.pdf}\label{Fig:EHPrimer}
	}
	\hfill \\
    \subfloat[Current state-of-the-art of EH-WSN.]
    {
    \includegraphics[width=0.65\linewidth]{figs/32.pdf}\label{Fig:EHsota}
    }
    \caption{A primer on energy harvesting systems: Figure~\ref{Fig:EHPrimer} shows the basic building blocks of an EH node equipped with sensing and computation. Some of the units change according to the harvested energy source. Figure~\ref{Fig:EHsota} shows the capabilities of the current SOTA. The size of the circle representing the solutions depicts the compute capabilities of the sensor nodes, the shade shows the available power, and their position on the axes approximates the amount of compute done on the node and the amount of reliability on external communication. The power source is denoted in \textcolor{red}{(Red)} (notations used in Figure~\ref{Fig:EHsota}: COTS: Commercial-off-the-shelf, Bat.: Battery, Bonito~\cite{batteryfree}, Chinchilla~\cite{chinchilla}, ResiRCA~\cite{ResiRCA}, Origin~\cite{Origin})}
    \label{fig:EHIntro}
    %\vspace{-8pt}  
    % \vspace{-0.5cm}
\end{figure}

\noindent \textbf{\underline{Need for Specialized Hardware:~}}One of the major challenges in deploying learning tasks using EH-WSNs is to find the proper hardware platform. The current commercial-of-the-shelf (CotS) hardware capable of performing such compute are not energy efficient to run with all modalities of harvested energy since all of them do not have the same energy income (see Figure~\ref{Fig:EHsota}). For example, there has been significant work on enabling solar powered smart farming~\cite{smartfarmsolar, smartfarmsolar1}, but the same can not be done for smart manufacturing due to the lack of solar exposure and the low fidelity of the available EH sources such as vibration and RF (from WiFi or other sources). To estimate the required energy, we ran simple HAR inferences (optimized version of~\cite{HARHaChoi} for edge deployment using~\cite{netadapt}) on an Adafruit ItsyBitsy nRF52840 Express - Bluetooth LE~\cite{adafruitI} and found it to be consuming from 550mJ to 1.6J of energy (depending on the quantization). Compared to this, body movement and WiFi sources (the possible modalities of harvesting for HAR) harvests in order of milliwatts~\cite{batteryfree,ResiRCA}, making it almost impossible to have a feasible EH-WSN deployment, with the capabilities to perform modest learning tasks, using the CotS. Therefore, there has been a significant body of work~\cite{ResiRCA, Origin,NVPMa,NVPMicro} on developing appropriate next generation hardware (most of them on simulation). Although, we evaluate and show the communication cost savings of \textit{Seeker} on the battery backed CoTS hardware, we propose possible (simulated) hardware accelerator designs to fully deploying a EH-WSN capable of performing inference, compression and communication in harvested energy budget. 

\noindent \textbf{\underline{Complex Compute on EH-WSNs:~}}To quantify the scope performing complex compute using EH-WSNs, we took human activity recognition (HAR) as a workload\footnote{Throughout the paper we evaluate many of our motivation results using HAR as a workload as it is one such application, where the (EH-)WSN, used as body area network, fits perfectly with RF or body movement as the harvesting source. HAR has the nuances of human introduced unpredictability and sensor induced noises. HAR has been pervasive enough given the rise of smart wearables and has been studied well enough to have ample access to resources to make a judicious evaluation. Further, we also evaluate one more emerging application from predictive maintenance domain.}, and performed experiments on the MHEALTH data-set~\cite{mHealth,mHealthDroid} (see Section~\ref{sec:evaluation} for data-set details) using the DNNs proposed in~\cite{HARHaChoi,HARHompel}, an energy harvesting friendly DNN hardware accelerator~\cite{ResiRCA} (to ensure that we are using the state of the art EH-WSN hardware) and recently proposed HAR-specific optimizations for EH systems~\cite{Origin}. Our analysis (see Figure~\ref{Fig:frac-complete}) shows that the state-of-the-art system still only finishes $\approx 58.7\%$ of the inferences scheduled on a sensor. Although accuracy can increase by further tuning duty-cycle, as shown in Figure~\ref{Fig:RR-accuracy}, the returns are diminishing, and indefinite increase of duty cycle is also not an option as that might lead to skipping important data to infer. We observe that the system used in~\cite{Origin} does not aggressively employ quantization, which is a commonly used technique~\cite{drq-isca2020} to reduce both compute and transmission energy in DNN tasks. Our analysis, as shown in Figure~\ref{Fig:quantized-accuracy}, shows accuracy as a function of quantization (we took the approach of performing post training quantization and fine-tuned the DNN to work with reduced bit precision instead of training the DNN from scratch with a reduced precision). The quantized DNNs benefit from lower compute and memory footprints, but need specialized fine-tuning and often suffer from lower accuracy. Similarly, other  approximation-via-data-reduction techniques, such as sub-sampling, did not perform inference with a desirable accuracy. \textit{Collectively, the aforementioned figures demonstrate that the harvested energy budget is insufficient to perform {\em all} inferences with acceptable accuracy on currently proposed EH-WSN systems.} Therefore, to complete all the scheduled computations, and thereby to improve accuracy, the system must rely on another device (e.g. a mobile phone), where sufficient resources are available to complete any remaining inference, \textit{if the data can be sent from the sensor}. Said coordinating device completes the rest of the computations and finally, aggregates them with the ones completed in the sensor nodes. The challenge here is to \textit{send the data efficiently}, since communication is an expensive task and especially challenging for EH-WSNs~\cite{batteryfree} thanks to their fickle and ultra-low energy budget. The obvious solution is to reduce the communication data volume by compressing the data before transmitting. This also reduces energy footprint and the probability of data packet loss.


\begin{table}
\centering
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Algorithm}    & \textbf{Compression Ratio} & \textbf{Accuracy Loss (\%)} \\ \hline
Fourier Decomposition & 3 - 5                      & 9.1 - 18.3                  \\ \hline
DCT                  & 3 - 5                      & 5.8 - 16.2                  \\ \hline
DWT                 & 3 - 6                      & 5.3 - 12.7                 \\ \hline
Coreset               & 3 - 10                     & 0.02 - 0.76                 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Accuracy trade-off of different compression techniques: Low-dimensional data loses important features under lossy compression, dropping inference accuracy significantly compared to the original data. Details on Coreset are available on Section~\ref{sec:4}. (Notations used: DCT: Discrete Cosine Transform, DWT: Discrete Wavelet Transform.}
\label{tab:compression-table}
\end{table}
 
\begin{figure}
\centering
	\subfloat[Completion with ERR]
	{
	\includegraphics[width=0.45\linewidth, height=2.5cm]{figs/2.pdf}\label{Fig:frac-complete}
	}
	\hfill
    \subfloat[Accuracy of ERR]
    {
    \includegraphics[width=0.45\linewidth, height=2.5cm]{figs/3.pdf}\label{Fig:RR-accuracy}
    }
    \hfill \\
    \subfloat[Accuracy vs quantizations]
    {
    \includegraphics[width=0.45\linewidth, height=2.5cm]{figs/5.pdf}\label{Fig:quantized-accuracy}
    }
    \hfill
    \subfloat[Accuracy vs sub-sampling]
    {
    \includegraphics[width=0.45\linewidth, height=2.5cm]{figs/4.pdf}\label{Fig:subsampled-accuracy}
    }
    % \vspace{-0.1cm}
    \caption{Accuracy comparison of various classical node-level optimization techniques. The Extended-Round-Robin policy (ERR)~\cite{Origin} takes a store-and-execute approach, and the number associated represents the ratio of store cycles vs execute cycles (e.g. RR3 is 3 store cycles followed by 1 execute cycle). The 'Baseline' model is a fully powered system with no energy restrictions, and the quantized model runs on harvested energy using a RR12 policy.}
    \label{fig:Projection}
    %\vspace{-8pt}  
    % \vspace{-0.5cm}
\end{figure}

\noindent \textbf{\underline{Challenges with Data Compression:}~} Using standard compression algorithms, like discrete cosine transform, discrete wavelet transform, and Fourier decomposition etc., to minimize the communication overhead is not a viable solution~\cite{compression-marcelloni2008simple}. This is partly because we need a very high compression ratio with very low power. Secondly, these compression algorithms are not context-aware, and hence lose relevant features during the process of compression resulting in degraded inference accuracy (refer Table~\ref{tab:compression-table} for details). A key insight is that, while these compression techniques work very well for high dimensional data (e.g. images), inference on low-dimensional sensor data (such as inertial measurement unit or IMU vibration data) is much more sensitive to lossy compression as separating between features might be difficult to do. And we will not achieve a sufficient compression ratio from lossless approaches either. Therefore, the standard data compression techniques are not very useful, let alone their energy efficient (such as quantized versions~\cite{quantcompression}) counterparts. For data compression in EH-WSNs, we need the compression algorithm to be \circled{1} \textbf{light weight} (for energy efficiency), \circled{2} \textbf{feature preserving} (for higher accuracy), \circled{3} \textbf{having a high compression ratio} (for communication efficiency), and \circled{4} \textbf{context agnostic} (for better generalization); i.e., our deployment scenario demands a \textit{smaller representative form} of the data that still \textit{preserves enough  application-specific features to perform meaningful classifications in a given DNN}.


\noindent \textbf{\underline{Why Coresets?}}
The aforementioned requirements motivate us to consider \emph{coresets}  for forming representations of the original data. Coresets, primarily used in computational geometry~\cite{bachem2015coresets},  have been recently used~\cite{Ting-He-ArXiv, Ting-He-IEEE} for machine learning and sensor networks. Since coresets were designed to preserve the geometry of the data, we believe that they can be crafted to preserve features, and therefore be useful for performing accurate inference in subsequent stages, and thereby satisfying \circled{2}. Furthermore, constructing coresets do not need any application information, i.e. they are application/data agnostic and can represent any form of data (IMU~\cite{Ting-He-ArXiv}, Image~\cite{6907021}, DNN feature map~\cite{coresetDNN, coresets-compression-ECCV,coresets-traning-GMM,coresets-traning-pmlr}). This fulfils requirement \circled{4}. They are also an effective way to construct a representation of the data set with high compression ratios~\cite{practicalcoresets,coresets-compression-ECCV} without incurring unacceptable accuracy losses and thus useful for achieving \circled{3}. For the DNNs in question, coresets can achieve sufficient compression ratios to make communication energy-competitive with computation, as well as opening up new opportunities for optimizing DNN inference on the coreset, rather than original data.  Finally, most of the coreset construction algorithms are simple (hence can achieve \circled{1}) and do not need complex operations (like cosine, exponential, etc.~\cite{bachem2015coresets,practicalcoresets,Ting-He-ArXiv,Ting-He-IEEE}, they can also be quantized~\cite{Ting-He-IEEE} to further reduce their computation and memory footprints. Motivated by this, we explore possibilities of designing an efficient synergistic sensor-host ecosystem (involving the EH-WSNs and host), where we try to maximize the compute at the sensor nodes, yet for the incomplete tasks, we use coresets to compress and send the data to the host where the rest of the computations could occur. 
