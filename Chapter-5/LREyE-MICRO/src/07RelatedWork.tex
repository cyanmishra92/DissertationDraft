% \section{Related Work} 
% Processing-in-memory (PIM) architectures have gained prominence for accelerating deep neural networks, especially using analog or resistive memory devices to perform in-situ computation. Early analog PIM designs like ISAAC \cite{Shafiee16} and PRIME \cite{Chi16} demonstrated the feasibility of using ReRAM crossbar arrays to execute neural network inference by performing analog matrix-vector multiplications directly in memory. ISAAC introduced an accelerator that stores weights in memristive crossbars and executes CNN layers with in-situ analog dot products, relying on high-precision ADCs after each crossbar to handle multi-bit outputs. PRIME explored embedding neural computation within ReRAM-based main memory, showing that a portion of memory arrays could be repurposed for neural computing to reduce data movement. These and other mixed-signal accelerators confirmed that analog crossbars can dramatically improve energy efficiency for DNN inference. Another notable direction is RedEye \cite{LiKamWa16}, an image sensor architecture that pushes early convolutional processing into the analog domain. RedEye performs initial ConvNet layers analogously at the sensor focal plane before quantization, thereby alleviating the costly analog-to-digital conversion at the imager output. All of the above efforts, however, constrain the analog computation to essentially one layer at a time (one crossbar operation followed by digitization). Traditional non-linear activations (e.g., ReLU) are implemented outside the crossbar in the digital domain, which forces intermediate analog results to be converted to digital between layers. This limitation means prior analog PIM systems still incurred per-layer conversion overheads and could not truly cascade multiple neural layers entirely in analog. Some works have looked at extending PIM to training or deeper pipelines (e.g., PipeLayer \cite{Song17HPCA} supports weight updates in ReRAM and a pipelined flow), but they similarly stop short of enabling consecutive analog layers with native analog non-linearities. 

% In tandem with architectural advances, researchers have developed hardware-aware training techniques to cope with analog non-idealities. Deep networks mapped to analog substrates can suffer accuracy loss from device variability, IR drop across crossbar interconnects, sense amplifier noise, quantization effects, and other analog imperfections. To mitigate this, prior works incorporate circuit effects into the training process so that the model learns to compensate for hardware errors. For example, Song et al. propose an “imperfection-tolerable” training algorithm for ReRAM-based accelerators that models device non-idealities (e.g., conductance variations, IR drop) during backpropagation to preserve accuracy on crossbar hardware \cite{Song21TCAD}. More broadly, several studies inject noise and quantization errors into training or retraining phases to immunize the network against analog variability at inference time. Such co-design approaches significantly improve the robustness of analog accelerators, but earlier implementations still assumed the conventional layer-by-layer processing structure with analog computing limited to the MAC operations. In contrast to these efforts, our work tightly couples model and hardware design to enable a deeper analog execution pipeline. We introduce a Schottky diode-based analog activation function at the crossbar outputs, allowing two consecutive layers to be computed entirely in the analog domain without intermediate digitization. This innovation builds on the insights of hardware-aware learning: during training, we explicitly model crossbar IR drops, diode I–V characteristics shifts, amplifier noise, and quantization in the loop so that the network adapts to the analog hardware’s imperfections. By co-designing the DNN (architecture and training procedure) with the circuit primitives, our approach achieves what prior PIM systems could not: an analog accelerator that evaluates multiple neural layers back-to-back analogly, significantly reducing costly A/D conversions while maintaining accuracy. This combination of a novel analog activation mechanism and integrated analog-aware training distinguishes our work from prior PIM solutions in both architecture and methodology.

\section{Related Work}

Processing-in-memory (PIM) architectures leveraging analog or resistive memories have demonstrated substantial energy savings for DNN inference by performing matrix–vector multiplications in situ. Pioneering designs such as ISAAC~\cite{shafiee2016isaac} and PRIME~\cite{chi2016prime} store weights in ReRAM crossbars and rely on ADCs after each array to digitize partial sums, while RedEye~\cite{RedEye2020} embeds early convolutional layers into the image sensor to reduce conversion overhead. These systems, however, execute only one analog layer per crossbar cycle and implement non‑linear activations (e.g., ReLU) in the digital domain, incurring frequent A/D–D/A conversions. Extensions like PipeLayer~\cite{song2017pipelayer} support training and pipelined flows but still cannot cascade multiple analog layers with native non‑linearities.

Complementing architectural advances, hardware‑aware training methods inject analog non‑idealities—device variability, IR drop, noise, and quantization—into the training loop so that networks learn to compensate for circuit errors~\cite{xu2022multi}. While such co‑design improves robustness, prior work assumes a layer‑by‑layer pipeline with analog MACs only. In contrast, we propose a Schottky diode–based analog activation at each crossbar output and a two‑layer analog compute pipeline, eliminating intermediate digitization. By modeling crossbar IR drop, diode I–V shifts, amplifier errors, and quantization during training, our approach tightly fuses model and hardware design, enabling back‑to‑back analog execution and significantly reducing conversion overheads without sacrificing accuracy.