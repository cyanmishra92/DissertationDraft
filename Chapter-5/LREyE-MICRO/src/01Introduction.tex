% %%%%% %%%%% %%%%%
% The proliferation of Internet of Things (IoT) devices in diverse and challenging environments—such as wildlife monitoring~\cite{tuia2022perspectives,gobieski2019intelligence}, deep mine surveillance~\cite{bai2020real, ni2024detection}, and remote industrial automation~\cite{visconti2024machine, khalil2021deep}—demands efficient and reliable image classification~\cite{RedEye2020, qiu2020resirca}. These applications often operate under stringent energy constraints, requiring ultra-low power consumption, long device lifetimes, and minimal maintenance~\cite{qiu2020resirca, gobieski2019intelligence}. For example, wildlife monitoring sensors must often run for years without battery replacements, and deep-mine surveillance systems face hostile environments where frequent maintenance is infeasible. Energy-harvesting technologies are increasingly being leveraged to meet these requirements, enabling devices to operate indefinitely, but intermittently~\cite{Lv2022, qiu2020resirca, ma2017incidental}.  In such contexts, traditional image classification solutions fall short due to their significant energy overhead~\cite{RedEye2020, IMBNature}, necessitating novel paradigms for energy-efficient computing.

% Amongst these approaches, Processing-in-Memory (PIM) architectures have emerged as a transformative solution, 
% %for these constraints
% with Resistive RAM (ReRAM)--based crossbars (xBars) demonstrating particular promise, especially for DNN-based inference serving~\cite{shafiee2016isaac}. xBars excel in their ability to perform matrix-vector multiplications (MVMs) directly in the analog domain, drastically reducing the energy overhead of data movement and digital processing~\cite{reramNature, chi2016prime, shafiee2016isaac, singh2020nebula}. A typical image inference pipeline using ReRAM crossbars is depicted in Figure~\ref{fig:IntroDiagram}. Data from image sensors are stored in the memory (often an eDRAM)~\cite{singh2020nebula, shafiee2016isaac, chi2016prime} and part of the multiply-accumulate functionality of the DNN execution is performed in the crossbar. 
% Finally, the activation functions and data-flow orchestration are typically managed by an microcontroller unit (MCU) or a dedicated control block.

% Furthermore, the non-volatility and endurance of ReRAM make it particularly suitable for low-power scenarios, where reliability over extended lifetimes is paramount~\cite{chen2020reram, rizk2019demystifying}. 
% In addition, compatibility with existing CMOS fabrication processes further enhances the commercial viability of ReRAM-based systems~\cite{IMBNature}.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/RichIntroDiagram.pdf}
%     \vspace{-22pt}\caption {An example wildlife classification deployment - Analog image sensor data is stored in digital memory, then  processed in crossbar arrays in the mixed domain. The MCU, in the digital domain, performs activation functions. Every DNN layer undergoes expensive domain transfers.}
%     \label{fig:IntroDiagram}
% \end{figure}


% Despite these advantages, significant challenges remain in deploying ReRAM-based systems for ultra low-power and energy-harvesting environments. A critical limitation is the energy and latency overheads associated with analog-to-digital (A2D) and digital-to-analog (D2A) conversions~\cite{shafiee2016isaac, singh2020nebula}. As depicted in Figures~\ref{fig:IntroDiagram} and~\ref{fig:xbarFlow}, in a typical image inference flow, the matrix-vector multiplication (MVM) computations happen in the crossbar arrays operating in the analog domain, while the activation functions, control management, and memory management are performed in the digital domain~\cite{shafiee2016isaac, singh2020nebula, chi2016prime, qiu2020resirca}. After performing MVM, each layer in a DNN typically needs to execute a nonlinear activation function, particularly the rectified linear unit (ReLU)~\cite{lecun1998gradient, hahnloser2000digital, hahnloser2000correction}, which is indispensable for modern neural networks~\cite{howard2017mobilenets}. Although activation computations are generally inexpensive, they are typically done off-crossbar and require A2D conversion~\cite{shafiee2016isaac}, leading to frequent A2D and D2A conversions. These conversions can consume up to $82\%$ of the power and $31\%$ of the area~\cite{shafiee2016isaac, chi2016prime, IMBNature}, severely undermining the efficiency gains of the PIM architecture. Traditional designs often offload these activation computations to external digital processors, such as microcontroller units (MCUs)~\cite{singh2020nebula, qiu2020resirca} or specialized digital circuits~\cite{IMBNature}, which not only incurs additional energy and latency costs but also undermines the core advantage of PIM architectures by requiring frequent data movement between analog crossbars and digital processing units. For instance, each off-crossbar activation processing step can add a latency overhead of $15\%$ to $18\%$ and contribute approximately $5\%$ to the total energy budget in ultra-low-power systems~\cite{shafiee2016isaac, chi2016prime, IMBNature}.

% Another significant issue is the degradation of computational accuracy due to IR drops and other analog noise/variability in large-scale crossbar arrays. Since the crossbar computation is entirely based on Ohmic properties, the voltage drop across stages causes an uneven distribution of voltage within the crossbar. This leads to reduced fidelity in analog computations, with accuracy drops of up to $25\%$ reported in certain configurations~\cite{IRDropDAC, IRDropTCAD, lee2024mitigating}. 
% Mitigation strategies, such as current balancing and the inclusion of intermediate storage registers, exacerbate energy consumption, often increasing the overall power budget~\cite{crafton2022characterization, IRDropICCD}. Furthermore, traditional neural network models are often designed without consideration of the non-idealities inherent in analog hardware. Hardware-induced noise, variability in device characteristics, and diverse operating conditions can lead to significant performance degradation, particularly in scenarios with tight energy budgets and low data fidelity~\cite{lee2024mitigating, IRtrainingBL}. For example, analog noise can introduce errors that degrade classification accuracy by $15\%$ if not properly accounted for in the network design~\cite{singh2020nebula}.

% To address these challenges, we advocate for the co-optimization of model architectures and hardware configurations, a strategy that remains underexplored. Current designs often fail to consider how tile sizes, data representations, and hardware resources interact to impact overall energy efficiency and performance in low-power xBars. Although recent work has explored adaptive tiling strategies~\cite{qiu2020resirca}, they lack joint optimization of neural networks and PIM architectures. Such co-design strategies are critical for unlocking the full potential of ReRAM-based accelerators in ultra-low-power and energy-harvesting systems.
% In this paper, we address these challenges by proposing a novel framework that integrates \emph{hardware-aware neural network design} and \emph{energy-efficient activation strategies}. Our approach bridges the gap between hardware limitations and neural network requirements, delivering practical solutions for deploying ReRAM-based systems in energy-constrained environments. Our experimental results demonstrate up to \textbf{68\%} improvement in energy efficiency and \textbf{25\%} reduction in latency, allowing robust image classification for applications such as wildlife monitoring and industrial automation under strict energy constraints. The \textbf{key contributions} of this paper are as follows:

% \noindent$\bullet$ \textbf{Analog Activation with Schottky Diodes:} We present a novel analog activation function implemented using Schottky diodes, which offer low voltage drop, rapid switching and minimal leakage. This eliminates the need for off-chip or off-crossbar digital activation processing, reducing energy consumption and latency while ensuring seamless compatibility with analog crossbar operations.

% \noindent$\bullet$ \textbf{Hardware-Aware Training:} Our training pipeline incorporates real-world hardware non-idealities to enhance robustness and efficiency. We introduce an activation function that models Schottky diode behavior in the analog domain, enabling the neural network to adapt to hardware constraints during training. We also design a loss function that accounts for IR drop and hardware noise profiles, resulting in models resilient to voltage fluctuations and analog noise—critical factors in ultra low-power applications.

% \noindent$\bullet$ \textbf{Model-Hardware Co-Design:} We adopt a co-design strategy that simultaneously optimizes the DNN and hardware architecture for superior energy efficiency and performance. Key innovations include a tile-based architecture that allows parallel analog computations within crossbar arrays and the ability to compute two consecutive neural network layers entirely in the analog domain. Additionally, during training, techniques such as tile dropout are employed to boost model robustness against real-world challenges like incomplete or noisy data.

% \noindent$\bullet$ \textbf{Comprehensive Evaluation:} We conduct extensive evaluations to validate the proposed system. Our results demonstrate significant improvements over state-of-the-art methods, achieving up to \textbf{68\%} energy savings and \textbf{25\%} latency reductions while maintaining high classification accuracy. Specifically, on benchmark datasets, our analog-aware trained models maintain high classification accuracy, achieving up to \textbf{1.63\%} improvement over 4-bit quantized models. In a case study using the NTLNP wildlife image dataset~\cite{ntlnpdataset, tan2022animal, ntlnprepo}, our system achieves an accuracy of \textbf{88.1\%} with MicroNet, outperforming the 4-bit baseline by \textbf{4.9\%} and RedEye by \textbf{6.06\%}, highlighting the versatility and practicality of our solution. \textcolor{blue}{We further expand our evaluations on modern transformer based architectures to show generalization of the new activation function.}

% \textcolor{blue}{To the best of our knowledge, this is the first work to integrate hardware-aware training, 
% an analog activation circuit based on Schottky diodes, 
% and a comprehensive model--hardware co-design \emph{specifically} for \textbf{ReRAM}-based PIM architectures.}
% By bridging the gap between neural network design and analog hardware constraints, our approach enables robust and energy-efficient image classification tailored for ultra-low-power and edge systems.
% %%%%% %%%%% %%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \section{Introduction}
\label{sec:intro}

The rapid proliferation of Internet of Things (IoT) devices in diverse and often hostile environments—such as wildlife monitoring~\cite{tuia2022perspectives,gobieski2019intelligence}, deep mine surveillance~\cite{bai2020real, ni2024detection}, and remote industrial automation~\cite{visconti2024machine, khalil2021deep}—demands \emph{efficient} and \emph{reliable} image classification solutions~\cite{RedEye2020, qiu2020resirca}. These applications characteristically operate under stringent energy constraints, where ultra-low power consumption, extended device lifetimes, and minimal maintenance are paramount~\cite{qiu2020resirca, gobieski2019intelligence}. For instance, wildlife monitoring sensors may need to run for years in remote habitats without battery replacements, while deep-mine surveillance nodes must withstand harsh conditions that render frequent maintenance impractical. To address these limitations, energy-harvesting technologies are increasingly employed, enabling devices to operate \emph{indefinitely} but often \emph{intermittently}~\cite{Lv2022, qiu2020resirca, ma2017incidental}. Under such conditions, traditional image classification approaches—usually reliant on power-hungry digital processing—fail to meet the rigorous energy constraints~\cite{RedEye2020, IMBNature}, thus demanding novel paradigms for \emph{energy-efficient computing}.

One promising line of research for achieving such efficiency targets is \emph{Processing-in-Memory} (PIM) architectures, particularly those that leverage \emph{Resistive RAM} (ReRAM) crossbars (xBars) for DNN-based inference~\cite{shafiee2016isaac}. Unlike standard digital designs, ReRAM xBars perform matrix-vector multiplications (MVMs) \emph{directly} in the analog domain, significantly reducing the energy overhead of data movement and digital processing~\cite{reramNature, chi2016prime, shafiee2016isaac, singh2020nebula}. Figure~\ref{fig:IntroDiagram} illustrates a typical image inference pipeline wherein data from image sensors are stored in memory (often eDRAM)~\cite{singh2020nebula, shafiee2016isaac, chi2016prime}, and a portion of the multiply-accumulate operations is offloaded to analog crossbars. Subsequently, activation functions and data-flow orchestration are conventionally managed by a microcontroller unit (MCU) or a dedicated control block. Further strengthening the prospects of ReRAM, its non-volatility and high endurance make it particularly suited for long-term reliability in low-power scenarios~\cite{chen2020reram, rizk2019demystifying}, and its compatibility with existing CMOS fabrication processes elevates its commercial viability~\cite{IMBNature}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/RichIntroDiagram.pdf}
    \vspace{-22pt}
    \caption{An example wildlife classification deployment---analog image sensor data is stored in digital memory, then processed in crossbar arrays in the mixed domain. The MCU, in the digital domain, performs activation functions. Every DNN layer undergoes expensive domain transfers.}
    \label{fig:IntroDiagram}
\end{figure}

Despite these notable advantages, significant hurdles remain in deploying ReRAM-based PIM systems for ultra low-power and intermittently powered settings. A primary challenge is the steep energy and latency overhead of analog-to-digital (A2D) and digital-to-analog (D2A) conversions~\cite{shafiee2016isaac, singh2020nebula}. As depicted in Figures~\ref{fig:IntroDiagram} and~\ref{fig:BaseArch}\textit{(I)}, while crossbar arrays compute MVM in the analog domain, crucial tasks such as activation functions, memory management, and control logic typically occur in the digital domain~\cite{shafiee2016isaac, singh2020nebula, chi2016prime, qiu2020resirca}. Modern DNNs rely heavily on nonlinear activations, particularly rectified linear units (ReLUs)~\cite{lecun1998gradient, hahnloser2000digital, hahnloser2000correction}, for boosting network performance~\cite{howard2017mobilenets}. Although the activation calculations themselves are often computationally light, the off-crossbar requirement for digital processing triggers frequent A2D and D2A conversions~\cite{shafiee2016isaac}, which can consume up to $82\%$ of the total power and occupy $31\%$ of the die area~\cite{shafiee2016isaac, chi2016prime, IMBNature}. Typical solutions offload these computations to external MCUs~\cite{singh2020nebula, qiu2020resirca} or specialized digital hardware~\cite{IMBNature}, but this offloading undercuts the central advantage of PIM—eliminating most data movement—and further inflates energy and latency. Notably, each off-crossbar activation step can add $15\%$ to $18\%$ latency overhead and approximately $5\%$ extra energy budget in ultra-low-power implementations~\cite{shafiee2016isaac, chi2016prime, IMBNature}.

Moreover, computational accuracy in analog crossbars suffers from IR drops and device-level variations inherent to large-scale arrays. Because crossbar computations hinge on Ohmic properties, voltage drop across rows and columns leads to uneven signal distribution, culminating in accuracy losses of up to $25\%$ in certain setups~\cite{IRDropDAC, IRDropTCAD, lee2024mitigating}. Existing mitigation techniques—ranging from current balancing to insertion of intermediate storage registers—amplify energy usage and degrade overall efficiency~\cite{crafton2022characterization, IRDropICCD}. Complicating matters, mainstream DNN architectures are typically designed with digital hardware assumptions, disregarding analog noise, device variability, and constrained precision in crossbars~\cite{lee2024mitigating, IRtrainingBL}. Such mismatch can introduce classification errors exceeding $15\%$~\cite{singh2020nebula} if models are not adapted to the underlying analog realities—an unacceptable performance drop for mission-critical or resource-starved applications.

To surmount these obstacles, we underscore the importance of jointly optimizing model architectures and hardware configurations. Few existing solutions fully account for how tile sizes, data representations, and analog hardware constraints interplay to affect energy efficiency and accuracy, particularly in ultra-low-power ReRAM designs~\cite{qiu2020resirca}. Although adaptive tiling has been explored, most lack comprehensive co-design of both the neural network and PIM architecture. Bridging this gap is paramount for extracting the full performance and energy gains that ReRAM crossbars can offer in edge deployments.

In this paper, we propose a novel framework that integrates \emph{hardware-aware neural network design} with \emph{energy-efficient activation strategies}, delivering a holistic solution for deploying ReRAM-based systems under tight energy budgets. Our approach confronts the dual constraints of analog non-idealities and constrained power budgets, achieving \textbf{up to 68\%} improvement in energy efficiency and \textbf{25\%} reduction in latency. These gains enable robust, continuous image classification for domains like wildlife monitoring and industrial automation, even under intermittent or harvested power. The \textbf{key contributions} of this paper include:

\begin{itemize}[leftmargin=*]
    \item \textbf{Analog Activation with Schottky Diodes:} We propose a novel analog activation function employing Schottky diodes, which feature low forward voltage drop, rapid switching speeds, and negligible leakage. This design obviates the need for off-chip or off-crossbar digital activations, cutting energy consumption and latency while preserving seamless compatibility with analog crossbar operations.
    
    \item \textbf{Hardware-Aware Training:} We build a training pipeline that explicitly incorporates real-world hardware non-idealities. By modeling Schottky diode dynamics and IR drop behavior in the activation function, we enable the DNN to adapt to analog-domain constraints. We further incorporate hardware noise profiles into the loss function, ensuring that trained models remain robust against voltage fluctuations and noise—pivotal in ultra low-power applications.
    
    \item \textbf{Model-Hardware Co-Design:} Our approach enforces a co-design paradigm wherein DNN architectures and the underlying ReRAM xBar hardware are optimized in tandem. We introduce a tile-based design to facilitate parallel analog MVMs across crossbar arrays, and exploit the potential to compute two consecutive network layers entirely in the analog domain. 
    %Furthermore, we apply \emph{tile dropout} techniques during training to bolster the model’s resilience against partial or noisy data, a common occurrence in energy-scarce environments.
    
    \item \textbf{Comprehensive Evaluation:} We conduct extensive validations to gauge the effectiveness of our framework against state-of-the-art references, observing up to \textbf{68\%} energy savings and \textbf{25\%} latency reductions, all while preserving high classification accuracy. Specifically, across standard benchmarks, our analog-aware models yield up to \textbf{1.63\%} higher accuracy compared to 4-bit quantized networks. Furthermore, on the NTLNP wildlife image dataset~\cite{ntlnpdataset, tan2022animal, ntlnprepo}, our method achieves an \textbf{88.1\%} accuracy using MicroNet, surpassing the 4-bit baseline by \textbf{4.9\%} and outperforming RedEye by \textbf{6.06\%}. We also extend our evaluation to modern transformer-based architectures, underscoring our activation function’s broad applicability.
\end{itemize}

To the best of our knowledge, this is the first comprehensive approach to integrate hardware-aware training, a Schottky diode-based analog activation circuit, and a model--hardware co-design specifically tailored to \textbf{ReRAM}-based PIM accelerators. By reconciling neural network design with the realities of analog crossbar hardware, our strategy achieves \emph{robust, energy-efficient} image classification suitable for ultra-low-power and edge deployments.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Introduction}
% \label{sec:intro}

% The rapid expansion of Internet of Things (IoT) devices in diverse, often harsh environments—such as wildlife monitoring~\cite{tuia2022perspectives,gobieski2019intelligence}, deep-mine surveillance~\cite{bai2020real, ni2024detection}, and remote industrial automation~\cite{visconti2024machine, khalil2021deep}—has created a pressing need for highly \emph{energy-efficient} and \emph{robust} image classification solutions~\cite{RedEye2020, qiu2020resirca}. These deployment scenarios typically demand ultra-low-power consumption and minimal maintenance over extended timeframes~\cite{gobieski2019intelligence, qiu2020resirca}, with devices often operating on tightly constrained power budgets or intermittent energy-harvesting schemes~\cite{Lv2022, qiu2020resirca, ma2017incidental}. For instance, wildlife monitoring sensors in remote forests must run continuously for years with limited battery replacements, and deep-mine surveillance systems may have to withstand extreme temperatures, physical obstructions, and hazardous conditions with infrequent recharges.

% In pursuit of these stringent requirements, \emph{Processing-in-Memory} (PIM) architectures based on \emph{Resistive RAM} (ReRAM) crossbars (xBars) have emerged as a transformative solution for deep neural network (DNN) inference~\cite{shafiee2016isaac, chi2016prime, singh2020nebula}. ReRAM xBars directly execute matrix-vector multiplications (MVMs) in the analog domain, mitigating energy overheads associated with data movement and memory accesses~\cite{reramNature, singh2020nebula}. Their non-volatile nature and compatibility with modern CMOS processes further bolster their suitability for embedded and edge AI~\cite{chen2020reram, rizk2019demystifying, IMBNature}. Figure~\ref{fig:IntroDiagram} depicts a typical ReRAM-based image inference pipeline in which analog operations (e.g., partial multiply-accumulate computations) occur inside crossbar arrays, while a microcontroller or specialized digital block handles activation functions and orchestrates the data flow.

% Despite these advantages, realizing \emph{fully analog inference} at ultra-low-power faces three critical challenges. First, frequent analog-to-digital (A2D) and digital-to-analog (D2A) conversions—required each time intermediate results or activations move between analog and digital domains—can consume up to $82\%$ of the overall power and $31\%$ of the area in PIM accelerators~\cite{shafiee2016isaac, chi2016prime, IMBNature}. Second, large-scale crossbar arrays suffer from IR drops and inherent hardware noise, triggering notable accuracy degradation if the network is not tailored to these analog non-idealities~\cite{IRDropDAC, IRDropTCAD, lee2024mitigating, IRDropICCD}. Third, many prior approaches lack a holistic co-design framework that \emph{simultaneously} optimizes both the DNN model and the underlying PIM hardware, missing opportunities to leverage tile-level parallelism, reduce conversion overheads, and incorporate real device parameters into training~\cite{qiu2020resirca, singh2020nebula}.

% In this paper, we tackle these gaps by introducing a comprehensive framework that unifies \emph{hardware-aware neural network training} with \emph{energy-efficient analog activation} in ReRAM xBars. Specifically, we replace the conventional digital off-chip activation with a Schottky-diode-based analog activation module, dramatically reducing cross-domain conversions. We further strengthen our \emph{co-design methodology} to account for hardware noise, IR drops, and intermittent power scenarios, and we experimentally validate our approach on both classic network architectures and emerging transformer-based designs. Our key contributions are as follows:

% \begin{itemize}[leftmargin=*]
%     \item \textbf{Analog Activation via Schottky Diodes:} We propose a novel analog activation circuit leveraging Schottky diodes for low forward voltage drop, rapid switching, and minimal leakage current. By embedding the non-linear activation directly in the analog domain, our design avoids repeated A2D/D2A conversions, substantially cutting energy and latency costs.
%     %
%     \item \textbf{Hardware-Aware Training:} We incorporate Schottky diode characteristics and crossbar-specific non-idealities (e.g., IR drop, device variability, and noise) into the training pipeline. The resulting networks are \emph{analog-aware}—they learn to maintain high classification accuracy under the voltage and signal distortions endemic to ReRAM crossbars, which is vital in ultra-low-power deployments.
%     %
%     \item \textbf{Model--Hardware Co-Design:} Beyond analog activations, we outline a tile-based crossbar architecture that processes two consecutive DNN layers fully in analog, reducing back-and-forth domain transfers. Techniques like \emph{tile dropout} bolster resilience to partial node failures or incomplete powering in energy-harvesting systems. This joint optimization of model topology and hardware resources closes the gap between analog capabilities and DNN requirements.
%     %
%     \item \textbf{Comprehensive Evaluation:} We extensively evaluate our framework on multiple benchmarks, including wildlife images~\cite{ntlnpdataset, tan2022animal, ntlnprepo} and transformer-based models. Our analog-aware networks achieve up to \textbf{68\%} energy savings and \textbf{25\%} latency reductions compared to state-of-the-art baselines, while maintaining—and in some cases improving—top-1 accuracy by up to \textbf{4.9\%} over a 4-bit quantized baseline in real-world scenarios.
% \end{itemize}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/RichIntroDiagram.pdf}
%     \caption{An example wildlife classification deployment. Analog image sensor data are stored in on-chip memory, partially processed via ReRAM crossbars, and then passed to an MCU for data orchestration. In typical designs, frequent cross-domain transfers for activation functions incur significant overheads. By embedding analog activations, our approach reduces these energy and latency bottlenecks.}
%     \label{fig:IntroDiagram}
% \end{figure}

% \noindent \textbf{Significance.} To the best of our knowledge, this is the first work to \emph{jointly} introduce a Schottky-diode-based analog activation design, a robust hardware-aware training flow, and a holistic model--hardware co-design specifically tailored for ReRAM-based PIM accelerators. By aligning the network’s structure with real-world device behavior, our solution not only maximizes energy efficiency but also maintains accuracy under power-scarce and noisy conditions, thus paving the way for next-generation ultra-low-power IoT deployments.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%