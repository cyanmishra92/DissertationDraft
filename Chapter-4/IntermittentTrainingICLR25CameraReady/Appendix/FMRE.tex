\subsection{Feature Map Reconstruction Error Dropout with QuantaTask Optimization}
\label{appendix:FMRE}
Feature Map Reconstruction Error Dropout leverages the reconstruction error of feature maps to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems.

\textbf{Mathematical Formulation:}
Let \(\mathbf{W}\) be the weight matrix of a layer and \(\mathbf{F}\) be the feature maps produced by the layer. The reconstruction error of a feature map \(F_i\) is calculated as:
\[
\text{RE}_i = \|\mathbf{F}_i - \hat{\mathbf{F}}_i\|_2
\]
where \(\hat{\mathbf{F}}_i\) is the reconstructed feature map, and \(\|\cdot\|_2\) denotes the L2 norm.

Define the dropout probability \(p_i\) for neuron \(i\) based on the reconstruction error of its corresponding feature map. The idea is to use the reconstruction error to determine the probability:
\[
p_i = \frac{\gamma \, \text{RE}_i}{\max(\text{RE}) + \epsilon}
\]
where \(\gamma\) is a scaling factor to adjust the overall dropout rate, and \(\epsilon\) is a small constant to avoid division by zero.

Define a binary dropout mask \(\mathbf{m} = [m_1, m_2, \ldots, m_n]\) where \(m_i \in \{0, 1\}\). Each element of the mask is determined by sampling from a Bernoulli distribution with probability \(1 - p_i\):
\[
m_i \sim \text{Bernoulli}(1 - p_i)
\]

Apply the dropout mask during the forward pass. Let \(\mathbf{a}_i\) denote the activation of neuron \(i\):
\[
\mathbf{a}_i^{\text{dropout}} = \mathbf{a}_i \cdot m_i
\]

\textbf{Training with Feature Map Reconstruction Error Dropout and QuantaTask Optimization:}
Initialize the network parameters \(\mathbf{W}\), dropout mask \(\mathbf{m}\), and scaling factor \(\gamma\). Define the energy budget \(E_b\) for a single quanta and for the entire inference. Initialize the loop iteration parameters \(l\).


Compute the activations \(\mathbf{a}\) and apply the dropout mask:
\[
\mathbf{a}_i^{\text{dropout}} = \mathbf{a}_i \cdot m_i
\]

Compute the loss \(\mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}})\) where \(\mathbf{Y}\) is the output of the network and \(\mathbf{\hat{Y}}\) is the target output.

Calculate the gradients of the loss with respect to the weights:
\[
\frac{\partial \mathcal{L}}{\partial W_{ij}}
\]

For each layer \(L\) and loop \(i\) within the layer, estimate the energy \(E_i\) required for the current quanta size \(l_i\):
\[
E_i \gets \text{DynAgent.estimateEnergy}(L, i, l_i)
\]
If \(E_i > E_b\), fuse tasks to reduce the overhead:
\[
\text{FuseTasks}(L, i, l_i, E_b)
\]
Update \(E_i\) after task fusion:
\[
E_i \gets \text{DynAgent.estimateEnergy}(L, i, l_i)
\]

Update the dropout mask \(\mathbf{m}\) based on the reconstruction error of the feature maps:
\[
p_i = \frac{\gamma \, \text{RE}_i}{\max(\text{RE}) + \epsilon}
\]
\[
m_i = 
\begin{cases} 
0 & \text{if } \text{Bernoulli}(1 - p_i) = 0 \\
1 & \text{otherwise}
\end{cases}
\]

Perform the backward pass to update the network weights, considering the dropout mask:
\[
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{W}} \odot \mathbf{m}
\]
where \(\eta\) is the learning rate and \(\odot\) denotes element-wise multiplication.

\textbf{Inference with Feature Map Reconstruction Error Dropout and QuantaTask Optimization:}
Check the available energy using DynAgent.
If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy.
Perform the forward pass with the updated dropout mask to obtain the output \(\mathbf{Y}\).
This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the reconstruction error of the feature maps, along with the QuantaTask optimization to handle energy constraints.