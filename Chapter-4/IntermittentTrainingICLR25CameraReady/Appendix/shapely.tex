\subsection{Neuron Shapley Value Dropout with QuantaTask Optimization}
\label{appendix:shapely}
Neuron Shapley Value Dropout applies the concept of Shapley values from game theory (Aas et al., 2021) to assess neuron importance for dropout, combined with the QuantaTask optimization to handle energy constraints in intermittent systems.

\textbf{Mathematical Formulation:}
The Shapley value \(\phi_i\) of neuron \(i\) is a measure of its contribution to the overall network performance. It is calculated by considering all possible subsets of neurons and computing the marginal contribution of neuron \(i\) to the network's output:
\[
\phi_i = \frac{1}{|\mathcal{N}|!} \sum_{S \subseteq \mathcal{N} \setminus \{i\}} \frac{|S|! (|\mathcal{N}| - |S| - 1)!}{|\mathcal{N}|} \left[ \mathcal{L}(S \cup \{i\}) - \mathcal{L}(S) \right]
\]
where \(\mathcal{N}\) is the set of all neurons, \(S\) is a subset of neurons not containing \(i\), and \(\mathcal{L}(\cdot)\) denotes the loss function.

Define the dropout probability \(p_i\) for neuron \(i\) based on its Shapley value. Neurons with lower Shapley values are more likely to be dropped:
\[
p_i = \frac{\delta}{\phi_i + \epsilon}
\]
where \(\delta\) is a scaling factor to adjust the overall dropout rate, and \(\epsilon\) is a small constant to avoid division by zero.

Define a binary dropout mask \(\mathbf{m} = [m_1, m_2, \ldots, m_n]\) where \(m_i \in \{0, 1\}\). Each element of the mask is determined by sampling from a Bernoulli distribution with probability \(1 - p_i\):
\[
m_i \sim \text{Bernoulli}(1 - p_i)
\]

Apply the dropout mask during the forward pass. Let \(\mathbf{a}_i\) denote the activation of neuron \(i\):
\[
\mathbf{a}_i^{\text{dropout}} = \mathbf{a}_i \cdot m_i
\]

\textbf{Training with Neuron Shapley Value Dropout and QuantaTask Optimization:}
Initialize the network parameters \(\mathbf{W}\), dropout mask \(\mathbf{m}\), and scaling factor \(\delta\). Define the energy budget \(E_b\) for a single quanta and for the entire inference. Initialize the loop iteration parameters \(l\).

Compute the activations \(\mathbf{a}\) and apply the dropout mask:
\[
\mathbf{a}_i^{\text{dropout}} = \mathbf{a}_i \cdot m_i
\]

Compute the loss \(\mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}})\) where \(\mathbf{Y}\) is the output of the network and \(\mathbf{\hat{Y}}\) is the target output.

Calculate the Shapley values \(\phi_i\) for each neuron based on their contribution to the network's performance.

For each layer \(L\) and loop \(i\) within the layer, estimate the energy \(E_i\) required for the current quanta size \(l_i\):
\[
E_i \gets \text{DynAgent.estimateEnergy}(L, i, l_i)
\]
If \(E_i > E_b\), fuse tasks to reduce the overhead:
\[
\text{FuseTasks}(L, i, l_i, E_b)
\]
Update \(E_i\) after task fusion:
\[
E_i \gets \text{DynAgent.estimateEnergy}(L, i, l_i)
\]

Update the dropout mask \(\mathbf{m}\) based on the Shapley values:
\[
p_i = \frac{\delta}{\phi_i + \epsilon}
\]
\[
m_i = 
\begin{cases} 
0 & \text{if } \text{Bernoulli}(1 - p_i) = 0 \\
1 & \text{otherwise}
\end{cases}
\]

Perform the backward pass to update the network weights, considering the dropout mask:
\[
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{W}} \odot \mathbf{m}
\]
where \(\eta\) is the learning rate and \(\odot\) denotes element-wise multiplication.

\textbf{Inference with Neuron Shapley Value Dropout and QuantaTask Optimization:}
Check the available energy using DynAgent.
If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy.
Perform the forward pass with the updated dropout mask to obtain the output \(\mathbf{Y}\).
This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the Shapley values of the neurons, along with the QuantaTask optimization to handle energy constraints.
