To address the issues with ``intermittency-aware'' DNN training, we propose NExUME: (\textbf{N}eural \textbf{Ex}ecution \textbf{U}nder Inter\textbf{M}ittent \textbf{E}nvironment). NExUME has 3 different components: (1) Intermittency- and platform-aware neural architecture search (DynNAS); (2) Intermittency- and platform-aware DNN training with dynamic dropouts and quantization (DynFit); and finally, (3) Intermittency and platform aware task scheduling (DynInfer). Each of these components could work individually towards optimizing DNNs for intermittent environment. However, the combination of all of them is expected to provide the best results. To search for the best architecture for the given intermiitent enironment, DynNAS uses the work proposed by iNAS~\cite{intermittentNASs}. After the network is decided DynFit is used to train the network with the intermittency where as DynInfer is used to perform inference under intermittency. In this section,  we elaborate on these two different components.

%\subsection{Introduction to DynFit}
\subsection{DynFit: Intermittency-Aware Learning}
\textbf{DynFit} designed to optimize deep neural networks (DNNs) for execution in environments characterized by intermittent power supply due to energy harvesting. The primary goal of DynFit is to adapt the DNN's architecture and training process to operate efficiently under unpredictable energy budgets while maintaining acceptable accuracy and adhering to predefined service level objectives (SLOs).

DynFit introduces key mechanisms to dynamically adjust computational complexity based on energy availability, thereby enabling energy-efficient execution of DNN models in constrained environments. These mechanisms include (i) Dynamic Dropout, which adjusts the dropout rates based on available energy to reduce computational load; (ii) Dynamic Quantization, which modifies quantization levels in response to energy constraints to save energy; and (iii) QuantaTasks, which define atomic computational units that can be executed without interruption given the current energy budget. This section elaborates on the formulation of DynFit by precisely defining QuantaTasks, modeling energy consumption, formulating the optimization problem, and proposing an algorithm to solve it effectively.

\textbf{Precise Definition of QuantaTasks:} A \textit{QuantaTask} is defined as the smallest atomic unit of computation that can be executed entirely without interruption under the current energy and hardware constraints. Each QuantaTask ensures that execution proceeds without partial computation, which would otherwise lead to overhead from checkpointing and potential data corruption. The main properties of QuantaTasks are atomicity and respect for energy constraints. Specifically, each QuantaTask \( q \) is mathematically defined as \( q = (\ell_q, E_q) \), where \( \ell_q \in \mathbb{N} \) represents the size of the task, measured as the number of computational operations or loop iterations, and \( E_q \in \mathbb{R}^+ \) represents the estimated energy required to execute the task \( q \) without interruption.

The atomicity property ensures that each QuantaTask must be completed fully once started. Mathematically, this is expressed as \( E_q \leq E_b \), where \( E_b \in \mathbb{R}^+ \) denotes the available energy budget at the time of execution. This constraint guarantees that the energy required for each QuantaTask does not exceed the available energy to avoid mid-execution power failures.

\textbf{Modeling Energy Consumption:} The energy consumption of DNN operations is modeled based on empirical profiling data from the hardware platform. Let \( e_{\text{op}} \) denote the energy consumed per computational operation. This value may vary depending on the type of operation, such as addition or multiplication, and the data precision used. The total energy consumption of a QuantaTask \( q \) can thus be modeled as \( E_q = e_{\text{op}} \times \ell_q \), where \( \ell_q \) is the number of operations in the task, and \( e_{\text{op}} \) is the average energy consumption per operation. For more precise modeling, \( e_{\text{op}} \) can be a function of the operation type, data precision (due to quantization), and hardware-specific characteristics. The available energy budget \( E_b \) is determined by the current energy stored in the systemâ€™s capacitor or battery, and can also take into account predicted energy harvesting in the near future if predictive models are utilized.

\textbf{Optimization Variables, Constraints, and Objective Function:} The optimization problem is formulated with several variables: the weights of the DNN (\( \mathbf{W} \)), the dropout rates (\( \mathbf{d} \)), the quantization levels (\( \mathbf{q} \)), and the QuantaTask sizes (\( \boldsymbol{\ell} \)). These variables are defined as follows: (i) \( \mathbf{W} = \{w_1, w_2, \dots, w_P\} \), where \( P \) is the total number of weights in the network; (ii) \( \mathbf{d} = \{d_1, d_2, \dots, d_N\} \), where \( N \) is the total number of neurons (or layers, depending on the granularity) and \( d_i \in [0, 1] \) represents the dropout rate for neuron \( i \); (iii) \( \mathbf{q} = \{q_1, q_2, \dots, q_M\} \), where \( M \) is the total number of layers or parameters to quantize, \( q_j \in \mathcal{Q} \) represents the quantization level (bit-width) for parameter \( j \), and \( \mathcal{Q} = \{4, 8, 12, 16\} \) is the set of allowable quantization bit-widths; and (iv) \( \boldsymbol{\ell} = \{\ell_1, \ell_2, \dots, \ell_K\} \), where \( K \) is the total number of QuantaTasks and \( \ell_k \in \mathbb{N} \) represents the size of QuantaTask \( k \).

The objective is to minimize the total loss, which includes the prediction loss and regularization terms that penalize energy consumption. The objective function is expressed as follows:

\[
\min_{\mathbf{W}, \mathbf{d}, \mathbf{q}, \boldsymbol{\ell}} \quad \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}}) + \lambda_1 \sum_{j=1}^{M} c_q(q_j) + \lambda_2 \sum_{i=1}^{N} c_d(d_i)
\]

where \( \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}}) \) denotes the prediction loss (e.g., cross-entropy loss), \( \mathbf{Y} \) represents model predictions, and \( \mathbf{\hat{Y}} \) are the ground truth labels. The functions \( c_q(q_j) = \alpha_q \times q_j \) and \( c_d(d_i) = \alpha_d \times d_i \) are the cost functions associated with the quantization level and dropout rate, respectively. The hyperparameters \( \lambda_1 \) and \( \lambda_2 \) balance the regularization terms with prediction accuracy.

\textbf{Formulation of the Composite Optimization Problem:} The composite optimization problem, combining the energy constraints and objective function, is formulated as follows:

\[
\begin{aligned}
\min_{\mathbf{W}, \mathbf{d}, \mathbf{q}, \boldsymbol{\ell}} \quad & \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}}) + \lambda_1 \sum_{j=1}^{M} \alpha_q q_j + \lambda_2 \sum_{i=1}^{N} \alpha_d d_i \\
\text{subject to} \quad & \sum_{k=1}^{K} e_{\text{op}} \ell_k \leq E_b \\
& e_{\text{op}} \ell_k \leq E_b, \quad \forall k \\
& q_j \in \mathcal{Q}, \quad \forall j \\
& 0 \leq d_i \leq d_{\max}, \quad \forall i \\
& \ell_k \in \mathbb{N}, \quad \forall k
\end{aligned}
\]

This problem formulation minimizes the total loss while ensuring that energy consumption does not exceed the available budget and that all variables are within their allowable ranges.

\textbf{Solution Approach and Algorithm Design:} The optimization problem is non-convex due to the non-linear and discrete nature of quantization levels, the dependence of the loss function on dropout rates and quantization, and the discrete variables representing QuantaTask sizes. To address this, we employ an alternating optimization strategy, where subsets of variables are iteratively optimized while keeping others fixed. The process iterates over four main steps: (i) optimizing weights \( \mathbf{W} \) using Stochastic Gradient Descent (SGD), (ii) adjusting dropout rates \( \mathbf{d} \) using projected gradient descent, (iii) selecting quantization levels \( \mathbf{q} \) through exhaustive evaluation, and (iv) determining optimal QuantaTask sizes \( \boldsymbol{\ell} \) based on energy constraints.


\subsection{Fine-tuning for Regularization and Prevention of Overfitting}
The DynFit component of the NExUME framework leverages dynamic dropout and quantization during training to adapt deep neural networks (DNNs) to the challenges of intermittent energy availability. While these techniques are effective in reducing energy consumption and computational load, they introduce specific challenges in the training process. Primarily, dynamic dropout and quantization can lead to uneven weight updates and increase the risk of overfitting. Uneven weight updates occur when certain neurons and their associated weights are dropped more frequently due to higher dropout rates, resulting in under-trained weights. Meanwhile, weights that are consistently updated may overfit to the training data, reducing the model's generalization ability. Addressing these issues is crucial for maintaining the overall performance of the network.

Our objective to design a fine-tuning and regularization strategy that ensures adequate training for all weights, mitigates overfitting, and enhances the model's ability to generalize across varying energy availability scenarios. This strategy addresses the specific challenges introduced by dynamic dropout and quantization during training, providing a comprehensive solution for balancing training effectiveness and energy efficiency.

Dynamic dropout and quantization, while necessary for energy adaptation, create unique effects on the model's training dynamics. Dynamic dropout rates \( d_i \) vary with the available energy \( E_b \). Under low energy conditions, high dropout rates can result in frequent omission of specific neurons, leading to inadequate updates for their associated weights. Similarly, dynamic quantization levels \( q_j \) adjust according to the energy budget, where lower bit-widths reduce the representational capacity of some weights. Consequently, weights corresponding to frequently dropped neurons or low bit-width quantization levels receive fewer updates and become under-trained. On the other hand, weights associated with neurons that are seldom dropped or quantized at higher bit-widths may dominate the learning process, resulting in overfitting.

\textbf{Formulating the Regularization Strategy:} To address these issues, we propose an adaptive regularization and fine-tuning strategy. The strategy monitors the update frequencies of weights during training, identifies under-trained and overfitting weights based on their update frequencies, and applies targeted fine-tuning to under-trained weights while implementing regularization techniques for overfitting weights. 

We begin by defining an update indicator function \( U_p(t) \) for weight \( w_p \) at iteration \( t \):

\[
U_p(t) = \begin{cases}
1, & \text{if } w_p \text{ is updated at iteration } t \\
0, & \text{otherwise}
\end{cases}
\]

The update frequency \( F_p \) over \( T \) total iterations is then calculated as:

\[
F_p = \frac{1}{T} \sum_{t=1}^{T} U_p(t)
\]

where \( F_p \in [0,1] \) represents the fraction of iterations where \( w_p \) was updated. Based on the update frequencies, we categorize weights into under-trained and overfitting groups using two thresholds. Under-trained weights are defined as those with an update frequency below a threshold \( \theta_{\text{low}} \), i.e.,

\[
\mathcal{W}_{\text{under}} = \{ w_p \mid F_p < \theta_{\text{low}} \}
\]

Similarly, overfitting weights are defined as those with an update frequency above a threshold \( \theta_{\text{high}} \), i.e.,

\[
\mathcal{W}_{\text{overfit}} = \{ w_p \mid F_p > \theta_{\text{high}} \}
\]

\textbf{Designing the Fine-tuning and Regularization Process:} The fine-tuning process focuses on ensuring that under-trained weights receive adequate updates, while the regularization process aims to prevent overfitting in frequently updated weights. To fine-tune under-trained weights, we first freeze all well-trained weights (i.e., weights not in \( \mathcal{W}_{\text{under}} \)) temporarily during fine-tuning. We then adjust the dropout and quantization settings for the neurons associated with under-trained weights by setting \( d_i = 0 \) and using the maximum quantization bit-width \( q_j = q_{\max} \) to enhance precision. This targeted fine-tuning continues with a reduced learning rate \( \eta_{\text{ft}} \) to optimize the loss function:

\[
\min_{\mathbf{W}_{\text{under}}} \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}})
\]

For overfitting weights, we implement L2 regularization by adding a penalty term to the loss function:

\[
\mathcal{L}_{\text{reg}} = \lambda_{\text{L2}} \sum_{w_p \in \mathcal{W}_{\text{overfit}}} w_p^2
\]

The total loss function then becomes:

\[
\mathcal{L}_{\text{total}} = \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}}) + \mathcal{L}_{\text{reg}}
\]

Additionally, we increase the dropout rates \( d_i \) for neurons associated with overfitting weights within acceptable limits to encourage regularization. Adjustments in learning rates are made based on the categorization: under-trained weights are updated with a slightly higher learning rate, while overfitting weights are updated with a reduced learning rate to promote stability.

\textbf{Formulating the Fine-tuning Optimization Problem:} The fine-tuning optimization problem for under-trained weights is formulated as:

\[
\min_{\mathbf{W}_{\text{under}}} \quad \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}})
\]

For overfitting weights, the objective is defined as:

\[
\min_{\mathbf{W}_{\text{overfit}}} \quad \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}}) + \lambda_{\text{L2}} \sum_{w_p \in \mathcal{W}_{\text{overfit}}} w_p^2
\]

with constraints on dropout rates and quantization levels. For under-trained weights, we set \( d_i = 0 \) and \( q_j = q_{\max} \). For overfitting weights, we increase \( d_i \) but ensure \( d_i \leq d_{\max} \), while maintaining standard quantization levels.

\textbf{Bringing It All Together:} The fine-tuning and regularization strategy is executed through a systematic procedure. First, weights are categorized based on their update frequencies into under-trained and overfitting groups. For under-trained weights, the algorithm temporarily freezes other weights and fine-tunes the identified weights with adjusted dropout and quantization settings. For overfitting weights, the algorithm applies L2 regularization and increases dropout rates to discourage overfitting. The learning rates are adjusted accordingly to prevent drastic changes and promote stability in the optimization process. This iterative fine-tuning loop continues until the validation loss stops improving or a maximum number of epochs is reached, ensuring a balanced and generalized model that performs well across varying energy availability scenarios. If necessary, all weights are unfrozen for a final round of standard training with adjusted learning rates and regularization parameters to fine-tune the entire network.


\subsection{DynInfer - Inference with Intermittent Power}
DynInfer is designed to optimize the inference phase of deep neural networks (DNNs) operating in environments with intermittent power supply due to energy harvesting. Unlike traditional systems with stable power, intermittent environments pose unique challenges for executing inference tasks efficiently and reliably. To address these challenges, DynInfer introduces a novel set of mechanisms, including real-time task scheduling, task decomposition and prioritization, and checkpointing and recovery. These mechanisms work in tandem to ensure robust inference performance even under fluctuating energy conditions.

The primary objective of DynInfer is to achieve the highest possible accuracy and responsiveness within the given energy constraints while ensuring that high-priority tasks are completed without interruption and meeting service level objectives (SLOs). To this end, DynInfer utilizes dynamic scheduling strategies that adapt to energy availability and task deadlines, prioritizing tasks based on criticality and energy requirements. Additionally, DynInfer employs a checkpointing mechanism to preserve computational progress across power failures, enabling reliable execution of inference tasks.

The problem addressed by DynInfer is characterized by three main objectives: maximizing inference performance, ensuring task completion, and adhering to service level objectives (SLOs). Achieving these objectives is challenging due to the inherent variability in energy availability caused by intermittent energy harvesting. Additionally, tasks must be executed atomically to produce valid results, and the limited computational resources necessitate efficient utilization. These challenges require a sophisticated scheduling strategy that considers energy variability, task atomicity, and computational constraints.

The inference process is represented as a set of tasks \( \mathcal{T} = \{ T_1, T_2, \dots, T_N \} \), where each task \( T_i \) is characterized by its energy requirement \( E_i \), execution time \( \tau_i \), priority \( p_i \), deadline \( D_i \), and criticality level \( c_i \). At any given time \( t \), the available energy is denoted as \( E_b(t) \), and the energy profile can be predicted as \( E_b(t + \Delta t) \). The scheduling problem is formulated using variables such as start time \( s_i \) and completion time \( f_i = s_i + \tau_i \). A binary decision variable \( x_i \in \{0,1\} \) indicates whether a task is scheduled (\( x_i = 1 \)) or not (\( x_i = 0 \)).

The scheduling problem is governed by a set of constraints that ensure energy efficiency and task feasibility. The energy constraint guarantees that, at any time \( t \), the total energy consumed by running tasks does not exceed the available energy:

\[
\sum_{i: s_i \leq t < f_i} E_i \leq E_b(t)
\]

The deadline constraint ensures that tasks are completed before their deadlines:

\[
f_i \leq D_i, \quad \forall i \text{ such that } x_i = 1
\]

Additionally, the task atomicity constraint mandates that once a task starts, it must run to completion without interruption. If tasks cannot run in parallel, a non-overlapping constraint is enforced:

\[
[s_i, f_i) \cap [s_j, f_j) = \emptyset, \quad \forall i \neq j \text{ where } x_i = x_j = 1
\]

Limited computational resources also impose constraints on task execution:

\[
\sum_{i: s_i \leq t < f_i} r_i \leq R_{\text{max}}
\]

where \( r_i \) denotes the resource requirement of task \( T_i \) and \( R_{\text{max}} \) is the maximum available resources. The objective is to maximize the total weighted priority of scheduled tasks:

\[
\max_{\{ x_i, s_i \}} \quad \sum_{i=1}^{N} p_i x_i
\]

Alternatively, a more detailed objective function can be considered, balancing priority, energy efficiency, and adherence to deadlines:

\[
\max_{\{ x_i, s_i \}} \quad \sum_{i=1}^{N} \left( p_i - \alpha E_i - \beta (f_i - D_i)^+ \right) x_i
\]

where \( \alpha \) and \( \beta \) are weighting factors for energy consumption and deadline violation penalties, and \( (f_i - D_i)^+ = \max(0, f_i - D_i) \) is the penalty for missing deadlines.

Inference operations are decomposed into smaller tasks at various granularities, such as layer-level, operation-level, or block-level tasks. Each task \( T_i \) has an energy requirement \( E_i = e_{\text{op}} \times n_{\text{op}, i} \), where \( e_{\text{op}} \) is the average energy per operation, and \( n_{\text{op}, i} \) is the number of operations in task \( T_i \). The execution time of task \( T_i \) is similarly defined as \( \tau_i = t_{\text{op}} \times n_{\text{op}, i} \), where \( t_{\text{op}} \) is the average time per operation.

Energy profiling and execution time profiling can be conducted using DynAgent to obtain accurate estimates for energy consumption and execution times for various operations on the target hardware.

\textbf{Scheduling Problem Formulation:} The scheduling problem is formulated with decision variables \( s_i \) (task start times) and binary variables \( x_i \in \{0,1\} \) (indicating whether a task is scheduled). The energy availability constraint over time is expressed as:

\[
\int_{0}^{t} \sum_{i: s_i \leq \tau < f_i} P_i d\tau \leq \int_{0}^{t} P_b(\tau) d\tau
\]

where \( P_i \) is the power consumption of task \( T_i \) and \( P_b(t) \) is the power availability over time. In practice, this constraint can be simplified by discretizing time into slots of length \( \Delta t \) and applying an energy constraint per time slot:

\[
\sum_{i: t_k \in [s_i, f_i)} E_i \leq E_b(t_k)
\]

Task precedence constraints ensure that some tasks are executed only after the completion of others:

\[
s_j \geq f_i, \quad \text{if } T_j \text{ depends on } T_i
\]

The full optimization problem can now be expressed as:

\[
\begin{aligned}
\max_{\{ x_i, s_i \}} \quad & \sum_{i=1}^{N} p_i x_i \\
\text{subject to} \quad & \sum_{i: t_k \in [s_i, f_i)} E_i \leq E_b(t_k), \quad \forall t_k \\
& f_i = s_i + \tau_i, \quad \forall i \\
& f_i \leq D_i, \quad \forall i \text{ where } x_i = 1 \\
& s_j \geq f_i, \quad \text{if } T_j \text{ depends on } T_i \\
& x_i \in \{0,1\}, \quad \forall i \\
& s_i \geq 0, \quad \forall i
\end{aligned}
\]

The complexity of this scheduling problem is analogous to the Knapsack Problem and Job Shop Scheduling, making it NP-hard. As a result, exact solutions may be computationally infeasible for large \( N \) in real-time scenarios. To overcome this, we propose heuristic algorithms such as greedy scheduling, dynamic priority adjustment, and energy-aware earliest deadline first (EA-EDF). These algorithms prioritize tasks based on a priority metric, schedule tasks as early as possible, and adjust priorities in real-time based on energy availability and deadlines.

Our proposed algorithm, Energy-Aware Priority Scheduling, initializes by obtaining the set of tasks \( \mathcal{T} \) and their parameters, and setting the current time \( t = 0 \). At each scheduling step, the available energy is updated, and schedulable tasks are identified. An effective priority \( P_i^{\text{eff}} \) is computed for each candidate task, considering priority, energy consumption, and deadline urgency:

\[
P_i^{\text{eff}} = \frac{p_i}{E_i} \times \phi_i
\]

where \( \phi_i \) is a factor accounting for deadline urgency and criticality. The task with the highest \( P_i^{\text{eff}} \) that satisfies all constraints is selected for scheduling. If no task can be scheduled, the system waits until energy is harvested or tasks become schedulable.

\textbf{Integration with Checkpointing Mechanisms:} DynInfer is designed to handle power failures through checkpointing. Before executing each task, the system state is saved to non-volatile memory (NVM). In the event of a power failure, the system can resume from the last checkpoint. To minimize checkpointing overhead, task sizes are adjusted to balance between checkpoint frequency and the risk of power failure, and incremental checkpointing is used to save only changes since the last checkpoint. Upon power restoration, the system retrieves the last checkpointed state, assesses energy availability, and reschedules tasks based on the current energy and task deadlines.

