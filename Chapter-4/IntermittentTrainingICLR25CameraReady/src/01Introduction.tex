%%%%%
The increasing demand for ubiquitous, sustainable and energy-efficient computing combined with the maturation of energy harvesting systems has spurred a plethora of recent research and advancements~\cite{intelligencebeyondedge, mouse, origin, wisp, MIT} in the direction of battery-less devices.  Such platforms represent the future of the Internet of Things (IoT), and energy harvesting wireless sensor networks (EH-WSNs): equipped with modern machine learning (ML) techniques, these devices can revolutionize computing, monitoring and analytics in remote, risky and critical environments such as oil wells, mines, deep forests, oceans, remote industries and smart cities. However, the intermittent and limited energy income of these deployments demand optimizations for the ML applications starting from the algorithm~\cite{eap, tianyi,intermittentNAS}, orchestration~\cite{chinchilla, origin}, compilation~\cite{alpaca}, and hardware development~\cite{resirca, islam2022enabling, usas} layers. Despite these advancements, achieving consistent and accurate inference -- thereby meeting service level objectives (SLOs) -- in such intermittent environments remains a significant challenge. This difficulty is exacerbated by unpredictable resources, form-factor limitations, and variable computational availability, particularly when employing task-optimized deep neural networks (DNNs).


There are two major problems with performing DNN inference with intermittent power. \textbf{(I.) Energy Variability}: Even though  DNNs can be tailored to match the average energy income of the energy harvesting  (EH) source through pruning, quantization, distillation or network architecture search (NAS)~\cite{netadapt, eap, intermittentNAS}, there is no guarantee that the energy income consistently meets or exceeds this average. When the income falls below the threshold, the system halts the inference, and checkpoints the intermediate states (via software or persistent hardware)~\cite{chinchilla, resirca}, and resumes upon energy recovery. Depending on the EH profile, this might lead to significant delays and SLO violations. \textbf{(II.) Computational Approximation}: To address (I) and maintain continuous operation, EH-WSNs may skip some of the compute during energy shortfalls, by dropping neurons (zero padding) or by approximating (quantization). Adding further approximation to save energy atop an already heavily reduced network can propagate errors through the layers leading to significant accuracy drop~\cite{Zygarde, moreisless, DFI, kang},  further violating SLOs. 

In certain energy critical scenarios, even EH-WSNs applying state of the art techniques fail to consistently meet SLOs, at times skipping entire inferences to deliver at least some results on time. Fundamentally, while current DNNs can be trained or fine-tuned to fit within a given resource budget -- be it compute, memory or energy -- they are {\em not} trained to expect a variable or intermittent resource income. Although intermittency-aware NAS~\cite{intermittentNAS} could alleviate certain problems, it still falls into the first bucket many times. This calls for revisiting the entire training process, i.e., NAS is not the sole approach given the resource intermittency and we need to train the DNN in such a way that it is aware of the intermittency and {\em adapts} to it. 
runtime.

Motivated by these challenges, we propose \textbf{NExUME} (\textbf{N}eural \textbf{Ex}ecution \textbf{U}nder Inter\textbf{M}ittent \textbf{E}nvironment), a novel learning method designed specifically for environments with intermittent power and EH-WSNs, with potential applications in any ultra-low-power inference system. NExUME incorporates energy variability aware network architecture search (NAS) conducted across multiple commercial off-the-shelf IoT devices using a dataset of EH traces. This approach enables the identification of the optimal network architecture tailored to the specific constraints of a given hardware platform and resource budget. Recognizing that oracular knowledge of energy availability in intermittent environments is impractical, NExUME \emph{\textbf{learns}} to dynamically adapt the DNN's inference computations based on the current energy profile. This adaptation involves an innovative strategy of learning an instantaneous  energy-aware dynamic dropout and quantization selection. The method also includes targeted fine-tuning -- both during and after training -- that not only regularizes the model but also prevents over-fitting and enhances the robustness of the network to fluctuations in resource availability. The {\bf key contributions} of this paper can be summarized as follows: 
 


\begin{itemize} [leftmargin=*]
\itemsep-0.2em 
    % \item \textbf{DynAgent:} A dynamic ``co-optimization manager'' that coordinates network search and dynamic training processes, adapting to varying environmental, platform, model-specific, and SLO requirements. DynAgent, serving as a repository of EH traces and device configurations, significantly enhances the flexibility and efficiency of resource utilization across different operational contexts.
    \item \textbf{DynFit:} A novel ``training optimizer'' that embeds energy variability awareness directly into the DNN training process. This optimizer allows for ``dynamic adjustments'' of dropout rates and quantization levels based on real-time energy availability, thus maintaining learning stability and improving model accuracy under power constraints.
    \item \textbf{DynInfer:} An intermittency- and platform-aware ``task scheduler'' that optimizes computational tasks for intermittent power supply, ensuring consistent and reliable DNN operation. DynInfer takes advantage of software-compiler-hardware codesign to manage, and deploy tasks. With the help of DynFit, DynInfer provides $6\%$ -- $22\%$ accuracy improvements with $\le 5\%$ additional compute. 
    \item \textbf{Dataset:} A first-of-its-kind machine status monitoring dataset, which involves mounting multiple types of EH sensors at various locations on a Bridgeport machine to monitor its activity status. 
\end{itemize}



%%%%%

