%%%%%%

To address the issues with ``intermittency-aware'' DNN training, we propose NExUME: (\textbf{N}eural \textbf{Ex}ecution \textbf{U}nder Inter\textbf{M}ittent \textbf{E}nvironment). NExUME has 3 different components: (1) Intermittency- and platform-aware neural architecture search (DynNAS+); (2) Intermittency- and platform-aware DNN training with dynamic dropouts and quantization (DynFit); and finally, (3) Intermittency and platform aware task scheduling (DynInfer). Each of these components could work individually towards optimizing DNNs for intermittent environment. However, the combination of all of them is expected to provide the best results. Another major component of NExUME is the Resource Model (DynAgent) -- which works like an agent to understand the environment (energy income and consumption) and platform  (compute, memory, and activation constraints) as well as the model. In this section,  we elaborate on the different components of NExUME  and their roles. 


\subsection{DynAgent: Intermittency Aware Resource Model}
\label{sec:DynAgent}
One of the first step to design an intermittency aware DNN execution is to understand the execution framework. This includes: 1. \textbf{EH environment}: the type, fidelity and magnitude of EH; 2. \textbf{Hardware platform}: accessing the compute capabilities, parallelization, memory, elasticity and energy consumption; 3. \textbf{SLOs}: determining acceptable accuracy, latency and the allowable slack for these metrics. This model aids in determining the constraints on the network design and hyper-parameter tuning. Towards this, we introduce the \textit{Intermittency-Aware Resource Model} (\emph{DynAgent}). DynAgent serves as an environment manager for the NExUME framework. That is, given the EH environment and hardware platform, DynAgent interfaces with DynNAS+ and DynFit (section~\ref{sec:DynFit}) to give an estimation of instantaneous resource availability, hardware configuration and algorithmic framework, to facilitate intermittency aware NAS and training.  DynAgent is essentially a repository of multiple EH traces (photovoltaic power with outdoor sun, photovoltaic power with indoor lighting, RF power with WiFi in a home setting, RF power with WiFi in an office setting, thermal power with home HVAC, piezoelectric power from the vibration of industrial machines), hardware platforms (TI MSP430FR5994~\cite{ti_msp430fr5994}, Arduino Nano~\cite{arduino_nano33_ble_sense}, Adafruit TensorFlow kit~\cite{adafruit_tensorflow_kit}), and available computation libraries. DynAgent uses micro-profiling ($\mu-profile$) with predetermined ordered access along with size and stride sweep to understand the capabilities of hardware platform, i.e., to know the cache sizes, cache access latencies, memory access latency, etc. DynAgent implements a sweep of memory accesses to better profile the hardware and know its limitations. Although many of the device specifications are typically well known, the $\mu-profile$ helps us understand the capabilities of a new hardware tailored for the target applications. Similarly, to understand the latency of kernel execution, DynAgent has a  $\mu-profile$ of different DNN kernels (GeMM, Matrix Vector Multiplication, HadamardProduct2D, conv1D, conv2D, DepthWiseSeparableConv2D, etc.) with their execution latencies. Note that, all of these kernels are built with loop decomposition for checkpointing under intermittent execution (refer to Appendix~\ref{appendix:intermittentGeMM} for an example of such a for GeMM with loop decomposition and checkpointing).   

Along with the database of the hardware profile, DynAgent also includes an ``accelerator library''  targeted for CNN applications. Most modern micro-controllers are equipped with digital signal processing (DSP) libraries, and hardware~\cite{ti_msp_lea} which mostly include multiply accumulation function (MADD) and 1D convolution (conv1D) functions (typically implemented as inline assembly). Although these libraries suffice for 1D time-series data (audio, accelerometer, etc.), they lack the functions like 2D convolution (Conv2D), depth-wise separable convolution (DWSConv2D), etc. To bridge this gap, DynAgent implements the CNN-specific functions using the available primitives for most efficient execution.\footnote{Note that, it is possible to implement these functions without relying on libraries, but doing so generally results in a significant increase in energy and memory consumption, which is impractical for intermittent systems.}  Implementing Conv2D using conv1D is straightforward, and involves treating each row or column of the input matrix as a separate 1D input. On the other hand, depth-wise separable convolution, which involves a separate filter for each input channel followed by a 1x1 convolution, is not straightforward as it requires integrating both channel-wise and spatial filtering which complicates the usage of simple 1D convolutions as a direct primitive. To cater towards this, DynAgent implements an algorithm (a detailed explanation of this implementation using a micro-controller environment is given in the Appendix~\ref{appendix:LEAcodes}) to represent depth-wise separable convolution as a function of the standard available micro-controller primitives. DynAgent further interacts with the rest of the components for successful NAS and training.

\subsection{DynFit: Intermittency-Aware Learning}
\label{sec:DynFit}
Traditional energy scavenging systems employ deep neural networks (DNNs) optimized via post-training techniques such as resource-aware pruning, quantization, and distillation~\cite{netadapt, eap}. While these methods are practical, they can compromise accuracy and scalability. To address these limitations, ~\cite{intermittentNAS} introduced iNAS, tailored for intermittent computing systems. However, iNAS faces significant challenges: firstly, it presumes the existence of task-based DNN deployment facilitating efficient checkpointing, which demands comprehensive knowledge of the system architecture as well as the energy-harvesting (EH) environments. Inaccurate task partitioning can lead to suboptimal checkpointing, causing energy waste or computational inaccuracies. Secondly, it relies on conventional load-store Von Neumann architecture. Emerging research in intermittent computing advocates for non-volatile and non-Von Neumann architectures, such as resistive RAM-based crossbar architectures, to enhance energy efficiency and enable hardware-driven checkpointing. To overcome the said issues, we propose iNAS+, an iteration of iNAS that integrates support for non-Von Neumann architectures and utilizes a broad spectrum of EH profiles (DynAgent~\ref{sec:DynAgent}). This modification expands the hardware search space, facilitating the evaluation and selection of optimal network configurations. Despite these enhancements, determining effective checkpointing strategies in intricate task-based systems remains a persistent challenge.

Towards this, DynFit introduces QuantaTask -- a discrete task unit executable without interruption under a specific EH profile and hardware setup. Our profiling suggests that loops, particularly in the convolutional (conv) and fully connected (FC) layers, dominate the inference process (accounting for 83\% to 90\% of execution time but only about 25\% of program coverage in assembly code). This disparity renders these loops prone to intermittency failures, often resulting in erroneous outcomes or necessitating re-executions. An analysis using WiFi EH for human activity recognition with accelerometer data indicates a completion rate of about 60\%, consistent across different datasets and energy conditions
%(Appendix~\ref{appendix:complete})
. To mitigate these failures, DynFit defines execution quanta as the ``smallest task unit'' that can be performed uninterrupted. It optimizes the number of loop iterations in GeMM or conv kernels, treating them as trainable parameters to achieve optimal task partitioning. DynFit closely integrates with DynAgent, which serves as a ``repository'' for EH profiles and hardware characteristics. Let $\mathcal{Q}$ denote the set of execution quanta, where each quanta $q \in \mathcal{Q}$ is defined by a tuple $(l, e)$, with $l$ representing the number of loop iterations and $e$ the estimated energy required for these iterations. The optimization objective is:
$\text{minimize} \quad \sum_{q \in \mathcal{Q}} E_q \quad \text{subject to} \quad E_q \leq E_b$, 
where $E_q$ is the energy consumed by quanta $q$ and $E_b$ is the energy budget. We iterate through various loop iteration lengths to identify the most optimal quanta. If executing multiple quanta exceeds the energy budget, we fuse the tasks to minimize overheads:
\[
l_i = \underset{l_i}{\text{argmin}} \, \sum_{q \in \mathcal{Q}} E_q \quad \text{subject to} \quad E_q \leq E_b. 
\] 
This optimization occurs during the forward pass of the DNN, leveraging energy estimations from DynAgent. By incorporating these mechanisms into the training process, DynFit ensures that the DNN operates efficiently within the constraints of energy scavenging systems
%(see Algorithm~\ref{algo:OptimizeQuantaTask})
.
In scenarios where the available energy allows the execution of multiple quanta, DynFit fuses tasks to minimize load-store and checkpointing overheads. An implementation example of QuantaTask-based convolution using microcontroller primitives is provided in Algorithm~\ref{algo:QuantaConv}.  While this approach  significantly reduces the likelihood of intermittency-related failures, it does not guarantee the completion of the entire inference process. Systems may resort to approximation techniques, such as neuron skipping or quantization, to fulfill computation within given SLOs.

\subsubsection{Dynamic Dropout for Energy-Constrained Environments}
\label{sec:DynamicDropout}
In environments characterized by extremely low energy availability, approximations in computation may become necessary. One such strategy involves skipping some neurons in each layer by dynamically enabling dropout during execution, thereby ensuring the completion of all layers under constrained conditions. However, traditional neural network architectures do not support the incorporation of dropout during inference phases. To address this, we propose the adoption of ``dynamic dropout'' during the training phase to prepare the network for such operational scenarios. We employ various algorithms (see Appendix~\ref{appendix:DynDropMath} for the list and formulation) to dynamically adjust dropout rates based on specific criteria, each designed to optimize network resilience under energy constraints.

Conceptually, let \( \mathcal{D} \) represent the set of dynamic dropout algorithms, defined as: $\mathcal{D} = \{d_1, d_2, d_3, d_4, d_5, d_6\}$. Let \( \mathbf{W} \) denote the weights of the neural network, and \( \mathbf{X} \) represent the input data. The output \( \mathbf{Y} \) of the network can be modeled as: $\mathbf{Y} = f(\mathbf{W}, \mathbf{X}, \mathbf{m})$, where \( \mathbf{m} \) is the dropout mask applied to the weights during the forward pass. Consider \( \mathcal{Q} \) as the set of execution quanta, where each quanta \( q \in \mathcal{Q} \) is defined by a tuple \((l, e)\), with \( l \) representing the number of loop iterations and \( e \) is the estimated energy required for these iterations. The energy consumption \( E_q \) for a given quanta \( q \) and the overall energy budget \( E_b \) for the inference process are defined. The objective is to optimize the network's performance under these constraints:
\[
\text{minimize} \quad \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}}) \quad \text{subject to} \quad \sum_{q \in \mathcal{Q}} E_q \leq E_b. 
\]


\subsubsection{Dynamic Quantization for Energy-Constrained Environments}
\label{sec:DynamicQuantization}
Neuron skipping, while effective for managing energy constraints, often results in significant accuracy degradation. To address this issue, we propose  dynamic quantization of neurons that are candidates for dropout to lower bitwidths, such as reducing from 16-bit to 12-bit, 8-bit, or even 4-bit. This strategy  allows for some level of computation to persist, potentially enhancing accuracy compared to outright neuron dropout. Define $\mathbf{W}$ as the weights of the neural network, $\mathbf{X}$ as the input data, and $\mathbf{m}$ as the dropout mask applied during the forward pass. We introduce $\mathbf{Q}$ as the set of quantization levels:
$\mathbf{Q} = \{q_1, q_2, q_3, q_4\}$, where $q_1 = 16 \text{ bits}$, $q_2 = 12 \text{ bits}$, $q_3 = 8 \text{ bits}$, and $q_4 = 4 \text{ bits}$. During training, we optimize not only for the weights $\mathbf{W}$ but also for the selection of quantization levels $\mathbf{q}$ for neurons at the risk of being dropped. The network's output $\mathbf{Y}$ can thus be expressed as:
$\mathbf{Y} = f(\mathbf{W}, \mathbf{X}, \mathbf{m}, \mathbf{q})$, where $\mathbf{q}$ is the vector representing quantization levels applied to the weights. To effectively integrate dynamic quantization into the training phase, we propose adding a term to the loss function that penalizes higher bit-widths, thereby promoting configurations that are more energy-efficient. Define the modified loss function $\mathcal{L}'$ as follows:
\[
\mathcal{L}' = \mathcal{L}(\mathbf{Y}, \mathbf{\hat{Y}}) + \lambda \sum_{i=1}^{n} c_i \cdot q_i. 
\]
Here, $n$ represents the number of neurons, $c_i$ denotes the cost associated with the quantization level $q_i$, and $\lambda$ is a regularization parameter that balances accuracy against energy efficiency. The energy consumption $E_q$ for a given quantization level $q$ and the total energy budget $E_b$ for the inference process are defined as: $\text{minimize} \quad \mathcal{L}' \quad \text{subject to} \quad \sum_{q \in \mathbf{Q}} E_q \leq E_b$. 

By embedding this dynamic quantization strategy into the training regimen, we enable the neural network to adaptively quantize neurons based on the available energy. While dynamic dropouts only update the non-dropped weights, with dynamic quantization all the weights are updated, providing a robust network. This approach strikes a delicate balance between maintaining  accuracy and adhering to stringent energy  constraints which not only is essential for intermittent environments, but also helpful for (ultra) low power platforms. 

\subsubsection{Fine-tuning for Regularization and Prevention of Overfitting}
\label{sec:fine_tuning_regularization_overfitting}
Training deep neural networks under intermittent energy conditions introduces unique challenges, particularly in maintaining uniform parameter updates. The implementation of dynamic dropout and quantization, designed to adapt to fluctuating energy levels, can result in certain weights being under-trained. This discrepancy may cause the network to overfit on weights that are consistently updated. To address this issue, we propose an adaptive regularization strategy that actively monitors and adjusts the update frequency of each weight throughout the training process. Define $U_i(t)$ as the update status of weight $i$ at training iteration $t$, with $U_i(t) = 1$ indicating an update and $U_i(t) = 0$ otherwise. A threshold parameter $\theta$ represents the minimum proportion of iterations a weight must be updated to avoid being considered under-trained. The update ratio for weight $i$ is computed as $\text{update\_ratio}_i = \frac{1}{T} \sum_{t=1}^T U_i(t)$. If $\text{update\_ratio}_i < \theta$ after $T$ iterations, these under-trained weights undergo additional training cycles without dropout or quantization, ensuring uniform training across all weights we train additional cycles on $w_i$ if $\text{update\_ratio}_i < \theta$. After completing the standard training process, weights that have met or exceeded the threshold are frozen, and fine-tuning phases focus specifically on the previously under-trained weights. This strategy has been empirically shown to enhance model robustness and improve generalization under varying operational conditions. $\text{Fine-tune on } w_i \text{ for } i \text{ where } \text{update\_ratio}_i < \theta$. This approach not only ensures that all parts of the model receive appropriate training attention but also mitigates the risk of overfitting, thus preserving the model's ability to generalize across diverse energy availability scenarios. 

\subsection{DynInfer: Intermittency Aware Task Scheduling for Inference}
\label{sec:DynInfer}
Effective task scheduling in intermittently-powered environments requires precise control over computational tasks to align with fluctuating energy availability. Contrary to the other components of NExUME, \textit{DynInfer} takes a system and architecture-level  approach to implement a task scheduler that adjusts in real-time to the energy conditions reported by \textit{DynAgent}. This ensures that the network operations are not only energy-efficient but also robust against power uncertainties. \textit{DynInfer} utilizes information about the current energy state and computational demands to {\em decompose} deep neural network (DNN) operations into smaller, manageable tasks. These tasks are then scheduled based on their priorities and energy requirements. The primary goals during this decomposition are to minimize latency and avoid SLO violations by prioritizing tasks critical to inference completion and to ensure that no individual task exceeds the current available energy, thus avoiding mid-operation failures.  Tasks are prioritized  according to a dynamically-generated schedule that considers energy profile (predicted short-term energy availability from \textit{DynAgent}), task criticality (importance of the task in contributing to the accuracy and stability of the DNN output, e.g.,  important kernels are scheduled first), and deadline sensitivity (tasks closer to their deadlines are given higher priority to reduce the risk of SLO violations). The scheduler adjusts task execution order in real-time, responding to updated forecasts and actual energy harvests, thus maintaining a balance between operational demands and available resources. To further optimize the processing efficiency, \textit{DynInfer} employs the dynamic task fusion strategy. In anticipation of power failures, \textit{DynInfer} integrates tightly with hardware-level and software-level checkpointing mechanisms. Hardware checkpointing utilizes non-volatile memory components to quickly save the state of computation at minimal energy cost, whereas software checkpointing manages the state of higher-level features and data structures that are not handled by hardware checkpointing. Upon recovery, \textit{DynInfer} coordinates with \textit{DynAgent} to efficiently restore the computation state and resume task execution, minimizing data loss and computational redundancy. \textit{DynInfer}'s effectiveness is enhanced through its integration with \textit{DynAgent}, which provides real-time data on energy availability, and \textit{DynFit}, which adjusts computational tasks according to the current system state and energy forecasts. A detailed description of the DynInfer execution flow with block diagrams are given in Appendix~\ref{appendix:DynInferFlow}


%%%%%








