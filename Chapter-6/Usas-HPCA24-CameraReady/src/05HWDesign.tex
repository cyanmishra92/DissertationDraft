%%\vspace{-4pt}
\noindent\underline{\textbf{Why Not Commercial GP-GPUs?}} DNN training is massively parallel, fairly compute intensive, time consuming, and needs a lot of (albeit structured) data movements~\cite{TPU,dadiannao}. Therefore, GP-GPUs have classically been used to train DNN models. However, as mentioned in \S\ref{sec:introduction}, the commercial GPUs used for DNN training are typically power hungry (
%\textcolor{blue}
{typically in 100s of Watts TDP; We exprimented with multiple GPUs, server class A6000: 300W TDP, server class A100: 250W -- 400W TDP, client class TRX3090: 350W TDP, and client class T4: 70W TDP}), and are {\em not} equipped to handle intermittent power emergencies. However, these GPUs are often equipped with dynamic voltage and frequency scaling (DVFS).
\footnote{{NVIDIA provides the list of supported clocks through the API ``$\texttt{nvidia--smi --q --d SUPPORTED\_CLOCKS}$''; We did not creport the results from A100 for this, as it does not offer multiple memory clocks, significantly impacting its DVFS capabilities. T4, thanks to its limited compute capabilities, could not finish training tasks on time.}}
To understand the impact of DVFS on energy savings and dynamic compute scaling, we implemented a simple multi-arm bandit algorithm to select the right bucket of compute frequencies (SM frequency for NVIDIA GPUs), and memory frequencies to match the power-demands of the intermittent solar source. %\textcolor{blue}
{As shown in Fig.~\ref{Fig:DVFSPrim} even with DVFS, commercial off the shelf GPUs could only finish $<50\%$ of the scheduled training task. However, hardware is not the only limitation, as even with custom hardware~\cite{dadiannao} enabled with the state-of-the-art continuous learning algorithm~\cite{ekya} could only finish $\approx 75\%$ of the scheduled training without any intermittency support. It is clear that we {\em can neither}  use the commercial GPUs {\em nor} rely on the standard software and algorithmic approach for intermittent training purpose as they {\em cannot} finish the compute given the intermittent power budget. }

\begin{figure}[h]
%\vspace{-8pt}
  \centering 
\includegraphics[trim={0 0 0 0},clip,width=\linewidth]{figs/DVFS.pdf}
  %\vspace{-10pt}
  \caption{
  %\textcolor{blue}
  {Impact of DVFS on completion (average power budget 70W). Note that, even with DVFS, most scheduled compute could not be finished. This includes the intermittent failures ($\leq$20W where no compute could be done); we included check-pointing to ensure that progress is saved in power-failures. C/S is the ratio of \uline{\textbf{C}}ompleted over the \uline{\textbf{S}}cheduled training tasks over multiple time windows of 4hours. Our custom HW runs with intermittent support both by hardware and software.}}
  \label{Fig:DVFSPrim}
 %\vspace{-12pt}
\end{figure}

There have also been significant efforts in designing and optimizing specialized DNN training accelerators~\cite{dadiannao, anupsparse,sparsetrain}, and many commercial organizations have already developed their own accelerators~\cite{TPU, Tesladojo} as well. Considering the compute mapping of the DNN training, almost all of these designs are based on a ``systolic architecture'', performing chains of multiplication and accumulations (MACs).  However, these devices take a ``throughput-first'' approach, to minimize the time consumption and seldom optimize power consumption first. This has lead to a global concern of the energy and consequently carbon-footprint of the DNN training~\cite{DNNenergy01,DNNcarbon01,DNNcarbon02,DNNcarbon03}. Furthermore, these accelerators have been designed to operate under constantly available power. Although our proposed representation learning (\S\ref{sec:CL}) and micro-profiler (\S\ref{sec:HP}) help us find a better training configuration that can minimize the compute if deployed in the aforementioned accelerators, it does not solve sustainability: That is, with variable solar power, can we scale compute alongside power to continue to make ``forward progress'', even when minimum amount of power is available. The systolic array structure of the DNN accelerators is well suited for this as we can change the compute size, as well as the number of memory channels feeding to those compute units as per the power availability. However, we need to be innovative in terms of designing and placing the compute hierarchy to ensure minimum data movement and re-computations when compute scaling. The hardware design of $\USS$ (Fig.~\ref{Fig:ACDesign}) incorporates all the aforementioned points. Note that, $\USS$ introduces a design philosophy for building a morphable hardware, and it can easily be adapted by any of the systolic array based commercial off the shelf (or research prototype) DNN training accelerators.
% \begin{figure}[h]
%   \centering%\vspace{-8pt} 
% \includegraphics[width=0.6\linewidth]{figs/ACDesign.pdf}%\vspace{-4pt}
%   \caption{High level hardware design of $\USS$.}
%   \label{Fig:ACDesign}%\vspace{-8pt}
% \end{figure}

\begin{figure} 
 \centering
    \subfloat[High-level arch]
    {
     \includegraphics[width=0.8\linewidth]{figs/ACDesign.pdf}%
    \label{Fig:ACDesign}
    }
    
    \subfloat[Power-down/ Failure handling]
    {
     \includegraphics[width=\linewidth]{figs/pwrdwn.pdf}%
    \label{Fig:wave}
    }
    %\vspace{-8pt}
    \caption{Overall architecture with the components and the power failure handle sequence of $\USS$.}
    \label{fig:HWDes}  
    %\vspace{-20pt}
\end{figure}

%\caption{High level hardware design of $\USS$. It consists of hardware components to support representation learning and micro-profiling along with a morphable systolic array to facilitate training with intermittent power.}

\noindent\textbf{DNN Compute Mapping:} Typically there are three ways of mapping DNN compute into a systolic array, namely, 1. output stationary; 2. input stationary; and 3. weight stationary~\cite{scalesim}. Most large-scale  accelerators use the output stationary implementations to minimize the output feature map movement~\cite{anupsparse}, and some of available hardware even supports multiple types of mappings~\cite{TPU,eyeriss2}. However, our design objective is to {\em minimize} data movements in the case of compute reconfiguration. In an output stationary mapping, {\em both} input and weights are dynamic and any power failure or reconfiguration will need to save and restore a lot of current context (partial sums, indices of weights and inputs etc.) to resume and remap the compute. This problem reduces in both input stationary and weight stationary, but at the cost of throughput~\cite{sarmaSparse}. Typically, the input feature maps are larger than the (individual) weights, and more importantly large weights can easily be represented or decomposed as multiple units called ``kernels'' (or ``filters''). In a typical convolutional neural network (CNN), each kernel is convoluted over the entire input feature map, and hence there is an ``inter-kernel parallelism''  (all kernels of a single layer can be executed in parallel) and ``intra-kernel parallelism'' (multiple computes in a convolution can happen in parallel). This property is true both for the forward pass and the backward pass of the standard CNN training. The modular nature of the weight stationary mapping makes it a strong candidate for use in a re-configurable or morphable systolic structure as turning off some compute is the same as not computing a kernel and scheduling it for later. Therefore, $\USS$ employs a weight stationary compute mapping for executing the training tasks on the morphable hardware. 
\begin{figure}[t]
  \centering 
\includegraphics[width=\linewidth]{figs/FullComputeMapping_New.pdf}
  \caption{Weight stationary compute mapping. The PE-level shows how the input flows and the convolutions are computed with a 3x3 convolution toy example. The tile-level shows how each tile consists of multiple such PEs and will be working on one kernel at a time. The accelerator-level shows that the entire accelerator is made of multiple such tiles (4x4 in the toy example). Inputs are broadcast into each tile so that each tile can work on a kernel. Computation is {\em redistributed} when there is a change in the power availability, and multiple tiles are shutdown (redacted) without impacting the data flow.}
  \label{Fig:IPS}%\vspace{-16pt}
\end{figure} 

\subsection{Design Description of the DNN Hardware Augmentations}
\label{subsec:MHW}
%Fig.~\ref{Fig:IPS} shows the hierarchy of input stationary compute mapping in the morphable $\USS$ hardware. 
% The accelerator consists of 256 tiles (a $4\times4$ arrangement of 16 super-tiles, each super-tile has $4\times4$ tiles). Each tile can individually be turned ON or OFF to cater towards the power availability. Each tile further consists of 64 count of 16bit floating point MAC units ($8\times8$ systolic array architecture) to perform the convolutions. To keep the compute modular, each tile is responsible for one kernel of a CNN. Therefore, a kernel of size $[C\times H\times W]$ would need $\ceil{[C\times H\times W]/64}$ iterations over the input to finish convolution. To stream the data from the memory and to store the partial compute, the accelerator has 4 double buffered SRAM structures. The weights are stored in a doubled buffered multi banked SRAM. The filter SRAM consists of 256 banks, each for one tile, of size 1kB (double buffered, 512B per buffer per bank). The input is broadcast to all the tiles by using the double-buffered input feature map SRAM of size 64kB (32kB each). An input feature map of dimensions $[X\times Y\times Z]$ would need $\ceil{[X\times Y\times Z]\times 2/ 2 \times 1024} = \ceil{[X\times Y\times Z]/1024}$ iterations to load all the inputs, and each buffer will be loaded $[X\times Y\times Z]/2048$ times. The output of the convolution map $[X\times Y\times Z] \xrightarrow{M\times C\times W\times H} [M\times U\times V]$, produces an output tensor of $[M\times U\times V]$ dimensions, where $M$ is the number of kernels. To support this, we have a 256 banked (1 bank/tile) double buffered output feature map SRAM with each bank of size 8kB (4kB/buffer). Along with this, we also have a 128kB SRAM to be used as scratchpad memory for storing activation, transposes and intermediate differentials generated during the backward pass. To perform the ReLU activation (forward pass) and inverse activation (backward pass), the accelerator also has $256 \times 256$ compactor-mux combinational logic units (256 units per tile).   For smaller DNNs, we do not have 256 kernels in any layer and hence the the accelerator can operate in a batching mode. At any given time, the accelerator can work with a batch size of $B = \floor{A/L}$ images (where $L$ is the number of filters in the layer with least number of channels, and $A$ is the number of active tiles). Note that, this is a generic design, and can be tailored towards specific workloads.

% \textcolor{blue}{In the delineated systolic array architecture, the compute mapping, memory accesses, and operational formulas play pivotal roles in efficiently executing the forward and backward passes during DNN training. The dimensions provided include an input feature map \([X \times Y \times Z]\), kernel size \([C \times H \times W]\), a systolic array of \(8 \times 8\) MAC units per tile, and an output feature map \([M \times U \times V]\) where \(M\) is the number of kernels. In the forward pass, activations are computed through the convolution of the input feature map with the kernels across the systolic array, characterized by the formula \(A_{muv} = \sum_{c=0}^{C-1} \sum_{i=0}^{H-1} \sum_{j=0}^{W-1} X_{(u+i)(v+j)c} \cdot K_{mijc}\), where \(A_{muv}\) denotes the activation, \(X\) represents the input feature map, and \(K\) signifies the kernel. Each tile computes one kernel's convolutions, storing the results in a double-buffered output feature map SRAM. In the backward pass, gradients are computed by backpropagating the errors from subsequent layers. The gradient of the loss function concerning the weights is computed utilizing the chain rule of calculus and the original input data, encapsulated by the formula \(\frac{\partial L}{\partial K_{mijc}} = \sum_{u=0}^{U-1} \sum_{v=0}^{V-1} \frac{\partial L}{\partial A_{muv}} \cdot X_{(u+i)(v+j)c}\), where \(\frac{\partial L}{\partial K_{mijc}}\) denotes the gradient of the loss function with respect to the weight, and \(\frac{\partial L}{\partial A_{muv}}\) signifies the gradient of the loss function with respect to the activation. Memory accesses for the input feature map and kernels are managed by double-buffered input feature map SRAM and double-buffered multi-banked SRAM respectively, ensuring timely data availability for the MAC units. The double buffering allows for one buffer to be used for computation while the other is filled with new data. The computed activations and gradients are stored in the double-buffered output feature map SRAM for their retrieval in subsequent layers or for weight updates. The \(8 \times 8\) systolic array in each tile executes the multiply-accumulate operations in a pipelined and parallel manner, adhering to the Weight Stationary approach where weights remain stationary while data flows through, optimizing the throughput and efficiency of the training operations in this hardware architecture.}

\noindent 
%\textcolor{blue}
{\textbf{Compute Mapping:}  Fig.~\ref{Fig:ACDesign} shows the high level design, architecture and different components present in our proposed accelerator. The accelerator encompasses 256 tiles, structured in a \(4\times4\) configuration of 16 super-tiles, each harboring \(4\times4\) tiles. These super-tiles Each tile, individually switchable ON or OFF based on power availability, houses 64 16-bit floating point MAC units configured in an \(8\times8\) systolic array for convolution operations. A modular computational approach is adopted where each tile is accountable for one CNN kernel, necessitating \(\ceil{[C\times H\times W]/64}\) iterations for a kernel of size \( [C\times H\times W]\). Data streaming and partial compute storage are facilitated by four double buffered SRAM structures, with the weights residing in a double buffered multi-banked SRAM. The filter SRAM has 256 banks (one per tile), each with a size of 1kB (double buffered, 512B per buffer per bank). Input data broadcast to all tiles is managed by a 64kB double-buffered input feature map SRAM (32kB each), requiring \(\ceil{[X\times Y\times Z]/1024}\) iterations for full input loading, with each buffer loaded \( [X\times Y\times Z]/2048 \) times. The convolution map transforms \( [X\times Y\times Z] \xrightarrow{M\times C\times W\times H} [M\times U\times V] \) to yield an output tensor of dimensions \( [M\times U\times V] \), supported by a 256 banked double buffered output feature map SRAM, each bank of size 8kB (4kB/buffer). A 128kB SRAM serves as a scratchpad for storing activations, transposes, and intermediate differentials during the backward pass. The accelerator also houses \(256 \times 256\) compactor-mux combinational logic units (256 units per tile) for ReLU activation (forward pass) and inverse activation (backward pass). For smaller DNNs without 256 kernels in any layer, a batching mode is operational with a batch size of \( B = \floor{A/L} \) images, where \( L \) denotes the layer with the fewest channels, and \( A \) the number of active tiles. This generic design is adaptable for various workloads.}

%\textcolor{blue}
{In DNN training, meticulous compute mapping, memory access strategies, and operational formulas are instrumental for the forward and backward passes. The forward pass computes activations via the formula \( A_{muv} = \sum_{c=0}^{C-1} \sum_{i=0}^{H-1} \sum_{j=0}^{W-1} X_{(u+i)(v+j)c} \cdot K_{mijc} \), with results stored in the double-buffered output feature map SRAM. The backward pass emphasizes gradient computation through backpropagation, which is crucial for weight updates. The gradient of the loss function concerning the weights is computed through the formula \( \frac{\partial L}{\partial K_{mijc}} = \sum_{u=0}^{U-1} \sum_{v=0}^{V-1} \frac{\partial L}{\partial A_{muv}} \cdot X_{(u+i)(v+j)c} \). This gradient computation, fundamental for learning, is meticulously mapped across the systolic array, ensuring precise and efficient backpropagation. Memory accesses are optimally managed via the double-buffered SRAM structures, providing timely data availability for the MAC units. The \(8 \times 8\) systolic array in each tile executes multiply-accumulate operations in a pipelined and parallel fashion, abiding by the Weight Stationary approach, thereby optimizing the throughput and efficiency of the training operations within this hardware architecture.}

\noindent
%\textcolor{blue}
{\textbf{Power Control Logic:} Power emergency prediction in $\USS$ is always conservative, and the solar power predictor has a mean accuracy of 92\%, limiting false positives and helping the control unit select appropriate tile counts. The system needs at least 512 cycles of advanced notice to flush compute and enable a compute migration. Fig.~\ref{Fig:ACDesign} shows the block diagram of the mesh interconnect, and Fig.~\ref{Fig:wave} shows the power-down sequence and signal states. The network works at a super-tile (STile) granularity and each arbiter node uses an 8x8 priority-mux. The network only gets activated when it gets a \emph{w-pdown} warning signal from the predictor. This signal starts a graceful power-down sequence for the required number of tiles.  The \emph{w-pdown} triggers the \emph{backup} signal and the system goes into \emph{pwr-warning} state (other states being on, off, invalid and X). In the \emph{pwr-warning} phase the system finishes the remaining compute of the systolic arrays (which can take up to 64 cycles), and starts flushing the results for backup.}

\noindent
%\textcolor{blue}
{\textbf{Buffer Management:} $\USS$ uses non-volatile state buffers (NVSBs, 18 count, 1 per 4x4 tiles, each of 1kB, and 2 of 4kB each) for state saving and data backup. The control logic prioritizes writing data into the local NVSB for the arbiter (each arbiter caters to 2 STiles). If those get full because of continuous power failures, the control directs the data to the global NVSBS (NVSB-NW and NVSB-SE in Fig.~\ref{Fig:ACDesign}). The NVSB stores the global/asynchronous work queue and the shuffling configuration (mini-batch arrangement).}

\subsection{Power Failure and Compute Scheduling}
\label{subsec:cmpschd}
%\textcolor{blue}
{Central to the $\USS$ accelerator's operational efficiency is the work queueâ€”an intricately designed, hierarchical structure that meticulously catalogues pending computational tasks. Each task, represented in the queue, corresponds to the execution of specific CNN kernels, feature tiles and operations. As deep neural network models often have a complex interplay of layers, each with distinct computational needs, the work queue ensures a systematic, prioritized approach to handle these operations. Two distinct scheduling strategies, each complemented by its own type of work queue, govern the computational flow: Conservative Scheduling and Eager Scheduling. The dual-scheduling mechanism, bolstered by the work queue's flexible architecture, not only optimizes compute performance but also offers resilience against power uncertainties.
} %Upon predicting a power failure, the work queue schedule, intermediate result, network and layer information are saved. 
The work queue schedule, intermediate result, network and layer information are saved on predicted power failure, and data from the DRAM (the working set of IF/OF/filter and model state) are moved to an NV-RAM using STT-RAM
%\footnote{\textcolor{blue}{STT-RAMs have proven to have very high durability ($> 10^{15}$ cycles, and have been proposed to used as cache) Our design occasionally writes to the STT-RAM, at times in granularity of tens of minutes to hours. Therefore, the STT-RAM will have a significantly long life than other components, like capacitors, solar panels etc. We also have emerging technologies like Fe-FETs which can be used as an alternative solution.}} 
based buffers in the memory hierarchy (parallel to the IF/OF/filter). We do {\emph{not}} replace the DRAM buffers with NVM because of limited lifetimes~\cite{STTlife}. The host writes the latest copy of the completed iteration (in epoch granularity) into the STT-RAMs (STT-RAM-N for the upper 128 SAs, and STT-RAM-S for the lower 128SAs, Fig.~\ref{Fig:ACDesign}). In case of a complete power failure, the compute in flight are rejected and, once the system starts working, the work queues get invalidated and the host starts the compute again from the last checkpoint. 
%\textcolor{blue}
{Along with that, the most common intermittent software libraries and software designs~\cite{chinchilla,IntBeyondEdge} (and most DNN training libraries like PyTorch, TensorFlow) also  offer periodic checkpoints.} Note that the power-up sequence for a tile runs in the exact opposite order of the \emph{powerdown} sequence (a tile becomes computationally active 512 cycles after it gets the power up signal). $\USS$ uses two kinds of scheduling policies to handle the graceful \emph{powerdown} and work queue rearrangement.

\noindent\textbf{Conservative Scheduling:} The most important part of the $\USS$ accelerator design is to ensure proper ``compute placement'' even under a power emergency or power scaling. 
Fig.~\ref{Fig:IPS}:\textbf{Accelerator level} provides a high-level overview of the compute scheduling (where the redacted part of the hardware is turned off because of the lack of power). 
The key components of the scheduler are the ``moving average power predictor'' and the ``micro-profiler''. In the $i^{th}$ kernel scheduling iteration, given the power budget and power prediction, the micro-profiler decides the required training configuration, and the control logic (conservatively) enables suitable number of tiles (say $t_i$ tiles of the 256 tiles). Those $t_i$ tiles fetch $t_i$ unique kernels from the 1Byte wide, 256 deep global kernel dispatch queue (GKDQ, $t_i$ kernels scheduled in parallel
%, refer Fig.~\ref{Fig:WQS}
). Note that the power requirement of each tile is known in advance (please refer to \S\ref{sec:eval}, TABLE~\ref{tab:specs} for details). Once the scheduled ($t_i$) tiles are completed, the micro-profiler again finds the right configuration for the $i+1^{th}$ iteration and the scheduler again conservatively enables $t_{i+1}$ number of tiles suitable for the power budget. The $t_{i+1}$ tiles fetch the next $t_{i+1}$ kernels from the GKDQ and the process continues. This conservative compute and power estimation ensures that none of the kernel computes (the lowest decomposed level of compute unit for the hardware) ever fails and hence there is no need for any partial data movement. The GKDQ always points to the next available kernel location. The control fetches the right number of kernels and all of them are synchronously executed in the active tiles.

% \begin{figure} 
%  \centering
%     \subfloat[Global Work Queue]
%     {
%      \includegraphics[width=0.75\linewidth]{figs/WQSchedule.pdf}%
%     \label{Fig:WQS}
%     }
    
%     \subfloat[Asynchronous work queue]
%     {
%      \includegraphics[width=0.75\linewidth]{figs/WQ_Reallocation.pdf}%
%     \label{Fig:Async}
%     }
%     %\vspace{-4pt}\caption{Scheduling kernels on the morphable hardware during power failures.}
%     \label{fig:SCHED}  
%     %\vspace{-18pt}
% \end{figure}

\noindent\textbf{Eager Scheduling:} A weight stationary implementation with a conservative scheduling will always run synchronously. However, in the middle of an kernel execution iteration, if the hardware gains access to more power which in turn can enable more tiles, it cannot do so without breaking synchrony (i.e. when some of the tiles are half way through the compute, some other tiles can just start execution). Facilitating such scheduling will provide us less idle time, more forward progress and more efficient use of the incoming power but at the expense of more control overheads. We call this \textit{Eager Scheduling}. To enable eager scheduling, we decentralized the global kernel dispatch queue and equipped each tile with a local kernel dispatch queue (1Byte wide 16 deep). At the beginning of each kernel scheduling iteration, the micro-profiler decides the right configuration, and the control distributes equal number of kernels to each active tile (given $A$ active kernel, and $K$ total kernels, each tile gets $\floor{K/A}$ kernels to execute). The conservative scheduler ensures that no tile loses power before finishing the current scheduled kernel. However, in the middle of the execution if any new tiles becomes alive (because of an increase in harvested power), the scheduler immediately marks it ready to start working and the tile fetches a kernel (currently not scheduled in any of the tiles) and starts working on it. We face three issues here: 1. How does the new tile get any kernel to work on? 2. Over multiple iterations of such asynchronous scheduling, the kernel queue for each tile will be of different size creating a load imbalance; how to tackle this? 3. How do we know when to stop executing?

%\textcolor{blue}
{To address the first two issues, we developed a work-stealing mechanism for each tile. 
%Fig.~\ref{Fig:Async} shows a toy example of how eager scheduling will work with work stealing. 
When any of the active tiles are marked ready by the scheduler, the tile employees a state machine to decide where to get work from. Each time the tile finishes some work, if its remaining work queue (the local work queue size) is less than the average of all other active tiles, it seeks a new kernel to work on. Considering the global control always enqueues any idle tile with work, whenever the tile has no work left, it steals a kernel from the most heavily loaded tiles. We implemented a counter (local kernel counter) to keep track of the size of the remaining local work queue of each of the tile. We also implemented a counter (layer kernel counter) which keeps track of the total kernels to be scheduled for each layer. Whenever all the local work queue counter hits zero along with the layer kernel counter, the control moves to schedule the next layer (or previous layer in backward propagation) for computation. }

% \begin{figure}[ht] 
%   \centering%\vspace{-4pt}  
% \includegraphics[trim={0 0 0 7pt},clip,width=\linewidth]{figs/LoadBalancing.pdf}
%   %\vspace{-18pt}\caption{Load balancing state machine with eager scheduling example: any worker with less than average work asks for new work or steals it from a neighbor with more.} 
%   \label{Fig:loadBal} 
%   %\vspace{-10pt} 
% \end{figure} 
Note that we do not delve into the details of the computational primitives involved in training the DNN as several prior works~\cite{dadiannao, eyeriss2, sarmaSparse} provide a very detailed accounting of it (for both forward and backward pass) along with the hardware and control requirements. We treat the convolution scheduling (using input stationary and at a kernel level) in a morphable systolic hardware to be the main challenge and explain it. 

%\subsection{Supporting Representation Learning} 
%\label{subsec:RLHW} 
%As shown in Fig.~\ref{Fig:iCARLD}, the most compute-intensive part of representation learning is feature extraction (using multiple large models). Since exemplar selection using representation learning is infrequent (further details in \S\ref{sec:eval}), we utilize the existing systolic array hardware to perform the feature extraction. The host programs the weights of the teacher models and the convolution is executed as mentioned in \S\ref{subsec:cmpschd}. We also reuse the adder tree (used for calculating the partial sums of the convolution) to calculate the sample mean. Once the exemplar image goes though all the teacher models, its feature vector (flattened output of the last convolution layer) is sent to the host. The host then performs the distance estimation and the cluster modification using K-means+. Note that K-means+ clustering is an iterative process with many vector operations (mean squared error calculation) and hence is suitable for modern day CPUs (e.g., ARM Cortex A-78) capable of working as a host for video data handling. 

%%%%%

