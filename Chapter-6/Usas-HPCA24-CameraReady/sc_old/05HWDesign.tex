\noindent\underline{\textbf{Why Won't Commercial GP-GPUs Work?}} DNN training is massively parallel, fairly compute intensive, time consuming, and needs a lot of (albeit structured) data movements~\cite{TPU,dadiannao}. Therefore, GP-GPUs have classically been used to train DNN models. However, as mentioned in \S\ref{sec:introduction}, the commercial GPUs used for DNN training are typically power hungry (typically in 100s of Watts TDP; A6000: 300W, A100: 250W -- 400W, TRX3090: 350W, T4: 70W), and are {\em not} equipped to handle intermittent power failure while operating with an intermittent power source like solar power. However, these GPUs are often equipped with dynamic voltage and frequency scaling (DVFS)\footnote{NVIDIA provides the list of supported clocks through the API ``$\texttt{nvidia--smi --q --d SUPPORTED\_CLOCKS}$''}. To understand the impact of DVFS on energy savings and dynamic compute scaling, we implemented a simple multi-arm bandit algorithm to select the right bucket of compute frequencies (SM frequency for NVIDIA GPUs), and memory frequencies to match the power-demands of the intermittent solar source. Figure~\ref{Fig:DVFSPrim} shows the impact of performing compute with DVFS on the commercial off the shelf GPUs\footnote{We did not choose A100 for this, as it does not offer multiple memory clocks -- significantly impacting its swath of DVFS ability.} along with perfoming compute on a custom hardware~\cite{dadiannao} enabled with the state-of-the-art continuous learning algorithm~\cite{ekya} but without any intermittency support. It is clear that we {\em cannot}  use the commercial GPUs for this purpose as they {\em cannot} finish the compute given the intermittent power budget. 

\begin{figure}[h]
  \centering 
\includegraphics[trim={0 0 0 0},clip,width=\linewidth]{figs/DVFS.pdf}
  \vspace{-20pt}\caption{Impact of DVFS on completion (average power budget 70W). Note that, even with DVFS most of the scheduled compute could not be finished. This includes the intermittent failures (<20W where no compute could be done). We included check-pointing to ensure that the progress is saved in case of power-failures.}
  \label{Fig:DVFSPrim}
  \vspace{-8pt}
\end{figure}

There have also been significant efforts in designing and optimizing specialized DNN training accelerators~\cite{dadiannao, anupsparse,sparsetrain}, and in fact many commercial organizations have already developed their own accelerators~\cite{TPU, Tesladojo} as well. Considering the compute mapping of the DNN training, almost all of these designs are based on a ``systolic architecture'', performing chains of multiplication and accumulations (MACs).  However, these devices take a ``throughput-first'' approach, to minimize the time consumption and seldom optimize power consumption first. This has lead to a global concern of the energy and consequently carbon-footprint of the DNN training~\cite{DNNenergy01,DNNcarbon01,DNNcarbon02,DNNcarbon03}. Furthermore, these accelerators have been designed to operate under constantly available power. Although our proposed representation learning (\S\ref{sec:CL}) and micro-profiler (\S\ref{sec:HP}) help us find a better training configuration that can minimize the compute if deployed in the aforementioned accelerators, it does not solve sustainability: That is, with variable solar power, can we scale compute alongside power to continue to make ``forward progress'', even when minimum amount of power is available. The systolic array structure of the DNN accelerators is well suited for this as we can change the compute size, as well as the number of memory channels feeding to those compute units as per the power availability. However, we need to be innovative in terms of designing and placing the compute hierarchy to ensure minimum data movement and re-computations when compute scaling.
The hardware design of $\USS$ 
(Figure~\ref{Fig:ACDesign}) 
incorporates all the aforementioned points. 
\begin{figure}[h]
  \centering\vspace{-8pt} 
\includegraphics[width=0.6\linewidth]{figs/ACDesign.pdf}\vspace{-4pt}
  \caption{High level hardware design of $\USS$.}
  \label{Fig:ACDesign}\vspace{-8pt}
\end{figure}

%\caption{High level hardware design of $\USS$. It consists of hardware components to support representation learning and micro-profiling along with a morphable systolic array to facilitate training with intermittent power.}

\noindent\textbf{DNN Compute Mapping:} Typically there are three ways of mapping DNN compute into a systolic array, namely, 1. output stationary; 2. input stationary ;and 3. weight stationary~\cite{scalesim}. Most large-scale  accelerators use the output stationary implementations to minimize the output feature map movement~\cite{anupsparse}, and some of available hardware even supports multiple types of mappings~\cite{TPU,eyeriss2}. However, our design objective is to {\em minimize} data movements in the case of compute reconfiguration. In an output stationary mapping, {\em both} input and weights are dynamic and any power failure or reconfiguration will need to save and restore a lot of current context (partial sums, indices of weights and inputs etc.) to resume and remap the compute. This problem reduces in both input stationary and weight stationary, but at the cost of throughput~\cite{sarmaSparse}. Typically, the input feature maps are larger than the (individual) weights, and more importantly large weights can easily be represented or decomposed as multiple units called ``kernels'' (or ``filters''). In a typical convolutional neural network (CNN), each kernel is convoluted over the entire input feature map, and hence there is an ``inter-kernel parallelism''  (all kernels of a single layer can be executed in parallel) and ``intra-kernel parallelism'' (multiple computes in a convolution can happen in parallel). This property is true both for the forward pass and the backward pass of the standard CNN training. The modular nature of the weight stationary mapping makes it a strong candidate for use in a re-configurable or morphable systolic structure as turning off some compute is the same as not computing a kernel and scheduling it for later. Therefore, $\USS$ employs a weight stationary compute mapping for executing the training tasks on the morphable hardware. 
\begin{figure}[t]
  \centering 
\includegraphics[width=\linewidth]{figs/FullComputeMapping_New.pdf}
  \caption{Weight stationary compute mapping. The PE-level shows how the input flows and the convolutions are computed with a 3x3 convolution toy example. The tile-level shows how each tile consists of multiple such PEs and will be working on one kernel at a time. The accelerator-level shows that the entire accelerator is made of multiple such tiles (4x4 in the toy example). Inputs are broadcast into each tile so that each tile can work on a kernel. Computation is {\em redistributed} when there is a change in the power availability, and multiple tiles are shutdown (redacted) without impacting the data flow.}
  \label{Fig:IPS}\vspace{-16pt}
\end{figure} 

\subsection{Design Description of the DNN Hardware}
Figure~\ref{Fig:IPS} shows the hierarchy of input stationary compute mapping in the morphable $\USS$ hardware. The accelerator consists of 256 tiles (Accelerator level in Figure~\ref{Fig:IPS}) which can individually be turned ON or OFF to cater towards the power availability. Each tile consists of 64 count of 16bit floating point MAC units ($8\times8$ systolic array architecture) to perform the convolutions. To keep the compute modular, each tile is responsible for one kernel of a CNN. Therefore, a kernel of size $[C\times H\times W]$ would need $\ceil{[C\times H\times W]/64}$ iterations over the input to finish convolution. To stream the data from the memory and to store the partial compute, the accelerator has 4 double buffered SRAM structures. The weights are stored in a doubled buffered multi banked SRAM. The filter SRAM consists of 256 banks, each for one tile, of size 1kB (double buffered, 512B per buffer per bank). The input is broadcast to all the tiles by using the double-buffered input feature map SRAM of size 64kB (32kB each). An input feature map of dimensions $[X\times Y\times Z]$ would need $\ceil{[X\times Y\times Z]\times 2/ 2 \times 1024} = \ceil{[X\times Y\times Z]/1024}$ iterations to load all the inputs, and each buffer will be loaded $[X\times Y\times Z]/2048$ times. The output of the convolution map $[X\times Y\times Z] \xrightarrow{M\times C\times W\times H} [M\times U\times V]$, produces an output tensor of $[M\times U\times V]$ dimensions, where $M$ is the number of kernels. To support this, we have a 256 banked (1 bank/tile) double buffered output feature map SRAM with each bank of size 8kB (4kB/buffer). Along with this, we also have a 128kB SRAM to be used as scratchpad memory for storing activation, transposes and intermediate differentials generated during the backward pass. To perform the ReLU activation (forward pass) and inverse activation (backward pass), the accelerator also has $256 \times 256$ compactor-mux combinational logic units (256 units per tile).   For smaller DNNs, we do not have 256 kernels in any layer and hence the the accelerator can operate in a batching mode. At any given time, the accelerator can work with a batch size of $B = \floor{A/L}$ images (where $L$ is the number of filters in the layer with least number of channels, and $A$ is the number of active tiles). 

\subsection{Compute Scheduling}
\label{subsec:cmpschd}
\noindent\textbf{Conservative Scheduling:} The most important part of the $\USS$ accelerator design is to ensure proper ``compute placement'' even under a power emergency. Figure~\ref{Fig:IPS}:\textbf{Accelerator level} provides a high-level overview of the compute scheduling (where the redacted part of the hardware is turned off because of the lack of power). The key components of the scheduler are the ``moving average power predictor'' and the ``micro-profiler''. In the $i^{th}$ kernel scheduling iteration, given the power budget and power prediction, the micro-profiler decides the required training configuration, and the control logic (conservatively) enables suitable number of tiles (say $t_i$ tiles of the 256 tiles). Those $t_i$ tiles fetch $t_i$ unique kernels from the 1Byte wide, 256 deep global kernel dispatch queue (GKDQ, $t_i$ kernels scheduled in parallel, refer Figure~\ref{Fig:WQS}). Note that the power requirement of each tile is known in advance (please refer to \S\ref{sec:eval}, TABLE~\ref{tab:specs} for details). Once the scheduled ($t_i$) tiles are completed, the micro-profiler again finds the right configuration for the $i+1^{th}$ iteration and the scheduler again conservatively enables $t_{i+1}$ number of tiles suitable for the power budget. The $t_{i+1}$ tiles fetch the next $t_{i+1}$ kernels from the GKDQ and the process continues. This conservative compute and power estimation ensures that none of the kernel computes (the lowest decomposed level of compute unit for the hardware) ever fails and hence there is no need for any partial data movement. The GKDQ always points to the next available kernel location. The control fetches the right number of kernels and all of them are synchronously executed in the active tiles.

\begin{figure} 
 \centering
    \subfloat[Global Work Queue]
    {
     \includegraphics[width=0.45\linewidth]{figs/WQSchedule.pdf}%
    \label{Fig:WQS}
    }
    \subfloat[Asynchronous work queue]
    {
     \includegraphics[width=0.51\linewidth]{figs/WQ_Reallocation.pdf}%
    \label{Fig:Async}
    }
    \vspace{-8pt}\caption{Scheduling kernels on the morphable hardware during power failures.}
    \label{fig:SCHED}  
    \vspace{-14pt}
\end{figure}

\noindent\textbf{Eager Scheduling:} A weight stationary implementation with a conservative scheduling will always run synchronously. However, in the middle of an kernel execution iteration, if the hardware gains access to more power which in turn can enable more tiles, it cannot do so without breaking synchrony (i.e. when some of the tiles are half way through the compute, some other tiles can just start execution). Facilitating such scheduling will provide us less idle time, more forward progress and more efficient use of the incoming power but at the expense of more control overheads. We call this \textit{Eager Scheduling}. To enable eager scheduling, we decentralized the global kernel dispatch queue and equipped each tile with a local kernel dispatch queue (1Byte wide 16 deep). At the beginning of each kernel scheduling iteration, the micro-profiler decides the right configuration, and the control distributes equal number of kernels to each active tile (given $A$ active kernel, and $K$ total kernels, each tile gets $\floor{K/A}$ kernels to execute). The conservative scheduler ensures that no tile loses power before finishing the current scheduled kernel. However, in the middle of the execution if any new tiles becomes alive (because of an increase in harvested power), the scheduler immediately marks it ready to start working and the tile fetches a kernel (currently not scheduled in any of the tiles) and starts working on it. We face three issues here: 1. How does the new tile get any kernel to work on? 2. Over multiple iterations of such asynchronous scheduling, the kernel queue for each tile will be of different size creating a load imbalance; how to tackle this? 3. How do we know when to stop executing?

To address the first two issues, we developed a work-stealing mechanism for each tile. Figure~\ref{Fig:Async} shows a toy example of how eager scheduling will work with work stealing. When any of the active tiles are marked ready by the scheduler, the tile employees a state machine to decide where to get work from. Figure~\ref{Fig:loadBal} shows the state machine for work assignment with a load balancing toy example. Each time the tile finishes some work, if its remaining work queue (the local work queue size) is less than the average of all other active tiles, it seeks a new kernel to work on. Considering the global control always enqueues any idle tile with work, whenever the tile has no work left, it steals a kernel from the most heavily loaded tiles. We implemented a counter (local kernel counter) to keep track of the size of the remaining local work queue of each of the tile. We also implemented a counter (layer kernel counter) which keeps track of the total kernels to be scheduled for each layer. Whenever all the local work queue counter hits zero along with the layer kernel counter, the control moves to schedule the next layer (or previous layer in backward propagation) for computation. 

\begin{figure}[h] 
  \centering  
\includegraphics[trim={0 0 0 7pt},clip,width=\linewidth]{figs/LoadBalancing.pdf}
  \vspace{-16pt}\caption{Load balancing state machine with eager scheduling example: any worker with less than average work asks for new work or steals it from a neighbor with more.} 
  \label{Fig:loadBal} 
  \vspace{-8pt} 
\end{figure} 

Note that we do not delve into the details of the computational primitives involved in training the DNN as several prior works~\cite{dadiannao, eyeriss2, sarmaSparse} have very detailed accounting of every step of performing a training (both forward and backward pass) along with the hardware and control requirement for the same. We treat the convolution scheduling (using input stationary and at a kernel level) in a morphable systolic hardware to be the main challenge and explain the same. 

\subsection{Supporting Representation Learning} 
\label{subsec:RLHW} 
As shown in Figure~\ref{Fig:iCARLD}, the most compute-intensive part of the representation learning is feature extraction (using multiple large models). Since exemplar selection using representation learning is infrequent (further details in \S\ref{sec:eval}), we utilize the existing systolic array hardware to perform the feature extraction. The host programs the weights of the teacher models and the convolution is executed as mentioned in \S\ref{subsec:cmpschd}. We also reuse the adder tree (used for calculating the partial sums of the convolution) to calculate the sample mean. Once the exemplar image goes though all the teacher models, its feature vector (flattened output of the last convolution layer) is sent to the host. The host then performs the distance estimation and the cluster modification using the K-means algorithm. Note that, K-means clustering is an iterative process with a lot of vector operations (mean squared error calculation) and hence is suitable for modern day CPUs (e.g., ARM Cortex A-78) capable of working as a host for video data handling. 

%%%%%

