%%%%%

%\subsection{Edge Servers: The Panacea for Video Analytics}
%Video analytics using DNNs is a growing business and is expected to reach about \$19.3~Billion by the end of this decade~\cite{YahooVideo}. However, 
Performing video analytics at scale presents substantial challenges, especially for streaming applications that are deployed at the edge but often need real time analytics and accurate response~\cite{ekya}. The first challenge is \textbf{data traffic} and \textbf{network reliability}. Streaming video data from a traffic camera to the cloud is often expensive, especially for wireless communications (e.g., a 720p @ 30fps RGB video results in $\approx 25Gbps$ bandwidth requirement per camera (refer \S\ref{sec:eval} for details of the data set). Edge networks are also often unreliable~\cite{getmobile,ekya},  resulting in communication disruption. Secondly, video data are often \textbf{sensitive}, especially in the context of public deployment use-cases like urban mobility: Many modern privacy regulations and government policies~\cite{sweden-data,azure-data} prohibit streaming the data to a third party central server. This renders using any central data centers (sustainable or otherwise) useless. Therefore, edge servers, like AWS outpost~\cite{aws-outposts}, and Azure App Service Environment (Azure ASE)~\cite{azure-ase}, have become the go-to solutions for edge video analytics. 
%Edge servers, unlike their cloud counterpart can be deployed at the premise and operated by the governing agency. 
These edge servers often enjoy the flexibility and simplicity of cloud interfaces (like having access to the same APIs, tools, etc.), 
%while being in the control of the governing organization, yet being entirely managed and serviced by the service provider if an when required. Although, 
but they typically have limited resources (e.g., weak GPUs, smaller memory capacities~\cite{AWSOutpostPricing}). %footprints~\cite{AWSOutpostPricing} etc.
%, they still are able to service multiple video inference requests  (often in real time, or) without SLA violations~\cite{videoedge}.

\subsection{Model Compression: But, It Drifts}
To cater towards the limited resources and to maximize the throughput (and meet SLAs), these edge servers often use customized video-analytics services~\cite{MSrocket}. Furthermore, they also rely on {\em specialized DNN models} targeted for edge deployments~\cite{mobilenetv2,tinyyolo} for performing inference. In contrast to the typical DNN models deployed in the cloud premise, these edge models are compressed, quantized and often tailored towards the targeted hardware~\cite{deepcompression,netadapt,NASnetMobile}, which enables them to perform accurate inference with a high throughput and a low resource footprint (compressed models may have $\approx 50\times$ fewer parameters~\cite{deepcompression}). However, parameter reduction comes at the cost of loss of generality~\cite{noscope,seqlearning}, which leads to {\em data drift} sensitivity. 
%Although drifts are minimal if the training data accurately captures the nature of the deployment, 
In real-world systems, these drifts can become significant as the live video diverges from the training data and the environment changes rapidly~\cite{ekya}. Our experiments (Figure~\ref{Fig:DDrift}) on the Urban Traffic data set~\cite{ekya,UrbanTraffic} trained and tested on MobileNet-V2~\cite{mobilenetv2} (quantized to 16bits) shows that movement in time over a 2 hour window and a varied change of scene degrades the accuracy of the edge network by more than $15\%$ compared to its baseline accuracy.
\begin{figure}[ht]
  \centering\vspace{-0pt} 
\includegraphics[width=0.8\linewidth]{figs/1.pdf}\vspace{-4pt}
  \vspace{-4pt}\caption{Data drift over time of 10 hours and the effect of retraining on accuracy; each time window is 2 hours.}
  \label{Fig:DDrift}\vspace{-12pt}
\end{figure}

\vspace{-8pt}\subsection{Continuous Learning at the Edge}
{\em Continuous learning}, where the model keeps on learning from the new samples over time (updating for seen and previously unseen classes), has been the most preferred approach to compensate for data drift~\cite{continuelearn01,incremental01,icarl,incremental03}. The temporal locality of the videos allows the models to effectively learn from the recent data~\cite{incremental02}. Figure~\ref{Fig:DDrift} also shows the accuracy improvement of the MobileNet-V2 model with a na\"ive retraining over time. To further improve the accuracy and reduce sampling bias~\cite{samplingbias}, multiple task dedicated models are typically deployed. For example, in a traffic monitoring scenario, the morning traffic and noon traffic might be significantly different, and can benefit from different data. 

Due to their significant compute and time costs, DNN training is typically performed in the cloud. For a continuous learning paradigm, training becomes an essential, repetitive (and perhaps perennial) task. Considering the prior limitations of communicating video data to cloud, edge servers are ideal candidates not only for inference but also for continuous learning. These edge servers are typically equipped with GPUs, and therefore, Ekya~\cite{ekya}, has proposed co-locating the learning algorithm along with the inference tasks on the edge servers. Since there could be multiple models with varying degree of drifts and different time bounds to get back the accuracy, Ekya proposes an intelligent scheduling algorithm to determine the priority of the training and inference tasks jointly optimizing the inference SLA and the drift over time. Intuitively, the inference tasks typically take priority due to their strict SLAs. However, with multiple resource types (e.g., CPUs, GPUs, and domain accelerators) and multiple tasks (different trainings, different SLA inferences), scheduling becomes non-trivial and Ekya shows significant improvement in resource utilization compared to a na\"ive fair scheduler (which treats training and inference with equal importance). Although Ekya partly solves issues of managing learning at the edge, it gives rise to a new problem: \textit{``can we pervasively (and sustainably) deploy these edge servers at the scale that future applications demand?"} 

\subsection{\underline{Power} play: Solution Becomes Problem}
Scaling applications with existing commercial edge servers exposes a serious problem. For example, a g4dn.12xlarge instance~\cite{AWSOutpostPricing}, one of the most popular AWS outpost offerings for edge analytics, consists of a 24~core Intel Xeon CPU (150W TDP)~\cite{IntelXeonPower} with 192GB of memory and 4 NVIDIA T4 (with tensor cores, 70W TDP)~\cite{T4Power} with 64GB GPU memory. A standard offering with $2\times$g4dn.12xlarge instances need 4kW power~\cite{aws-outposts} (the compute units have a TDP of $\approx 1kW$~\cite{IntelXeonPower,T4Power}) for performing the analytics. With state-of-the-art learning APIs~\cite{MSrocket} and intelligent co-location and scheduling of inference and continuous learning~\cite{ekya}, these edge servers can support about 8 videos streams~\cite{ekya}, resulting in $\approx 120W$ (just dedicated for compute) per video stream. Scaling this to crowded cities like Beverly Hills ($> 35k$ cameras~\cite{BeverlyHills}), Los Angeles ($\approx 35k$ cameras~\cite{NYCcamera}), New York city ($\approx$56k cameras~\cite{NYCcamera}), or Chicago ($\approx$30k cameras) will need a lot of power. In fact, it will take $\ge$3Million cameras\footnote{Assuming $\approx$9 cameras per 1000 people (same as LA), and scaling to the US population} to just enable autonomous urban mobility throughout United States, which might consume $360MW$ power ($1296GWh$ energy, 0.03\% of entire US power grid) 
%which is similar to half the energy requirement of Yemen~\cite{EIAWorld} 
to perform just video analytics. Clearly, the current solution is {\em not}  sustainable, neither in terms of the load on the power grid, nor in terms of the $CO_2$ footprint (1.1$\times$$10^9$lbs of emission).

\subsection{Intermittent Computing: A Ray of Hope}
An obvious solution to the power problem is to run the training in a self-sustained way, i.e., without depending on the power grid and by relying on a renewable energy source like solar power; opportunities for harvesting renewables naturally scale alongside a greater number of deployment locations and solar power, even though not always available, is in abundance. In United State, considering a typical $12\%$ efficient solar panel~\cite{solardata}, we can have an annual average of $50W/m^2-150W/m^2$ solar power available to use~\cite{NRELDB}. Furthermore, solar power has reasonable predictability characteristics. Typically, inference tasks have significantly less compute time and power requirement, and commercial off the shelf devices, like edgeTPU~\cite{coralT} can perform object detection using the aforementioned compressed models at a reasonable frame rate (at times $\ge 71fps$). Therefore, {\em designing a training platform to perform continuous learning with the intermittent solar power and within the typical harvested budget} would be the best solution. The power sustainability consequently reduces the cost of deployment\footnote{The specialized AWS outpost offering, one of the cheaper and lower power consuming ones, for performing edge inference costs \$5,134.92/month~\cite{AWSOutpostPricing}.} as well.  

\subsection{Our Novelty:}
\textcolor{blue}{As mentioned earlier (in~\S\ref{sec:introduction}), in this work we plan to tackle this issue by proposing a multi-pronged optimization: \textbf{1.} At \textbf{Algorithm level} to adapt the right continuous learning approach (in \S\ref{sec:CL}) while enabling robust exemplar selection; \textbf{2.} At \textbf{application--deployment level} to determine the right set of learning parameters (in \S\ref{sec:HP}), i.e., to make the learning {\em both} power-aware and drift-aware while running the algorithm efficiently in the training hardware environment; \textbf{3.} A novel \textbf{hardware} (in \S\ref{sec:HW}) support for continuous learning, hyperparameter selection, and most importantly, to perform intermittent power-aware training. Finally, $\USS$ combines all these pieces of the puzzle together to design a sustainable intermittent computing framework for performing continuous learning on edge-gathered video data. $\USS$ is not the first work to propose continuous learning on video data at edge servers, but it is the first one to do so {\em sustainably}, while only using an intermittent energy environment. }

%%%%%

