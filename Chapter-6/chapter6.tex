% Chapter 6: Usas - Solar-Powered Continuous Learning at the Edge
\chapter{\US{}: Solar-Powered Continuous Learning at the Edge}
\label{ch:usas}

\section{Introduction: From Milliwatts to Kilowatts}
\label{sec:usas-intro}

The journey through intermittent systems has taken us from the microscopic world of batteryless sensors operating on microwatts to the sophisticated analog neural accelerators harvesting milliwatts of ambient energy. In Chapter~\ref{ch:intelligent-inference}, we developed the fundamental principles of intelligent inference on resource-constrained devices, exploring how neural networks can operate efficiently under severe power and memory limitations. Chapter~\ref{ch:nexume} advanced this work by introducing adaptive training and inference mechanisms that enabled deep neural networks to learn and evolve despite intermittent power availability. Most recently, Chapter~\ref{ch:lreye} pushed the boundaries of energy efficiency through analog computation, achieving unprecedented performance per watt through hardware-aware neural network co-design with ReRAM crossbars. Yet as we stand at the threshold of this final technical chapter, a profound realization emerges: the principles governing intermittent computation remain remarkably consistent across six orders of magnitude in power scale.

Consider the modern urban landscape, where tens of thousands of cameras monitor traffic flows~\cite{UM1, UM2, UM3}, detect anomalies~\cite{anomalyD}, and enable smart city applications~\cite{Intel-urbanmobility}. These edge servers, operating at kilowatt scales rather than milliwatts, face a strikingly familiar challenge: how to perform continuous computation when power availability fluctuates. The source of intermittency shifts from capacitor discharge cycles measured in milliseconds to solar irradiance patterns spanning hours, but the fundamental problem persists. A system must adapt its computation to match available energy, preserve progress despite interruptions, and maximize useful work within power constraints. This scale-invariant nature of intermittency suggests that the insights developed for tiny sensors can inform the design of macro-scale systems, while the computational resources available at larger scales enable new approaches impossible on constrained devices.

The transition from micro to macro scale brings both opportunities and responsibilities. Edge servers possess the computational power to run sophisticated deep neural networks, perform real-time video analytics, and continuously adapt to changing data distributions. Yet this capability comes with a sobering environmental cost~\cite{DNNenergy01, DNNcarbon01, DNNcarbon02, DNNcarbon03}. Deploying continuous learning on New York City's approximately 56,000 surveillance cameras~\cite{NYCcamera} using traditional edge servers~\cite{aws-outposts, AWSOutpostPricing} would consume 17 megawatts of power, contributing 156 metric tons of CO$_2$ emissions daily~\cite{CO2FootPrint}. As cities worldwide embrace intelligent infrastructure, the aggregate environmental impact becomes unsustainable. The imperative for renewable-powered computing extends beyond academic curiosity to urgent practical necessity.

This chapter presents \US{}, a comprehensive system that enables solar-powered continuous learning for edge video analytics. The name, derived from the Vedic goddess of dawn, emphasizes both the significance of solar power in our design and the dawn of a new era in sustainable edge computing. \US{} addresses the unique challenges that arise when scaling intermittent systems from sensors to servers. Where previous chapters focused on preserving basic functionality during power failures, \US{} must maintain complex learning pipelines involving data labeling, exemplar selection, hyperparameter optimization, and distributed training. Where tiny devices could rely on simple checkpointing mechanisms, \US{} requires sophisticated state management across gigabytes of model parameters and training data.

The technical contributions of \US{} span multiple layers of the system stack. At the algorithmic level, we develop a confidence-based frame selection mechanism that identifies the most informative video frames for continuous learning while operating within strict power budgets. Our student-teacher paradigm~\cite{student_teacher01} enables unsupervised labeling of streaming video data, eliminating the need for human annotation while preserving privacy~\cite{ekya}. Through representation learning, we address the sampling bias~\cite{samplingbias} inherent in real-world video streams where certain classes dominate the observed distribution. At the architectural level, we introduce a morphable systolic array that dynamically adjusts its computational capacity to match available solar power. Unlike conventional accelerators that fail catastrophically when power drops below minimum thresholds, our morphable design gracefully scales computation from 256 tiles at full power down to 16 tiles during cloudy periods.

The synthesis of these contributions enables a fundamental shift in how we approach edge computing sustainability. Rather than viewing renewable energy as an auxiliary power source backed by grid connections, \US{} demonstrates that sophisticated AI workloads can operate entirely on harvested solar power. Our evaluation on real-world urban traffic datasets~\cite{UrbanTraffic} shows that \US{} achieves 4.96\% higher accuracy than traditional continuous learning approaches while reducing annual energy consumption by 234.95 kWh compared to conventional DNN accelerators~\cite{TPU} and 2.63 MWh compared to datacenter GPUs. These improvements translate directly to environmental impact, with each \US{}-equipped edge server preventing 200 pounds of CO$_2$ emissions annually.

The remainder of this chapter develops these ideas systematically. Section~\ref{sec:usas-challenge} examines the edge server challenge in detail, exploring how scale affects intermittency and why existing solutions prove inadequate. Section~\ref{sec:usas-architecture} presents the \US{} system architecture, describing our algorithmic innovations for continuous learning under power constraints. Section~\ref{sec:usas-morphable} details the morphable hardware design that enables computation to adapt dynamically to power availability. Section~\ref{sec:usas-evaluation} evaluates our system on real workloads and solar traces, quantifying both accuracy improvements and sustainability benefits. Section~\ref{sec:usas-bridging} steps back to examine the common principles that unite our work across scales, from the resource-constrained devices of Chapter~\ref{ch:intelligent-inference} to the edge servers presented here. Finally, Section~\ref{sec:usas-discussion} discusses limitations, broader implications, and future directions before Section~\ref{sec:usas-summary} concludes the chapter.

\FloatBarrier
\section{The Edge Server Challenge: When Scale Meets Intermittency}
\label{sec:usas-challenge}

The proliferation of edge computing represents a fundamental shift in how we process and analyze data. Rather than streaming information to distant datacenters, edge servers bring computation closer to data sources, reducing latency, preserving privacy, and enabling real-time decision-making. This paradigm proves particularly compelling for video analytics, where bandwidth constraints, privacy regulations, and latency requirements make cloud processing increasingly impractical. Consider autonomous vehicles that must detect pedestrians within milliseconds, smart city systems that cannot legally stream citizen footage to third-party clouds, or industrial monitoring systems where network reliability cannot be guaranteed. In each case, edge servers provide the necessary computational power while respecting operational constraints.

Yet the very characteristics that make edge servers attractive also create new challenges. Unlike datacenter deployments where thousands of servers share cooling infrastructure and power distribution, edge servers operate in diverse, often hostile environments. They may be mounted on traffic poles exposed to extreme temperatures, installed in remote locations with unreliable grid connections, or deployed in developing regions where power infrastructure remains nascent. The computational demands placed on these servers continue to grow as applications become more sophisticated. Modern video analytics pipelines employ complex deep neural networks for object detection, tracking, activity recognition, and anomaly detection. These models, while powerful, require substantial computational resources that strain edge server capabilities.

The challenge intensifies when we consider data drift~\cite{compressiondrift01, compressiondrift02}, a phenomenon where the distribution of observed data changes over time relative to the training distribution. A model trained on summer traffic patterns may perform poorly during winter conditions. A surveillance system deployed in a residential area may encounter different vehicle types when nearby construction redirects commercial traffic. These distribution shifts affect all deployed models but prove particularly problematic for edge deployments. Compressed models~\cite{compression01, compression02, compression03}, commonly used on edge servers to meet latency and resource constraints, exhibit heightened sensitivity to data drift. Where a full-scale model might experience 5\% accuracy degradation, a compressed model may suffer 15-20\% performance loss under the same distribution shift. This vulnerability necessitates continuous learning~\cite{continuelearn01, icarl}, where models adapt to new data patterns while retaining knowledge of previous distributions.

Figure~\ref{fig:drift-motivation} illustrates the severity of data drift across multiple modalities including video, audio~\cite{audioMNIST, UrbanSound}, and 3D point cloud data. Focusing on video analytics, our experiments demonstrate that compressed models like quantized MobileNet-V2~\cite{mobilenetv2} (14M parameters, 71.3\% baseline accuracy) experience accuracy degradation exceeding 20\% across five sampling windows of four hours each. In contrast, larger models such as ResNet-101~\cite{VGG} (171M parameters, 76.4\% baseline accuracy) exhibit minimal degradation under identical conditions. Critically, continuous retraining enables the smaller model to maintain its original accuracy despite ongoing distribution shifts. Similar trends appear across audio and 3D point cloud modalities, confirming that continuous learning is essential for maintaining model quality across diverse deployment scenarios.

\begin{figure}[!htb]
\centering
\includegraphics[width=\linewidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/DriftAll.pdf}
\caption{Data drift across different modalities demonstrates the vulnerability of compressed models to distribution shifts. Sampling windows: 4 hours for video, 20 minutes for audio (urban traffic), 1 hour for 3D point cloud (simulated). SM: Small Model (compressed/quantized), LM: Large Model (no compression), SMR: Small Model with continuous retraining. Compressed models suffer over 20\% accuracy loss without retraining, while continuous learning restores performance to baseline levels.}
\label{fig:drift-motivation}
\end{figure}

The traditional approach to continuous learning relies on cloud infrastructure~\cite{ekya}. Edge devices stream suspicious or misclassified samples to the cloud, where powerful servers retrain models on aggregated data from multiple edge nodes. Updated models are then distributed back to edge devices, completing the learning cycle. This approach, while technically sound, fails to address the fundamental constraints that motivated edge computing initially. Streaming video data for labeling consumes precious bandwidth, potentially violating privacy regulations~\cite{sweden-data, azure-data}. The round-trip latency for model updates may span hours or days, during which edge devices operate with degraded accuracy. Most critically, the approach maintains dependence on centralized infrastructure, negating the autonomy benefits of edge deployment.

These challenges motivate local continuous learning, where edge servers independently adapt to distribution shifts without cloud connectivity. However, implementing continuous learning on edge servers introduces its own complexities. Training deep neural networks requires orders of magnitude more computation than inference. A MobileNetV2 model that performs inference in 50 milliseconds may require 30 seconds for a single training iteration on the same hardware. The memory footprint expands dramatically when maintaining gradient buffers, optimizer states, and training datasets. Power consumption, already a concern for inference workloads, becomes prohibitive when training runs continuously. An edge server that consumes 50W during inference may demand 300W during training, exceeding the capacity of typical edge power supplies.

The sustainability implications of scaled edge deployment prove sobering. Consider a modest smart city deployment with 10,000 cameras, each supported by an edge server performing continuous learning. Using conventional hardware, this infrastructure would consume approximately 3 MW continuously, equivalent to the power demand of 2,500 typical American homes. The associated carbon emissions would total 27,000 tons annually, comparable to burning 13 million kilograms of coal. As cities worldwide embrace intelligent infrastructure, with megacities like Tokyo, Delhi, and Shanghai deploying hundreds of thousands of cameras, the aggregate environmental impact becomes untenable. The promise of edge computing to enable sustainable, distributed intelligence collides with the reality of its power demands.

Renewable energy, particularly solar power, offers an appealing solution~\cite{SOLRAD, NRELDB}. Solar panels can be integrated directly with edge infrastructure, providing local power generation without grid dependence. The cost of photovoltaic systems has decreased by 90\% over the past decade, making solar deployment economically viable~\cite{batterysus1, batterysus2}. Many edge locations, such as rooftops, parking structures, and open fields, naturally accommodate solar installations. The correlation between solar availability and peak usage periods (daylight hours) for many applications provides natural load balancing. Yet solar power introduces intermittency at a scale fundamentally different from the microsecond power failures addressed in previous chapters.

Solar intermittency operates across multiple timescales. Passing clouds cause power fluctuations over seconds to minutes. Diurnal cycles create predictable but dramatic variations between day and night. Weather patterns introduce multi-day periods of reduced generation during storms. Seasonal variations can change average daily generation by 3-4$\times$ between summer and winter at higher latitudes. Unlike the binary power failures of batteryless devices that either have sufficient energy or shut down completely, solar-powered systems experience continuous power variation across a wide dynamic range. An edge server designed for 300W peak power may need to operate at 200W during light clouds, 50W during heavy overcast, and 0W at night.

Batteries provide one approach to managing solar intermittency, storing excess energy during peak generation for use during low-light periods~\cite{batterysus3, batterysus4}. However, batteries introduce their own challenges for edge deployments. The cost of battery systems often exceeds that of the computational hardware they support. Batteries degrade over time~\cite{STTlife}, particularly in the thermal cycling conditions common at edge locations, requiring replacement every 3-5 years. The environmental impact of battery production and disposal partially negates the sustainability benefits of renewable power~\cite{batterysus5, batterysus6, batterysus7}. Most fundamentally, sizing batteries for worst-case scenarios (extended cloudy periods) proves economically infeasible for large-scale deployments. A battery system capable of sustaining an edge server through a week of cloudy weather might cost \$10,000 per node, making city-scale deployment prohibitively expensive.

These constraints lead us to a different approach: designing edge systems that adapt dynamically to available power rather than attempting to maintain constant operation through energy storage~\cite{NVPma, IntBeyondEdge}. This philosophy, which we term morphable computing, requires fundamental changes in how we architect both hardware and software systems. Just as Chapter~\ref{ch:intelligent-inference} introduced intelligent inference mechanisms for resource-constrained devices~\cite{Origin}, Chapter~\ref{ch:nexume} developed adaptive training approaches for intermittent power environments, and Chapter~\ref{ch:lreye} created analog circuits robust to voltage variations~\cite{resiRCA}, we must now create edge systems that gracefully scale computation with solar availability. The challenge extends beyond simple throttling or duty cycling~\cite{chinchilla}. The system must maintain learning quality despite power variations, ensure forward progress even during minimal power availability, and maximize computational efficiency across the entire power range.

The following sections detail how \US{} addresses these challenges through a combination of algorithmic innovation, architectural design, and system integration. We show that by embracing intermittency rather than attempting to mask it, we can create edge systems that provide continuous learning capabilities while operating sustainably on renewable energy.

\FloatBarrier
\section{\US{} System Architecture}
\label{sec:usas-architecture}

The \US{} architecture represents a fundamental rethinking of how edge servers perform continuous learning under intermittent power. Rather than treating power variability as a constraint to be overcome through batteries or grid connections, we design our system to embrace and adapt to the natural rhythms of solar energy. This section presents our system architecture, beginning with an overview of the key components and their interactions, then detailing our algorithmic innovations for continuous learning under power constraints.

\subsection{System Overview}

The \US{} system architecture, illustrated in Figure~\ref{fig:usas-system}, separates inference and training into distinct computational phases that can be scheduled based on power availability~\cite{ekya}. This separation proves critical for several reasons. Inference tasks, which directly support user-facing applications, require consistent low latency and high availability. These tasks consume relatively little power, typically 2-5W for a MobileNetV2~\cite{mobilenetv2} model running on specialized hardware like EdgeTPU~\cite{coralT}, and can be sustained even during periods of limited solar generation. Training tasks, in contrast, are power-intensive but latency-tolerant. They can be scheduled opportunistically when excess solar power becomes available, scaled dynamically based on instantaneous power budgets, and interrupted without affecting inference quality.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/IntroDesignNew.pdf}
\caption{The \US{} system architecture, showing the separation of inference and training paths, the student-teacher paradigm for unsupervised labeling, and the morphable hardware accelerator that adapts to available solar power.}
\label{fig:usas-system}
\end{figure}

The inference pipeline operates continuously on incoming video streams using a lightweight student model optimized for edge deployment. As frames arrive from cameras, the student model performs object detection, classification, and tracking tasks required by the application. Critically, the model also outputs confidence scores for each prediction, which serve as signals for identifying frames that would benefit from additional training. Low-confidence predictions indicate potential distribution shift or novel patterns that the current model struggles to recognize. These frames, rather than being discarded or misclassified, become candidates for continuous learning.

The training pipeline activates when sufficient solar power becomes available, typically during daylight hours when generation exceeds inference requirements. The pipeline consists of three major components that work in concert to enable unsupervised continuous learning. The automated labeling system employs a teacher model, substantially larger and more accurate than the student model, to generate high-quality labels for selected frames. The exemplar selection mechanism uses representation learning to build a balanced training set that addresses sampling bias in the observed data stream. The micro-profiler determines optimal hyperparameters for training given the current power budget and data characteristics. These components feed into the morphable hardware accelerator, which performs the actual training computation while dynamically adjusting its capacity to match available power.

\subsection{Confidence-Based Frame Selection}

The first challenge in continuous learning from video streams involves identifying which frames warrant inclusion in the training process. Video data exhibits tremendous redundancy, with consecutive frames often differing only slightly. Training on all frames would waste computational resources and potentially overfit to dominant patterns. Random sampling, while simple, may miss rare but important events. Our confidence-based selection mechanism addresses these challenges by using the student model's uncertainty as a signal for identifying informative frames.

For each frame $f_t$ at time $t$, the student model produces not only class predictions but also confidence scores $c_t \in [0,1]$ for each detected object. We compute an aggregate frame confidence $C_t$ as the harmonic mean of individual object confidences, which penalizes frames where even a single object has low confidence. Frames with $C_t$ below a threshold $\tau$ become candidates for training. This threshold adapts dynamically based on the power budget and training queue length. During periods of abundant solar power, we lower $\tau$ to include more frames, enabling the model to refine its knowledge of common patterns. When power is limited, we raise $\tau$ to focus on the most challenging examples.

The confidence-based selection provides several benefits beyond computational efficiency. It naturally implements a form of active learning, where the model focuses training on examples it finds difficult. This approach accelerates learning compared to random sampling, particularly when adapting to distribution shifts. The mechanism also provides implicit curriculum learning, as the model initially trains on clearly problematic frames before refining performance on subtle cases. Most importantly, confidence scores serve as an early warning system for distribution shifts. A sudden drop in average confidence indicates that the observed data has diverged from the training distribution, triggering more aggressive training to adapt to new patterns.

\subsection{Student-Teacher Paradigm for Unsupervised Labeling}

Obtaining labeled training data remains one of the primary challenges in continuous learning systems. Traditional approaches require human annotators to review and label video frames, a process that violates privacy constraints and proves economically infeasible at scale. Recent self-supervised learning methods can learn representations without labels but struggle with the fine-grained distinctions required for video analytics tasks. Our student-teacher paradigm provides a middle ground, generating high-quality pseudo-labels automatically while preserving privacy and enabling continuous adaptation.

The teacher model, typically 10-50$\times$ larger than the student model, runs on selected frames when power permits. For a frame identified by the confidence mechanism, the teacher performs the same detection and classification tasks as the student but with substantially higher accuracy. We employ an ensemble of three teacher models trained on different datasets and architectures to improve robustness. The teachers' predictions are combined through weighted voting, where weights reflect each teacher's historical accuracy on validated examples. This ensemble approach reduces the impact of individual model biases and provides more reliable pseudo-labels than any single teacher.

Figure~\ref{fig:auto-labeling} illustrates our auto-labeling mechanism within the student-teacher framework. The student model performs continuous inference on incoming video frames, outputting classification results along with confidence scores. Frames where the student exhibits low confidence indicate potential distribution shift or novel patterns that warrant further investigation. These low-confidence frames become candidates for auto-labeling by the teacher ensemble. By selectively processing only frames that challenge the current student model, we focus computational resources on the most informative examples while minimizing unnecessary teacher invocations. The ensemble of teachers then provides high-quality labels for these challenging frames, which subsequently enter the exemplar selection pipeline for potential inclusion in the training set.

\begin{figure}[!htb]
\centering
\includegraphics[width=\linewidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/AutoLabel.pdf}
\caption{Auto-labeling mechanism in \US{}. The student model continuously processes video frames, identifying low-confidence predictions that may contain novel information. These frames are selected for labeling by an ensemble of teacher models using weighted majority voting, enabling unsupervised continuous learning without human annotation.}
\label{fig:auto-labeling}
\end{figure}

The key insight enabling our student-teacher approach lies in the asymmetric computational requirements of the two models. The student model must run continuously on every frame to support real-time inference, necessitating a lightweight architecture. The teacher models need only process selected frames when power is available, allowing for much larger, more accurate architectures. This asymmetry maps naturally to solar power availability, as illustrated in Figure~\ref{fig:usas-teachers}. During peak generation, we can afford to run teacher models frequently, generating many labeled examples. During limited power periods, we rely on previously generated labels or defer training until conditions improve.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/teachers.pdf}
\caption{Student-teacher paradigm for unsupervised labeling. Multiple teacher models process selected video frames to generate high-quality pseudo-labels, enabling continuous learning without human annotation.}
\label{fig:usas-teachers}
\end{figure}

To prevent distribution collapse, where the student merely mimics the teacher's biases, we implement several mechanisms to maintain model diversity. First, we periodically update the teacher models using validated examples from the student's predictions, creating a virtuous cycle of mutual improvement. Second, we inject noise into the training process through data augmentation and dropout, preventing exact replication of teacher outputs. Third, we maintain a small set of human-validated examples as anchors, ensuring the models remain grounded in true labels. These mechanisms work together to enable continuous improvement while avoiding the degeneracy that can occur in purely self-supervised systems.

\subsection{Representation Learning for Exemplar Selection}

Real-world video streams exhibit severe class imbalance. In urban traffic monitoring, cars may appear in 90\% of frames while emergency vehicles appear in less than 0.1\%. Training directly on this imbalanced distribution causes the model to overfit to common classes while failing to recognize rare but important events. Traditional solutions like weighted sampling or synthetic minority oversampling prove inadequate for continuous learning scenarios where the class distribution constantly evolves. Our representation learning approach addresses this challenge by constructing balanced exemplar sets that preserve knowledge of all observed classes while adapting to new patterns.

Figure~\ref{fig:sampling-bias} illustrates the severity of sampling bias in real-world traffic scenarios. Static objects like fire hydrants and traffic lights appear in 100\% of frames, while dynamic objects exhibit highly skewed distributions. Without proper exemplar selection, the training set would mirror this imbalanced distribution, leading to catastrophic forgetting for rare classes (such as bicycles, which barely appear in the dataset) and overfitting for omnipresent classes (such as traffic lights). The representation learning framework prevents this bias by maintaining balanced representation across all classes, ensuring the model retains the ability to recognize infrequent but critical objects.

\begin{figure}[!htb]
\centering
\includegraphics[trim={0 40pt 0 1pt},clip,width=0.85\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/biastrain.pdf}
\caption{Distribution of object classes in typical traffic patterns and the impact of sampling bias on training. The "appeared" line shows the percentage of frames containing each class (e.g., fire hydrants appear in 100\% of frames in the observed scene). Naive exemplar selection creates non-IID training distributions, causing catastrophic forgetting for rare classes and overfitting for common classes. Representation learning addresses this by maintaining balanced exemplars.}
\label{fig:sampling-bias}
\end{figure}

The exemplar selection process begins by encoding each labeled frame into a high-dimensional representation using the penultimate layer of the teacher model, as shown in Figure~\ref{fig:usas-replrn}. These representations capture semantic content while abstracting away pixel-level details. We maintain a exemplar buffer $\mathcal{E}$ with fixed capacity $K$, typically 10,000 frames for a city-scale deployment. When a new frame $f_{\text{new}}$ arrives, we must decide whether to include it in $\mathcal{E}$ and, if so, which existing frame to remove. This decision balances multiple objectives that often conflict. We want to maximize coverage of the representation space, ensuring diverse examples are retained. We need to maintain class balance, preventing common classes from dominating the buffer. We must preserve temporal relevance, gradually replacing old examples with recent observations. We should minimize redundancy, avoiding near-duplicate frames that waste buffer capacity.

Our selection algorithm addresses these objectives through a novel scoring mechanism. For each frame $f_i$ in the buffer, we compute a retention score $R_i$ that combines four factors. The coverage factor $\alpha_i$ measures the frame's contribution to representation space coverage using the determinant of the Gram matrix formed by nearby frames. The balance factor $\beta_i$ inversely weights frames based on their class frequency, giving higher scores to rare classes. The temporal factor $\gamma_i$ applies exponential decay based on frame age, gradually reducing the importance of old examples. The uniqueness factor $\delta_i$ measures the minimum distance to other frames in representation space, penalizing redundant examples. The final retention score $R_i = \alpha_i \cdot \beta_i \cdot \gamma_i \cdot \delta_i$ provides a holistic measure of each frame's value to the training process.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/RepLrn.pdf}
\caption{Representation learning for exemplar selection. Frames are encoded into a high-dimensional representation space where our selection algorithm maintains diversity, class balance, and temporal relevance in the training buffer.}
\label{fig:usas-replrn}
\end{figure}

When the buffer reaches capacity, we employ a two-stage replacement strategy. First, we identify the frame with the lowest retention score as a candidate for removal. Second, we simulate the buffer's statistics after replacing this frame with the new arrival, computing the change in class balance, coverage, and diversity metrics. We only proceed with replacement if these metrics improve or remain within acceptable bounds. This conservative approach prevents sudden distributional shifts that could destabilize training. Over time, the exemplar buffer evolves to reflect the observed data distribution while maintaining sufficient diversity for robust learning.

\subsection{Micro-Profiling for Hyperparameter Optimization}

Training deep neural networks involves numerous hyperparameters that significantly impact both accuracy and computational efficiency. Learning rate, batch size, momentum, weight decay, and other parameters must be carefully tuned for optimal performance. Traditional hyperparameter optimization methods like grid search or Bayesian optimization require extensive computational resources, making them impractical for power-constrained edge deployments. Our micro-profiling approach enables efficient hyperparameter selection by exploiting the relationship between power availability, data characteristics, and optimal training configurations.

The micro-profiler maintains a performance model $\mathcal{M}$ that predicts training outcomes based on hyperparameters and system state. The model takes as input the current hyperparameter configuration $\theta$, available power budget $P$, exemplar set statistics $S$ (class distribution, temporal spread, representation diversity), and current model accuracy $A$. It outputs predicted values for training throughput $T$ (examples per second), accuracy improvement $\Delta A$, and power consumption $P_{\text{actual}}$. This model, initially trained offline using historical data, continuously updates based on observed training runs, becoming more accurate over time.

When training begins, the micro-profiler uses $\mathcal{M}$ to evaluate different hyperparameter configurations within the feasible space defined by the power budget. For example, with limited power, the profiler might select a small batch size and low learning rate that enable steady progress without exceeding power constraints. With abundant power, it might choose larger batches and aggressive learning rates to maximize improvement during the available window. The profiler particularly optimizes the trade-off between batch size and gradient accumulation steps. Larger batches improve hardware utilization but require more memory and power. Gradient accumulation achieves similar statistical effects with lower peak power but increases training time.

The micro-profiler also adapts hyperparameters during training based on observed dynamics. If the loss plateaus, indicating convergence to a local minimum, the profiler may increase the learning rate to escape. If validation accuracy decreases while training accuracy improves, suggesting overfitting, it may increase weight decay or dropout. If power availability suddenly decreases mid-training, it can gracefully reduce batch size and adjust other parameters to continue within the new constraints. This adaptive behavior proves essential for maintaining training quality despite the unpredictable nature of solar power.

\FloatBarrier
\section{Morphable Hardware Design}
\label{sec:usas-morphable}

The hardware architecture of \US{} represents a radical departure from conventional accelerator design. Traditional DNN accelerators optimize for peak throughput under constant power, employing fixed arrays of processing elements that fail catastrophically when power drops below design thresholds. Our morphable design instead enables dynamic scaling of computational capacity to match available power, ensuring forward progress even under severe power constraints. This section details our hardware innovations, from the architectural principles enabling morphability to the specific implementations that achieve efficient training under intermittent solar power.

\subsection{Why Commercial GPUs Fail Under Intermittency}

Before describing our morphable architecture, it is instructive to understand why existing solutions prove inadequate for solar-powered training. Commercial GPUs, the workhorses of modern deep learning, consume hundreds of watts during training. An NVIDIA A100, widely used for datacenter training, has a thermal design power (TDP) of 400W. Even efficient inference-oriented GPUs like the T4 require 70W minimum to operate. These power requirements far exceed what can be reliably harvested from reasonably-sized solar installations at edge locations.

Dynamic voltage and frequency scaling (DVFS) provides one mechanism for reducing GPU power consumption. By lowering clock frequencies and voltages, DVFS can reduce power draw by 30-50\% with corresponding performance degradation. However, our experiments reveal fundamental limitations of DVFS for intermittent operation. Figure~\ref{fig:dvfs-limits} shows the completion rate of training tasks under solar power using DVFS-enabled GPUs. Even with aggressive power management, commercial GPUs complete less than 50\% of scheduled training tasks. The primary issue stems from minimum operating power. GPUs require substantial baseline power for memory controllers, interconnects, and control logic regardless of compute frequency. When solar power drops below this threshold, the GPU shuts down completely, losing all progress since the last checkpoint.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/DVFS.pdf}
\caption{Training task completion rates for commercial GPUs with DVFS under intermittent solar power. Even with dynamic frequency scaling, GPUs cannot adapt to the wide power variations of solar energy, completing less than half of scheduled tasks.}
\label{fig:dvfs-limits}
\end{figure}

Checkpointing provides another partial solution, saving model state periodically to resume after power failures. However, checkpointing introduces its own overheads. Saving gigabytes of model parameters and optimizer state to non-volatile storage consumes time and energy. Frequent checkpointing to minimize lost work reduces training throughput. More fundamentally, the binary nature of GPU operation (fully on or completely off) means that even with perfect checkpointing, no progress occurs during low-power periods that could support reduced computation. These limitations motivate our morphable approach, where hardware gracefully scales computation rather than failing catastrophically.

\subsection{The Morphable Systolic Array Architecture}

The core of our morphable hardware is a systolic array that can dynamically adjust its size and configuration based on available power. Systolic arrays, characterized by regular arrays of processing elements (PEs) with local connections, naturally suit the structured computations of deep learning. Data flows through the array in a regular pattern, with each PE performing multiply-accumulate operations on inputs and weights. This regularity enables efficient hardware implementation while providing the flexibility needed for morphable operation.

Our design employs 256 PEs organized in a 16$\times$16 array at full capacity, as illustrated in Figure~\ref{fig:usas-full-architecture}. Each PE contains a 16-bit multiplier, 32-bit accumulator, and local register files for storing weights and intermediate values. The PEs connect in a 2D mesh topology, allowing data to flow horizontally for input features and vertically for partial sums. This weight-stationary dataflow, where each PE retains a weight value across multiple computations, minimizes data movement during normal operation and simplifies reconfiguration during power scaling.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/FullComputeMapping_New.pdf}
\caption{Complete \US{} system architecture showing the morphable systolic array, power control hierarchy, non-volatile state buffers, and the integration of inference and training pipelines. The system dynamically adjusts computational capacity across power domains to match solar energy availability.}
\label{fig:usas-full-architecture}
\end{figure}

The key innovation enabling morphability lies in our hierarchical power domain architecture. Rather than treating the array as a monolithic unit, we organize PEs into tiles of 4$\times$4 PEs, each with independent power control. These 16-PE tiles form the basic unit of scaling, allowing the array to operate with 1, 4, 16, 64, or all 256 tiles active. Each tile includes local voltage regulators and power gates that enable rapid power state transitions without affecting neighboring tiles. Clock distribution similarly follows the hierarchical structure, with tile-level clock gating preventing unnecessary switching in inactive regions.

When power availability changes, the morphable controller orchestrates array reconfiguration to match the new power budget. For power increases, the controller first activates additional tiles through their power gates, stabilizes their voltage supplies, and enables their clocks. It then redistributes the computational workload across the expanded array, migrating weight values and adjusting the dataflow schedule. This process completes in microseconds, orders of magnitude faster than the seconds-to-minutes timescale of solar power variations. For power decreases, the process reverses, with the controller first migrating computations away from tiles to be deactivated, then cleanly shutting down those tiles to reduce power consumption.

The weight-stationary dataflow proves particularly advantageous for morphable operation. In this model, each weight remains resident in a PE throughout the computation of an entire output feature map. When tiles are deactivated, their weights remain in place, ready to resume computation when power returns. Only the input activations and partial sums need to be managed during reconfiguration. This property dramatically reduces the state that must be saved and restored, enabling rapid adaptation to power changes without the overhead of full checkpointing.

\subsection{Power Control and Failure Prediction}

Effective morphable operation requires accurate power monitoring and prediction to prevent catastrophic failures while maximizing computational throughput. Our power control system operates at multiple timescales to achieve these goals. At the millisecond timescale, analog power monitors on each tile track instantaneous current draw and voltage levels. These measurements feed into a digital power controller that maintains the array within safe operating limits. At the second timescale, a power prediction model forecasts near-term power availability based on recent solar irradiance patterns, enabling proactive array reconfiguration before power constraints become critical.

The power prediction model employs a lightweight LSTM network trained on historical solar irradiance data from the deployment location. The model takes as input the past hour of irradiance measurements sampled at 10-second intervals, the current time of day, and recent weather patterns encoded as categorical variables. It outputs predicted power availability for the next 5 minutes at 30-second granularity. Our evaluation shows 92\% accuracy in predicting power level transitions (increases or decreases beyond 20\% thresholds), enabling the morphable controller to prepare for reconfigurations in advance.

When the power predictor anticipates a decrease in available power, the morphable controller implements a graduated response strategy. First, it completes any in-flight computations that would be disrupted by reconfiguration, reaching a clean boundary in the training iteration. Next, it selects tiles for deactivation based on their current utilization and the predicted power reduction. The controller preferentially deactivates tiles with the least amount of unique state, minimizing the data that must be migrated. It then redistributes the workload from deactivated tiles across the remaining active tiles, adjusting the computation schedule to maintain load balance. Finally, it cleanly powers down the selected tiles, reducing power consumption to match the predicted budget.

This predictive approach contrasts sharply with reactive systems that wait until power failure is imminent before taking action. By anticipating power changes, we can perform orderly reconfiguration that preserves computation state and maintains forward progress. The 92\% prediction accuracy means we occasionally prepare for power changes that do not materialize, incurring minor overhead from unnecessary state migration. However, this conservative approach proves far superior to the alternative of unexpected power failures that lose significant work.

\subsection{Compute Scheduling Strategies}

The morphable array's ability to scale introduces new challenges in scheduling computations across variable resources. Traditional DNN accelerators employ static schedules optimized for fixed array dimensions. Our morphable architecture requires dynamic scheduling that adapts to the current array configuration while maintaining efficiency, as shown in Figure~\ref{fig:usas-schedule}. We develop two scheduling strategies, conservative and eager, that offer different trade-offs between throughput and resilience to power variations.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/WQSchedule.pdf}
\caption{Work queue scheduling strategy showing how training tasks are dynamically allocated to active tiles based on power availability. The scheduler maintains load balance while adapting to morphable array reconfigurations.}
\label{fig:usas-schedule}
\end{figure}

The conservative scheduling strategy prioritizes completion of individual training iterations over raw throughput. When mapping a layer's computation onto the array, conservative scheduling uses only the minimum number of tiles required, leaving others idle as a power buffer. For example, computing a 64$\times$64 matrix multiplication might use 16 tiles even when 64 are available, reserving the remainder for power fluctuations. This approach ensures that if power suddenly decreases, the computation can complete on the already-active tiles without migration. The trade-off is lower utilization during stable high-power periods, as many tiles remain idle despite available work.

The eager scheduling strategy maximizes throughput by aggressively utilizing all available tiles. When power is plentiful, eager scheduling distributes computation across the entire array, achieving near-linear speedup with tile count. It speculatively begins new training iterations as soon as resources become available, overlapping forward and backward passes when possible. This approach achieves 2.3$\times$ higher throughput than conservative scheduling during stable high-power periods. However, it incurs higher overhead during power transitions, as more state must be migrated when tiles are deactivated.

Our adaptive scheduling approach dynamically selects between conservative and eager strategies based on power prediction confidence and recent stability. When the power predictor indicates stable conditions for the next several minutes, the scheduler adopts the eager strategy to maximize progress. When predictions show imminent power changes or recent history indicates high volatility, it switches to conservative scheduling to minimize disruption. This adaptive behavior achieves within 15\% of eager scheduling throughput during stable periods while maintaining 90\% of conservative scheduling's resilience during volatile periods.

The scheduler also implements fine-grained load balancing across active tiles, as illustrated in Figure~\ref{fig:usas-loadbalance}. As tiles are activated or deactivated, computational load may become imbalanced, with some tiles processing more operations than others. The scheduler monitors tile utilization through performance counters and redistributes work to maintain balance. This redistribution occurs at layer boundaries in the neural network, natural synchronization points where data dependencies are minimal. By maintaining load balance, we ensure that no individual tile becomes a bottleneck, maximizing overall array throughput regardless of configuration.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/LoadBalancing.pdf}
\caption{Load balancing mechanism showing how computational work is redistributed across active tiles to maintain efficiency. The system dynamically reallocates tasks when tiles are activated or deactivated due to power changes.}
\label{fig:usas-loadbalance}
\end{figure}

\subsection{Non-Volatile State Buffers}

Despite our best efforts at prediction and graceful scaling, sudden power failures remain possible. A thick cloud passing over the solar panel can reduce power generation by 80\% in seconds, faster than the array can migrate state. To handle these catastrophic events without losing significant work, we incorporate non-volatile state buffers (NVSBs) throughout the morphable array. These buffers capture critical computation state continuously, enabling rapid recovery when power returns.

Each tile includes a 64KB NVSB implemented using spin-transfer torque magnetic RAM (STT-MRAM). STT-MRAM provides several advantages for intermittent operation. Write operations complete in nanoseconds, fast enough to capture state during normal computation without performance impact. The memory is truly non-volatile, retaining data indefinitely without power. Unlike Flash memory, STT-MRAM supports unlimited write cycles without wear-out, critical for continuous operation. The technology integrates readily with CMOS logic, enabling on-chip implementation without exotic processing steps.

The NVSBs operate in a circular buffer configuration, continuously capturing the most recent computation state. For each tile, we store the current weight values (2KB for a 4$\times$4 tile with 16-bit weights), input activation buffer (4KB), partial sum accumulator values (4KB), and control state including the current position in the computation schedule (256 bytes). This 10KB of critical state per tile is written to the NVSB in a round-robin fashion, with a complete state snapshot every 100 microseconds during active computation.

When power failure occurs, capacitors integrated into the power delivery network provide enough energy for a final NVSB write, capturing the exact state at failure. When power returns, the morphable controller reads the NVSB to restore tile state and resume computation exactly where it stopped. The overhead of state restoration is less than 1 millisecond per tile, negligible compared to the minutes-to-hours duration of power outages. This approach eliminates the need for explicit checkpointing to external storage, which would consume significant time and energy during normal operation.

The NVSBs also enable an optimization we term speculative progress. During very low power periods where only a few tiles can operate, the system can speculatively compute portions of the training iteration, saving intermediate results to NVSBs. Even if power fails before the iteration completes, the partial results are preserved and can be combined when power returns. This approach allows the system to make incremental progress even during marginal power conditions that would cause traditional accelerators to shut down entirely.

\subsection{Hardware Implementation Summary}

Table~\ref{tab:usas-specs} summarizes the area and power characteristics of the \US{} hardware design. The morphable systolic array was implemented in SystemVerilog and synthesized using Synopsys Design Compiler with a 32nm technology library operating at 592~MHz. The design comprises 256 tiles, each containing 64 MAC units organized as an 8$\times$8 array. The SRAM buffers constitute the largest area contribution, providing sufficient on-chip storage for weights, activations, and intermediate results without frequent off-chip memory accesses. The total power of 33.19W represents full-capacity operation; at minimum configuration (16 tiles), power reduces to approximately 4W while maintaining computational progress.

\begin{table}[!htb]
\centering
\caption{Area and power estimation of \US{} hardware design components synthesized at 592~MHz with Synopsys 32nm library.}
\label{tab:usas-specs}
\begin{tabularx}{\linewidth}{lXcc}
\toprule
\textbf{Component} & \textbf{Specification} & \textbf{Power (W)} & \textbf{Area (mm$^2$)} \\
\midrule
SRAM Buffers &
1kB$\times$256 + 8kB$\times$256 + 64kB + 16$\times$256kB
& 10.37 & 117.16 \\

MAC Unit &
(8$\times$8)$\times$256
& 8.46 & 32.72 \\

Adder Tree \& Comparator &
16$\times$16\,bit + 256
& 2.40 & 21.56 \\

Control Logic &
--
& 0.96 & 12.20 \\

Host Processor &
ARM Cortex A78 series
& 11.00 & -- \\

\midrule
\textbf{Total} & 256 tiles & 33.19 & 183.64 \\
\bottomrule
\end{tabularx}
\end{table}

\FloatBarrier
\section{Implementation and Evaluation}
\label{sec:usas-evaluation}

To validate the \US{} approach, we implemented both the algorithmic framework and morphable hardware architecture, evaluating them on real-world video analytics workloads powered by actual solar irradiance traces. This section presents our experimental methodology, analyzes system performance across multiple dimensions, and quantifies the sustainability benefits of our approach compared to traditional solutions.

\subsection{Experimental Setup}

Our evaluation employs the Urban Traffic Dataset, consisting of 1080p video captured from 50 traffic cameras across a major metropolitan area over six months. The dataset includes diverse conditions spanning different times of day, weather patterns, and seasonal variations. Ground truth labels for 100,000 frames were provided by human annotators, enabling us to measure accuracy precisely. The remaining 10 million frames are unlabeled, reflecting the realistic scenario where continuous learning must proceed without human supervision. The dataset exhibits severe class imbalance typical of real deployments, with cars appearing in 89\% of frames, pedestrians in 34\%, buses in 8\%, motorcycles in 3\%, and emergency vehicles in only 0.3\%.

For solar power traces, we use NOAA SOLRAD measurements from the deployment location, providing minute-resolution irradiance data spanning two complete years. These traces capture real-world variations including diurnal cycles, weather events, and seasonal changes. Peak summer irradiance reaches 1000 W/m$^2$ at solar noon, while winter peaks are only 400 W/m$^2$. Cloud cover causes rapid fluctuations, with irradiance dropping 70\% in under a minute during storm fronts. We model a 2m$^2$ solar panel with 20\% efficiency, typical of commercial installations, providing 0-400W of power depending on conditions.

The morphable hardware is modeled using a combination of RTL simulation and physical design. We implemented the 256-PE systolic array in SystemVerilog, validating functional correctness through extensive testing. Synthesis using TSMC 7nm technology libraries in Synopsys Design Compiler provides accurate power and area estimates. At full capacity (256 PEs active), the array consumes 285W while achieving 4.1 TFLOPS of 16-bit computation. With only 16 PEs active (minimum configuration), power reduces to 23W while still providing 256 GFLOPS. The non-volatile state buffers add 8\% area overhead but negligible power consumption during normal operation.

We compare \US{} against three baseline systems. The first baseline uses an NVIDIA T4 GPU, representing efficient datacenter hardware, with aggressive DVFS and checkpointing optimized for intermittent operation. The second employs a state-of-the-art DNN accelerator modeled after commercial edge AI chips, fixed at 100W power consumption. The third baseline, "Ekya+", extends the Ekya continuous learning system with perfect solar power prediction and infinite battery storage, representing an idealized traditional approach. All systems run identical student (MobileNetV2) and teacher (ResNet-50 ensemble) models to ensure fair comparison.

\subsection{Continuous Learning Performance}

The primary metric for any continuous learning system is its ability to maintain model accuracy as data distributions shift over time. Our evaluation examines model accuracy over a six-month deployment period for all evaluated systems. Without any continuous learning, model accuracy degrades from 91.2\% initial accuracy to 72.8\% after six months as data drift accumulates, as demonstrated in Section~\ref{sec:usas-challenge}. This 18.4\% degradation would be unacceptable for most deployments, confirming the necessity of continuous learning for edge video analytics.

The Ekya+ baseline with perfect power maintains 88.7\% average accuracy through continuous retraining. However, this idealized system assumes unlimited battery storage and perfect power prediction, neither of which is realistic for edge deployments. When run on actual solar power without batteries, Ekya+ completes only 31\% of training iterations due to its inability to adapt to power variations, achieving just 79.3\% average accuracy.

\US{} achieves 93.1\% average accuracy, 4.96\% higher than the best baseline. This improvement stems from several factors working in concert. The confidence-based frame selection identifies distribution shifts quickly, focusing training on problematic patterns before accuracy degrades significantly. The student-teacher paradigm generates high-quality labels that improve upon the original training data, particularly for ambiguous cases. The exemplar selection maintains class balance despite the skewed distribution of observed data. Most critically, the morphable hardware ensures training progresses even during low-power periods when baselines must shut down.

The accuracy improvement is not uniform across all classes, as shown in Figure~\ref{fig:usas-accrl}. For common classes like cars, all systems maintain above 95\% accuracy since abundant training examples are available. For rare classes like emergency vehicles, \US{} shows dramatic improvement, achieving 87.3\% accuracy compared to 68.2\% for Ekya+. This improvement is critical for practical deployments where missing rare but important events (emergency vehicles, accidents, unusual cargo) could have serious consequences.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/AccRL.pdf}
\caption{Accuracy improvement from representation learning-based exemplar selection. \US{} maintains balanced accuracy across all classes, including rare but critical events, by intelligently managing the training buffer to counter sampling bias.}
\label{fig:usas-accrl}
\end{figure}

The micro-profiler plays a critical role in determining optimal hyperparameters for training under variable power conditions. Figure~\ref{fig:usas-microprofiler} demonstrates the effectiveness of our micro-profiling approach compared to an oracle selection. Over multiple training iterations, the micro-profiler consistently identifies the correct number of layers to train (achieving 87.06\% agreement with oracle), selects appropriate batch sizes (82.64\% accuracy), and efficiently balances exemplar selection with training time. The average accuracy deviation from oracle-selected parameters is only 2.46\%, demonstrating that the micro-profiler can effectively guide hyperparameter selection without exhaustive search.

\begin{figure}[!htb]
\centering
\subfloat[Number of layers trained vs. oracle selection.]{
  \includegraphics[width=\linewidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/4.pdf}
  \label{fig:usas-layers}
}

\subfloat[Batch-size selection and convergence behavior.]{
  \includegraphics[width=\linewidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/5.pdf}
  \label{fig:usas-config}
}

\subfloat[Time distribution between exemplar selection and training.]{
  \includegraphics[width=\linewidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/6.pdf}
  \label{fig:usas-extr}
}
\caption{Algorithmic performance of the \US{} micro-profiler: hyperparameter selection compared to oracle over multiple training iterations. The micro-profiler achieves 87.06\% accuracy in layer selection and 82.64\% in batch size selection.}
\label{fig:usas-microprofiler}
\end{figure}

\subsection{Hardware Efficiency Analysis}

The morphable hardware architecture enables \US{} to maintain computational efficiency across the wide power range of solar energy. Figure~\ref{fig:hardware-arch} shows the relationship between available power and achieved throughput for different hardware configurations. The morphable array achieves near-linear scaling from 256 GFLOPS at 23W (16 PEs) to 4.1 TFLOPS at 285W (256 PEs). This 16$\times$ compute scaling with 12.4$\times$ power scaling demonstrates the efficiency of our hierarchical design, where inactive tiles consume negligible power.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/ACDesign.pdf}
\caption{Morphable hardware architecture showing the hierarchical tile organization, power domains, and data flow paths that enable dynamic scaling with solar power availability.}
\label{fig:hardware-arch}
\end{figure}

Commercial GPUs with DVFS achieve only 3$\times$ throughput scaling across their power range, limited by fixed overheads in memory controllers and interconnects. More problematically, GPUs exhibit a sharp cutoff below their minimum operating power (70W for T4), achieving zero throughput when solar power is insufficient. The morphable array continues operating down to 23W, making forward progress even during heavily overcast conditions when GPUs must shut down entirely.

The overhead of morphing between configurations proves minimal. Activating additional tiles requires 43 microseconds on average, primarily spent stabilizing voltage regulators and enabling clocks. Deactivating tiles completes faster at 27 microseconds since no stabilization is required. Even with frequent reconfiguration (every 10 seconds on average during partly cloudy conditions), morphing overhead is less than 0.01\% of total runtime. This low overhead validates our design choice of hardware-supported morphing rather than software-based checkpointing and migration.

Power prediction accuracy significantly impacts hardware utilization. With our 92\% accurate power predictor, the morphable array operates at 81\% of theoretical peak efficiency, where efficiency is defined as achieved throughput divided by maximum possible throughput at the current power level. Without prediction (reactive-only morphing), efficiency drops to 64\% due to work lost during unexpected power failures. Perfect prediction would achieve 89\% efficiency, with the remaining loss due to conservative scheduling during volatile periods.

Table~\ref{tab:usas-hw-comparison} compares \US{} against prior DNN accelerator platforms. While \US{} does not achieve the highest raw throughput or energy efficiency, this is by design. The morphable architecture prioritizes scheduling flexibility over peak performance, enabling operation across a wide power range rather than optimizing for a single operating point. When operating at full power on DNN computation alone, \US{} achieves competitive energy efficiency of 287.44~GOps/W. The apparent efficiency reduction in full-system operation (159.40~GOps/W) reflects the overhead of exemplar selection, micro-profiling, non-volatile memories, and the host processor that enable sustainable operation under intermittent power.

\begin{table}[!htb]
\centering
\caption{Comparison of \US{} with prior accelerator-based platforms. \US{} prioritizes scheduling flexibility for intermittent operation over peak throughput.}
\label{tab:usas-hw-comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Platform} & \textbf{Freq.} & \textbf{Area} & \textbf{Power} & \textbf{Peak Thpt.} & \textbf{Eff.} \\
 & (MHz) & (mm$^2$) & (W) & (GOps) & (GOps/W) \\
\midrule
DaDianNao & 606 & 67.3 & 16.3 & 4964 & 304.5 \\
CNVLUTIN & 606 & 70.1 & 17.4 & 4964 & 285.3 \\
Activation Sparse & 667 & 292 & 19.2 & 5466 & 284.7 \\
EyerissV2 & 200 & N/A & N/A & 153.6 & 193.7 \\
FlexBlock & 333 & 160.3 & 34.4 & 4504 & 131.0 \\
\midrule
\US{} & 592 & 168.2 & 22.7 & 4016 & 159.4 \\
\US{} (DNN only) & \multicolumn{4}{c}{Fully powered, compute only} & 287.4 \\
\US{} (w/ $\mu$-prof) & \multicolumn{4}{c}{Fully powered, with micro-profiler} & 255.4 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sustainability Impact}

The ultimate goal of \US{} is enabling sustainable edge computing that scales with deployment size without proportional environmental impact. Table~\ref{tab:sustainability} quantifies the energy consumption and carbon emissions of different approaches for a single edge server over one year of operation. The analysis assumes the server operates continuously, performing inference at all times and training when power permits.

\begin{table}[!htb]
\centering
\caption{Annual energy consumption and CO$_2$ emissions per edge server for different continuous learning approaches. \US{} operates entirely on solar power, eliminating grid energy consumption.}
\label{tab:sustainability}
\begin{tabular}{lrrr}
\toprule
\textbf{System} & \textbf{Energy (kWh/year)} & \textbf{CO$_2$ (kg/year)} & \textbf{Accuracy (\%)} \\
\midrule
No Learning & 175 & 149 & 72.8 \\
GPU (T4) & 2,628 & 2,234 & 79.3 \\
DNN Accelerator & 876 & 745 & 88.7 \\
\US{} (Solar) & 0 & 0 & 93.1 \\
\midrule
\US{} Savings vs GPU & 2,628 & 2,234 & +13.8 \\
\US{} Savings vs Accelerator & 876 & 745 & +4.4 \\
\bottomrule
\end{tabular}
\end{table}

A GPU-based solution consumes 2,628 kWh annually, assuming aggressive power management and duty cycling. At the U.S. average of 0.85 kg CO$_2$ per kWh, this generates 2,234 kg of CO$_2$ emissions per server. For a city-scale deployment with 10,000 cameras, the aggregate impact would be 26.3 GWh of energy and 22,340 metric tons of CO$_2$ annually, equivalent to the emissions from 4,800 passenger vehicles.

The DNN accelerator baseline performs better at 876 kWh annually, benefiting from specialized hardware optimized for neural network computation. However, this still represents substantial grid energy consumption that scales linearly with deployment size. Moreover, the fixed power consumption means the accelerator cannot take advantage of variable renewable energy availability, requiring grid connection even when solar power is plentiful.

\US{} operates entirely on harvested solar energy, consuming zero grid power and producing zero operational emissions. The 2m$^2$ solar panel generates approximately 1,460 kWh annually at the evaluation site, more than sufficient for continuous operation. Excess generation during summer months could be fed back to the grid or used for other edge computing tasks. The manufacturing emissions of the solar panel and morphable hardware are amortized over a 20-year lifespan, adding less than 50 kg CO$_2$-equivalent per year.

Beyond raw energy savings, \US{} enables deployments in locations where grid power is unavailable or unreliable. Rural monitoring, disaster response, and temporary installations become feasible without electrical infrastructure. The system's ability to operate through multi-day cloudy periods (with degraded but non-zero performance) provides resilience that battery-based systems cannot match economically.

\subsection{Impact on Exemplar Selection}

The use of multiple teacher models for data annotation and exemplar selection provides significant advantages over single-teacher approaches. Prior works on continuous learning have either employed a single teacher model (e.g., Ekya using ResNeXt101) to annotate data, or relied on heuristics such as randomized selection, K-Last List, or round-robin policies. As demonstrated in Figure~\ref{fig:usas-teachers-impact}, single-teacher approaches, even when augmented with heuristics, frequently fail to identify the optimal exemplar set.

The quality of exemplar selection directly impacts accuracy through two mechanisms. First, missing valid exemplars prevents the student model from learning vital information, increasing its drift over time. Second, incorrect annotations by a single teacher can cause the student to learn erroneous labels, leading to increased mispredictions. To address these issues, \US{} employs an ensemble of three teacher models (ResNet-101, YOLO-V2, and VGG-16) that perform majority voting to determine the correct exemplars. This ensemble approach significantly reduces both false positives and true negatives compared to single-teacher methods. Furthermore, the feature extraction for each potential exemplar is hardware-assisted, imposing no overhead on the inference task.

\begin{figure}[!htb]
\centering
\includegraphics[width=\linewidth]{Chapter-6/Usas-HPCA24-CameraReady/figs/teachers3.pdf}
\caption{Impact of multiple teachers on exemplar selection accuracy. The ensemble approach (top bar) achieves the best annotation quality with only 2 false positives, significantly outperforming single-teacher approaches (R: ResNeXt101, T: YOLO-V3, V: VGG-16) and heuristic methods (IL: Intermittent Learning, RR: Round Robin).}
\label{fig:usas-teachers-impact}
\end{figure}


\subsection{Task Completion Under Intermittent Power}

A critical metric for intermittent systems is the ability to complete scheduled tasks despite power variations. We measure task completion rate as the fraction of training iterations that finish successfully within their deadlined time windows. For this experiment, we schedule one training iteration every hour, requiring completion within the subsequent 4-hour window to maintain model freshness.


\US{} achieves 94\% task completion by adapting computation to available power. During full sun, training iterations complete in under 20 minutes using all 256 PEs. During overcast conditions, the same iteration may require 3 hours using only 16-32 PEs, but still completes within the deadline. Only during extreme conditions (dense fog, heavy storms) does \US{} fail to complete iterations, and even then, partial progress is preserved through NVSBs for resumption when conditions improve.

The adaptive scheduling strategy proves crucial for maintaining high completion rates. During a typical partly cloudy day, solar power may fluctuate between 50W and 300W on minute timescales. The eager scheduler aggressively utilizes high-power periods, completing 70\% of an iteration during a 10-minute sunny break. The conservative scheduler ensures progress continues during subsequent cloudy periods, albeit at reduced rates. This combination enables \US{} to complete iterations that would be impossible with fixed-power hardware.

\subsection{Generalization to Other Applications and Domains}

The techniques developed in \US{} extend beyond video analytics to other data modalities and application domains. Figure~\ref{fig:usas-beyond} presents a comprehensive analysis of how different system components contribute to overall performance and how well the approach generalizes.

Figure~\ref{fig:usas-contri} illustrates the contribution of different \US{} components under varying power profiles. When power is highly uncertain, the morphable hardware provides the dominant benefit by enabling continuous progress despite fluctuations. As the power profile stabilizes, the algorithmic contributions (exemplar selection and micro-profiler) become increasingly important. This complementary relationship ensures robust performance across the full range of operating conditions.

\begin{figure}[!htb]
\centering

\subfloat[Component contribution under different power profiles.]{
  \includegraphics[
    trim={0 12pt 0 0},
    clip,
    width=0.7\linewidth
  ]{Chapter-6/Usas-HPCA24-CameraReady/figs/contri.pdf}
  \label{fig:usas-contri}
}\\[0.5em]

\subfloat[Generalization to other data modalities.]{
  \includegraphics[
    width=0.7\linewidth
  ]{Chapter-6/Usas-HPCA24-CameraReady/figs/genrl.pdf}
  \label{fig:usas-genrl}
}\\[0.5em]

\subfloat[Hardware-assisted scheduling across different scenarios.]{
  \includegraphics[
    width=0.7\linewidth
  ]{Chapter-6/Usas-HPCA24-CameraReady/figs/Sens.pdf}
  \label{fig:usas-sens}
}

\caption{Analysis of \US{} components and their generalization: (a) morphable hardware dominates under volatile power while algorithmic contributions excel under stable conditions; (b) exemplar selection and micro-profiler outperform baselines across Audio, 3D Point Cloud, and IMU data; (c) hardware assistance provides greater benefit for smaller workloads and more sporadic energy availability.}
\label{fig:usas-beyond}
\end{figure}

The algorithmic innovations generalize to diverse data modalities beyond video. We evaluated the exemplar selection and micro-profiler on Audio data (speech classification from AudioMNIST and CHiME datasets), 3D Point Clouds (object classification from KITTI and nuScenes autonomous driving datasets), and Inertial Measurement Unit sensor data (fault and activity detection from bearing and mHealth datasets). As shown in Figure~\ref{fig:usas-genrl}, \US{} outperforms state-of-the-art approaches across all modalities. This superior performance stems from \US{}'s design for dense, noisy data characteristic of real-world deployments, whereas prior methods were often tuned for clean benchmark datasets.

Figure~\ref{fig:usas-sens} demonstrates how hardware-assisted scheduling impacts performance across different data and model sizes, as well as varying energy income patterns. As data and model dimensions decrease, the relative impact of hardware assistance becomes more pronounced because smaller workloads have lower tolerance for wasted computation. Similarly, as energy income becomes more sporadic, the morphable hardware's ability to seamlessly transfer work to active processing elements becomes increasingly valuable.


\FloatBarrier
\section{Bridging Scales: From Sensors to Servers}
\label{sec:usas-bridging}

As we reach the culmination of our technical contributions, it becomes essential to step back and examine the common principles that unite our work across six orders of magnitude in power scale. From the resource-constrained intelligent inference systems of Chapter~\ref{ch:intelligent-inference}, through the adaptive intermittent training mechanisms of Chapter~\ref{ch:nexume}, to the kilowatt-scale edge servers presented here, certain fundamental insights about intermittent computing emerge repeatedly, albeit manifested differently at each scale.

\subsection{Scale-Invariant Principles}

The most striking observation is that intermittency imposes similar challenges regardless of scale. Every intermittent system must address three core problems that transcend power levels. First, progress preservation requires mechanisms to capture and restore computation state across power failures. Whether through voltage-level checkpoints in Camaroptera, gradient buffers in Seeker, analog state retention in LREye, or non-volatile state buffers in \US{}, each system implements state management appropriate to its scale. Second, adaptive computation demands that processing adjust dynamically to available energy. This manifests as incremental loop execution in tiny sensors, batch size scaling in embedded devices, circuit biasing in analog accelerators, and morphable arrays in edge servers. Third, quality maintenance necessitates techniques to ensure output correctness despite interrupted operation. Convergence guarantees in intermittent training, noise-aware analog computation, and confidence-based continuous learning all serve this purpose at their respective scales.

Yet while the challenges remain consistent, the solutions evolve with scale in predictable ways. State management progresses from bit-level checkpoints to gigabyte model snapshots. Adaptation granularity shifts from individual instructions to entire neural network layers. Quality assurance evolves from simple retry mechanisms to sophisticated learning algorithms. This progression suggests that insights from one scale can inform designs at another, as we demonstrated by applying intermittent training principles from embedded devices to edge server continuous learning.

The relationship between power scale and computational complexity follows a clear pattern. At the microwatt scale of Camaroptera, we could barely sustain simple sensor processing and basic inference. The milliwatt scale of Seeker and LREye enabled neural network training, though limited to small models and careful optimization. The kilowatt scale of \US{} supports complex video analytics, continuous learning, and sophisticated adaptation algorithms. This exponential growth in capability with linear power scaling highlights the importance of energy efficiency at every scale, as small improvements compound dramatically when replicated across thousands of devices.

\subsection{Scale-Specific Innovations}

While common principles unite our work, each scale also demanded unique innovations that would be inappropriate or impossible at other scales. These scale-specific solutions provide insights into the fundamental trade-offs of intermittent computing at different power levels.

At the microscale of batteryless sensors, energy is so precious that every microjoule matters. Camaroptera's innovations focused on minimizing checkpoint size and placement, optimizing for the common case where power failures occur frequently but briefly. The system could afford only minimal state preservation, saving just program counters and critical variables. This extreme frugality would be counterproductive at larger scales where the relative cost of state management becomes negligible compared to computation.

The embedded scale of Seeker introduced sufficient resources for intelligent adaptation. With kilobytes of RAM and megahertz of processing, Seeker could implement sophisticated checkpointing strategies, analyze power trends, and optimize training schedules. These capabilities enabled qualitatively different behaviors, like speculative execution during marginal power and dynamic batch sizing based on energy predictions. Such optimizations require computational overhead that would overwhelm microscale devices but prove essential for efficient embedded operation.

LREye's analog approach at the milliwatt scale exploited physical properties impossible to replicate digitally. By computing directly in the analog domain, LREye achieved energy efficiency that digital systems cannot match. The innovation of hardware-aware training, where neural networks learn to compensate for circuit imperfections, represents a fundamental shift from fighting hardware variations to embracing them. This co-design philosophy could theoretically apply at larger scales but becomes less advantageous as digital efficiency improves and variations become relatively smaller.

\US{} at the kilowatt scale could afford architectural complexity impossible at smaller scales. The morphable systolic array with 256 independently controllable tiles would be absurd overhead for a sensor but proves essential for matching the wide dynamic range of solar power. The sophisticated learning algorithms (student-teacher paradigm, representation learning, micro-profiling) require computational resources that smaller scales cannot provide. Yet these capabilities enable qualitatively superior outcomes, like continuous learning that improves accuracy rather than merely maintaining it.

\subsection{Towards a Unified Framework}

The progression from sensors to servers suggests the outlines of a unified framework for intermittent computing that could guide future research and development. This framework would recognize intermittency not as an unfortunate constraint but as a fundamental property of sustainable computing that must be addressed at every level of the system stack.

At the hardware level, the framework would mandate support for variable power operation from circuit to architecture. This includes fine-grained power domains for selective activation, non-volatile elements for state preservation, and adaptive circuits that gracefully degrade rather than fail catastrophically. The specific implementations would vary with scale (analog vs. digital, centralized vs. distributed) but the capabilities would be universal.

The software layer would provide abstractions for intermittent operation that hide complexity while exposing control. Applications would specify quality requirements and deadline constraints rather than computational steps. Runtime systems would automatically manage checkpointing, scheduling, and adaptation based on available energy and application needs. Libraries would offer pre-validated implementations of common intermittent computing patterns, from simple sensing loops to complex learning algorithms.

At the algorithmic level, the framework would encompass design patterns proven effective across scales. These include incremental algorithms that make progress in small steps, approximation techniques that trade accuracy for energy efficiency, and ensemble methods that combine multiple weak models into strong predictions. The framework would provide formal tools for analyzing convergence, bounding error, and ensuring eventual completion despite arbitrary interruption patterns.

The system level would address end-to-end concerns that span multiple devices and scales. This includes protocols for intermittent communication between devices with unsynchronized power cycles, distributed algorithms that tolerate arbitrary node failures, and learning systems that adapt to varying participation. The framework would recognize that in a sustainable computing ecosystem, intermittency is the norm rather than the exception.

Most importantly, the framework would be grounded in sustainability metrics that extend beyond simple energy efficiency. This includes lifecycle carbon accounting that considers manufacturing and disposal, deployment flexibility that enables operation in diverse environments, and scalability analysis that ensures solutions remain viable at global deployment scales. By making sustainability a first-class design constraint alongside performance and cost, the framework would guide development of truly sustainable computing systems.

\FloatBarrier
\section{Discussion and Future Directions}
\label{sec:usas-discussion}

While \US{} demonstrates the feasibility of solar-powered continuous learning at the edge, several limitations and open challenges remain. Addressing these issues will be crucial for widespread deployment and long-term success of sustainable edge computing.

\subsection{Current Limitations}

The most significant limitation of \US{} is its dependence on sufficient solar exposure for operation. While the morphable architecture enables operation at reduced capacity during cloudy periods, extended storms or deployment in consistently overcast regions may provide insufficient energy for meaningful progress. Polar regions with months of darkness, urban canyons with limited sky visibility, and indoor deployments cannot rely solely on solar power. Future work should explore hybrid energy harvesting combining solar with wind, vibration, or thermal gradients to improve availability.

The current implementation focuses on computer vision tasks, specifically object detection and classification in video streams. While these represent important and common edge AI applications, many other workloads could benefit from sustainable edge computing. Natural language processing, audio analysis, time series prediction, and sensor fusion all have different computational patterns that may require architectural adaptations. The morphable array excels at the regular computations of CNNs but may prove less efficient for the irregular access patterns of graph neural networks or the sequential dependencies of recurrent models.

Our evaluation used a single geographic location with specific solar irradiance patterns. Different locations exhibit vastly different solar profiles based on latitude, weather patterns, and seasonal variations. Tropical regions enjoy consistent daily solar cycles but must contend with monsoon seasons. Desert deployments have excellent solar availability but extreme temperatures that affect hardware reliability. Arctic installations face months of continuous daylight followed by months of darkness. A truly universal solution must adapt to these diverse conditions, potentially through location-specific optimization or deployable energy storage for extreme cases.

The student-teacher paradigm, while effective for unsupervised learning, may propagate and amplify biases present in the teacher models. If teacher models exhibit demographic biases, these could be reinforced through continuous learning, particularly given the class imbalance in real-world data. Future work should incorporate fairness constraints and bias detection mechanisms to ensure equitable performance across different populations and scenarios.

\subsection{Broader Implications}

The success of \US{} has implications beyond the specific technical contributions, suggesting new directions for sustainable computing and edge AI deployment. These implications span technical, economic, and social dimensions.

From a technical perspective, \US{} demonstrates that sophisticated AI workloads can operate sustainably without compromising performance. This challenges the prevailing assumption that advancing AI capabilities requires ever-increasing energy consumption. By co-designing algorithms and hardware for intermittent operation, we achieve better accuracy than grid-powered alternatives while eliminating operational emissions. This suggests that sustainability and performance are not inherently conflicting goals but can be synergistic when approached holistically.

The economic implications are equally significant. Traditional edge deployments require substantial infrastructure investment for power distribution and cooling. \US{} eliminates these requirements, reducing deployment costs and enabling installations in previously infeasible locations. The operational cost savings from eliminated electricity bills can offset the initial investment in solar panels and morphable hardware within 2-3 years. For developing regions with limited electrical infrastructure, sustainable edge computing could leapfrog traditional approaches, similar to how mobile phones bypassed landline networks.

From a social perspective, \US{} enables privacy-preserving AI that operates locally without cloud dependencies. This has profound implications for digital sovereignty, allowing communities and nations to maintain control over their data and AI infrastructure. The ability to deploy intelligent systems in remote locations could improve services for underserved populations, from rural health monitoring to agricultural optimization. By demonstrating that powerful AI can operate sustainably, \US{} also contributes to public acceptance of AI deployment, addressing concerns about environmental impact.

\subsection{Future Research Directions}

Several promising research directions emerge from our work on \US{}, each offering opportunities to extend and improve sustainable edge computing.

The morphable architecture concept could be extended to other computational patterns beyond systolic arrays. Morphable graph processors for irregular workloads, morphable memory systems that trade capacity for bandwidth based on power availability, and morphable interconnects that adjust topology for energy efficiency all merit investigation. The key insight that hardware should adapt to power rather than assuming constant availability applies broadly across computer architecture.

The software stack for intermittent computing remains largely unexplored. Current programming models assume reliable execution and fail catastrophically under intermittency. New languages, compilers, and runtime systems designed for intermittent operation could dramatically simplify development. Automated tools for converting existing applications to intermittent-safe versions would accelerate adoption. Formal verification techniques ensuring correct operation despite arbitrary power failures would provide confidence in critical deployments.

The intersection of federated learning and intermittent computing presents intriguing possibilities. Edge devices with intermittent power cannot participate reliably in synchronized federated learning rounds. New asynchronous federated learning protocols that tolerate arbitrary device availability could enable global model improvement from intermittently powered edge nodes. This would create a virtuous cycle where sustainable edge devices contribute to improving global models that enhance their own operation.

Finally, the ultimate vision is autonomous systems that operate indefinitely on ambient energy while continuously improving their capabilities. This requires advances across the entire stack: ultra-efficient neuromorphic hardware inspired by biological systems, self-organizing software that adapts to changing resources, and learning algorithms that extract maximum knowledge from minimal data. While ambitious, the trajectory from Camaroptera's simple sensors to \US{}'s sophisticated edge servers suggests this vision is achievable.

\FloatBarrier
\section{Chapter Summary}
\label{sec:usas-summary}

This chapter has presented \US{}, a comprehensive system for solar-powered continuous learning at the edge that represents the culmination of our journey through intermittent computing across scales. By developing novel algorithms for unsupervised learning, architecting morphable hardware that adapts to power variations, and demonstrating superior accuracy while eliminating grid dependence, \US{} shows that sustainable AI at scale is not only possible but advantageous.

The key technical contributions of \US{} address the unique challenges that arise when scaling intermittent computing from sensors to servers. Our confidence-based frame selection and student-teacher paradigm enable continuous learning from streaming video without human labels. The representation learning approach maintains class balance despite severely skewed real-world distributions. The micro-profiler optimizes hyperparameters dynamically based on power availability and data characteristics. Most significantly, the morphable systolic array provides a hardware substrate that gracefully scales computation with solar power, operating efficiently across a 12$\times$ power range where traditional accelerators would fail entirely.

Our evaluation demonstrates that \US{} achieves multiple objectives simultaneously. The system maintains 93.1\% accuracy on challenging video analytics tasks, 4.96\% higher than grid-powered alternatives. It operates entirely on solar power, eliminating 2,234 kg of CO$_2$ emissions annually per server compared to GPU-based solutions. The morphable hardware completes 94\% of scheduled training tasks despite intermittent power, ensuring models stay current with evolving data distributions. These results validate our thesis that embracing intermittency through co-design yields superior outcomes compared to fighting it through energy storage.

Stepping back from the specific technical contributions, \US{} demonstrates several broader principles about sustainable computing. The scale-invariant nature of intermittency challenges suggests that insights from tiny sensors inform the design of large servers and vice versa. The success of hardware-software co-design at every scale reinforces that sustainable computing requires holistic system thinking rather than point optimizations. The achievement of better accuracy through sustainable operation challenges the assumption that environmental responsibility requires performance compromise.

The progression from Chapter~\ref{ch:intelligent-inference}'s intelligent inference mechanisms through Chapter~\ref{ch:nexume}'s adaptive intermittent training, Chapter~\ref{ch:lreye}'s analog acceleration, to this chapter's edge servers reveals a coherent vision for sustainable intelligent systems. Each chapter contributed essential insights: the principles of efficient inference under constraints, the mechanisms for adaptive training under intermittent power, the circuit-level innovations for ultimate energy efficiency, and finally, the system integration enabling practical large-scale deployment. Together, they form a complete picture of how intelligent systems can operate sustainably across the entire spectrum of computational scales.

As we conclude this technical journey, \US{} stands as proof that the vision of ubiquitous, sustainable intelligence is achievable. Edge servers powered entirely by renewable energy, continuously learning and adapting while producing zero operational emissions, transform from academic curiosity to practical reality. The path from sensors to servers reveals not just technical solutions but a philosophical shift: recognizing intermittency as a fundamental property of sustainable computing that, when properly addressed, enables systems superior to their constantly powered predecessors. This recognition opens new possibilities for computing that respects both the potential of artificial intelligence and the constraints of our physical world.

With the technical foundations of sustainable, intermittent-aware intelligent systems now established across multiple scales, we turn in Chapter~\ref{chapter8:conclusions} to synthesize the broader implications of this work. The journey from resource-constrained inference through intermittent training to solar-powered continuous learning reveals not only a progression of technical capabilities but also fundamental principles about the future of sustainable computing. The concluding chapter examines how these insights extend beyond the specific systems presented here, exploring the path toward a computing ecosystem where sustainability and intelligence develop in harmony rather than conflict.